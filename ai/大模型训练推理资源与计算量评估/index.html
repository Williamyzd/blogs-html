<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="计算量评估 前馈神经网络计算量 前馈神经网络：Y = XW +B\n在这个过程中，完成的运算如下：\n$$ \\begin{bmatrix} x_0 &amp; x_1 &amp; x_2 \\end{bmatrix} \\times \\begin{bmatrix} w_{00} &amp; w_{01} \\ w_{10} &amp; w_{11} \\ w_{20} &amp; w_{21} \\end{bmatrix} + \\begin{bmatrix} b_0 \\ b_1 \\end{bmatrix} $$\n其中，W是权重矩阵，X是输入向量，B是偏置项。即：\n$$ y = \\begin{bmatrix} y_0 \\ y_1 \\end{bmatrix} \\begin{matrix} y_0 = w_{00}x_0 + w_{01}x_1 + b_0\\ y_1 = w_{10}x_0 + w_{11}x_1 + b_1\\\n\\end{matrix} $$\n上述2个公式中，每个公式都有3个加法、3个乘法计算。所以[1,3]的矩阵和[3,2]的矩阵相乘共包含 2132 =12 次浮点计算量(称为FLOPs)。从而推广到一般：[m,k]的矩阵和[k,n]的矩阵相乘，需要2mkn FLOPs。而GPU在不同精度单位下，浮点数运算的效率是不同的。例如，A100在16位精度(FP16或BF16)下每秒可以进行$31410^{12}$次浮点运算。而在32位精度下，每秒只能进行$15610^{12}$次浮点运算。所以，我们可以理所当然的想到一个提速思路 - 让所有运算在低精度下进行来提高训练/推理效率。但是，这里有个无法忽略的问题：某些计算必须要在高精度下进行。例如全程在低精度下进行模型训练，往往会出现由于精度不够导致数值溢出和无法收敛的问题。所以目前大模型训练通常采用的是混合精度方式进行：前向传播、反向传播这些无需高精度的计算采用16位精度(FP16或BF16)进行，优化器状态更新在32位精度(FP32)进行。而模型推理的要求就比较低，可以在16位、8位甚至4位精度进行，整个任务的计算效率非常高。\n">
<title>大模型训练推理资源与计算量评估</title>

<link rel='canonical' href='https://blog.liu-nian.top/ai/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86%E8%B5%84%E6%BA%90%E4%B8%8E%E8%AE%A1%E7%AE%97%E9%87%8F%E8%AF%84%E4%BC%B0/'>

<link rel="stylesheet" href="/scss/style.min.946cca6c6259ef94ac55abfae7c7bf3291ea3ed5eea17ef77500b257217c6710.css"><meta property='og:title' content="大模型训练推理资源与计算量评估">
<meta property='og:description' content="计算量评估 前馈神经网络计算量 前馈神经网络：Y = XW +B\n在这个过程中，完成的运算如下：\n$$ \\begin{bmatrix} x_0 &amp; x_1 &amp; x_2 \\end{bmatrix} \\times \\begin{bmatrix} w_{00} &amp; w_{01} \\ w_{10} &amp; w_{11} \\ w_{20} &amp; w_{21} \\end{bmatrix} + \\begin{bmatrix} b_0 \\ b_1 \\end{bmatrix} $$\n其中，W是权重矩阵，X是输入向量，B是偏置项。即：\n$$ y = \\begin{bmatrix} y_0 \\ y_1 \\end{bmatrix} \\begin{matrix} y_0 = w_{00}x_0 + w_{01}x_1 + b_0\\ y_1 = w_{10}x_0 + w_{11}x_1 + b_1\\\n\\end{matrix} $$\n上述2个公式中，每个公式都有3个加法、3个乘法计算。所以[1,3]的矩阵和[3,2]的矩阵相乘共包含 2132 =12 次浮点计算量(称为FLOPs)。从而推广到一般：[m,k]的矩阵和[k,n]的矩阵相乘，需要2mkn FLOPs。而GPU在不同精度单位下，浮点数运算的效率是不同的。例如，A100在16位精度(FP16或BF16)下每秒可以进行$31410^{12}$次浮点运算。而在32位精度下，每秒只能进行$15610^{12}$次浮点运算。所以，我们可以理所当然的想到一个提速思路 - 让所有运算在低精度下进行来提高训练/推理效率。但是，这里有个无法忽略的问题：某些计算必须要在高精度下进行。例如全程在低精度下进行模型训练，往往会出现由于精度不够导致数值溢出和无法收敛的问题。所以目前大模型训练通常采用的是混合精度方式进行：前向传播、反向传播这些无需高精度的计算采用16位精度(FP16或BF16)进行，优化器状态更新在32位精度(FP32)进行。而模型推理的要求就比较低，可以在16位、8位甚至4位精度进行，整个任务的计算效率非常高。\n">
<meta property='og:url' content='https://blog.liu-nian.top/ai/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86%E8%B5%84%E6%BA%90%E4%B8%8E%E8%AE%A1%E7%AE%97%E9%87%8F%E8%AF%84%E4%BC%B0/'>
<meta property='og:site_name' content='流年'>
<meta property='og:type' content='article'><meta property='article:section' content='Ai' /><meta property='article:published_time' content='2025-02-19T08:38:25&#43;00:00'/><meta property='article:modified_time' content='2025-02-19T08:38:25&#43;00:00'/>
<meta name="twitter:title" content="大模型训练推理资源与计算量评估">
<meta name="twitter:description" content="计算量评估 前馈神经网络计算量 前馈神经网络：Y = XW +B\n在这个过程中，完成的运算如下：\n$$ \\begin{bmatrix} x_0 &amp; x_1 &amp; x_2 \\end{bmatrix} \\times \\begin{bmatrix} w_{00} &amp; w_{01} \\ w_{10} &amp; w_{11} \\ w_{20} &amp; w_{21} \\end{bmatrix} + \\begin{bmatrix} b_0 \\ b_1 \\end{bmatrix} $$\n其中，W是权重矩阵，X是输入向量，B是偏置项。即：\n$$ y = \\begin{bmatrix} y_0 \\ y_1 \\end{bmatrix} \\begin{matrix} y_0 = w_{00}x_0 + w_{01}x_1 + b_0\\ y_1 = w_{10}x_0 + w_{11}x_1 + b_1\\\n\\end{matrix} $$\n上述2个公式中，每个公式都有3个加法、3个乘法计算。所以[1,3]的矩阵和[3,2]的矩阵相乘共包含 2132 =12 次浮点计算量(称为FLOPs)。从而推广到一般：[m,k]的矩阵和[k,n]的矩阵相乘，需要2mkn FLOPs。而GPU在不同精度单位下，浮点数运算的效率是不同的。例如，A100在16位精度(FP16或BF16)下每秒可以进行$31410^{12}$次浮点运算。而在32位精度下，每秒只能进行$15610^{12}$次浮点运算。所以，我们可以理所当然的想到一个提速思路 - 让所有运算在低精度下进行来提高训练/推理效率。但是，这里有个无法忽略的问题：某些计算必须要在高精度下进行。例如全程在低精度下进行模型训练，往往会出现由于精度不够导致数值溢出和无法收敛的问题。所以目前大模型训练通常采用的是混合精度方式进行：前向传播、反向传播这些无需高精度的计算采用16位精度(FP16或BF16)进行，优化器状态更新在32位精度(FP32)进行。而模型推理的要求就比较低，可以在16位、8位甚至4位精度进行，整个任务的计算效率非常高。\n">
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="切换菜单">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/logo_hu_fd3008965b65babe.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">🍥</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">流年</a></h1>
            <h2 class="site-description">这个人很懒，暂时啥都没写</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://github.com/Williamyzd'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://www.zhihu.com/people/william-19-37'
                        target="_blank"
                        title="知乎"
                        rel="me"
                    >
                        
                        
                            <?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1738662787698" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="10144" xmlns:xlink="http://www.w3.org/1999/xlink" width="48" height="48"><path d="M526.677333 766.421333l-72.021333 45.824-90.922667-142.933333c-18.773333 59.818667-50.005333 113.706667-91.264 163.2-17.152 20.608-34.986667 39.168-55.509333 58.666667-6.613333 6.272-33.066667 30.592-37.461333 34.986666l-60.330667-60.330666c5.930667-5.930667 33.578667-31.36 39.04-36.522667 18.346667-17.408 33.92-33.706667 48.725333-51.456 54.016-64.768 86.613333-136.96 91.178667-223.189333H128v-85.333334h170.666667V298.666667h-37.034667c-29.397333 54.016-66.474667 94.805333-111.701333 121.898666L106.069333 347.434667c59.52-35.754667 103.466667-111.104 129.621334-228.693334l83.285333 18.517334c-5.973333 27.008-12.928 52.352-20.864 76.074666H490.666667v85.333334H384v170.666666h106.666667v85.333334H391.893333l134.784 211.754666z m163.754667-2.986666L738.048 725.333333H810.666667V298.666667h-170.666667v426.666666h31.402667l19.029333 38.101334zM554.666667 213.333333h341.333333v597.333334h-128l-106.666667 85.333333-42.666666-85.333333H554.666667V213.333333z" p-id="10145" fill="#8a8a8a"></path></svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://mp.weixin.qq.com/s/YOHKU850wcmPfWCjeAZ7zw'
                        target="_blank"
                        title="wechat"
                        rel="me"
                    >
                        
                        
                            <?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1738662705411" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="9050" xmlns:xlink="http://www.w3.org/1999/xlink" width="48" height="48"><path d="M767.818667 409.173333C867.338667 444.266667 938.666667 539.136 938.666667 650.666667c0 42.709333-10.496 83.978667-30.261334 120.842666-1.792 3.338667-4.992 8.928-9.696 16.96l14.613334 53.557334c6.506667 23.893333-15.402667 45.813333-39.296 39.296l-53.642667-14.634667-6.229333 3.669333A254.933333 254.933333 0 0 1 682.666667 906.666667c-77.994667 0-147.84-34.88-194.805334-89.888a352.608 352.608 0 0 1-56.64 4.554666c-63.338667 0-124.266667-16.853333-177.472-48.298666-1.834667-1.088-6.410667-3.733333-13.632-7.893334l-80.544 21.653334c-23.914667 6.432-45.76-15.573333-39.146666-39.434667l21.792-78.752a961.205333 961.205333 0 0 1-15.904-27.317333A336.384 336.384 0 0 1 85.333333 480c0-188.618667 154.965333-341.333333 345.888-341.333333 159.914667 0 297.984 108.010667 335.818667 259.296 0.949333 3.765333 1.173333 7.552 0.778667 11.2z m-68.106667-13.952C662.88 282.037333 555.178667 202.666667 431.221333 202.666667 275.434667 202.666667 149.333333 326.933333 149.333333 480c0 46.272 11.498667 90.837333 33.194667 130.698667 2.88 5.290667 10.176 17.706667 21.621333 36.746666a32 32 0 0 1 3.413334 25.013334l-10.517334 37.994666 39.232-10.549333a32 32 0 0 1 24.234667 3.146667c14.272 8.192 22.773333 13.098667 25.802667 14.890666A283.882667 283.882667 0 0 0 431.221333 757.333333c6.154667 0 12.288-0.192 18.389334-0.576A255.061333 255.061333 0 0 1 426.666667 650.666667c0-141.386667 114.613333-256 256-256 5.728 0 11.413333 0.192 17.045333 0.554666z m133.706667 397.056a32 32 0 0 1 3.338666-24.725333 996.672 996.672 0 0 0 15.242667-26.293333A190.997333 190.997333 0 0 0 874.666667 650.666667c0-106.037333-85.962667-192-192-192s-192 85.962667-192 192 85.962667 192 192 192a190.933333 190.933333 0 0 0 98.570666-27.2c2.208-1.322667 8.288-4.874667 18.517334-10.837334a32 32 0 0 1 24.522666-3.210666l12.565334 3.424-3.424-12.565334zM330.666667 426.666667a42.666667 42.666667 0 1 1 0-85.333334 42.666667 42.666667 0 0 1 0 85.333334z m192 0a42.666667 42.666667 0 1 1 0-85.333334 42.666667 42.666667 0 0 1 0 85.333334z m85.333333 202.666666a32 32 0 1 1 0-64 32 32 0 0 1 0 64z m149.333333 0a32 32 0 1 1 0-64 32 32 0 0 1 0 64z" fill="#8a8a8a" p-id="9051"></path></svg>
                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/%E5%85%B3%E4%BA%8E/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>关于</span>
            </a>
        </li>
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>主页</span>
            </a>
        </li>
        
        
        <li >
            <a href='/ai' >
                
                
                
                    <?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1738342212304" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4225" width="48" height="48" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M501.824 32C303.552 32 141.504 176.992 141.504 357.76c0 23.712 2.816 47.104 8.32 69.856l-51.008 114.208a32 32 0 0 0 24.704 44.736c54.272 7.744 76.672 31.168 76.672 77.312v111.552a64 64 0 0 0 64 64h20.704a64 64 0 0 1 64 64V960a32 32 0 0 0 32 32h345.6a32 32 0 0 0 0-64h-313.6v-24.608a128 128 0 0 0-128-128h-20.736v-111.552c0-65.664-32.192-110.688-91.2-131.136l39.872-89.28a31.968 31.968 0 0 0 1.568-21.792 233.088 233.088 0 0 1-8.896-63.904c0-143.712 131.936-261.76 296.32-261.76s296.32 118.016 296.32 261.76a32 32 0 0 0 64 0C862.144 176.992 700.064 32 501.824 32zM904 448a32 32 0 0 0-32 32v360a32 32 0 0 0 64 0V480a32 32 0 0 0-32-32z" p-id="4226"></path><path d="M673.888 466.656c-11.744-25.568-48.416-24.64-58.816 1.536l-132.8 333.76a32 32 0 0 0 59.488 23.68l32.608-81.92c0.576 0.032 1.088 0.32 1.664 0.32h154.848l38.176 83.104a31.968 31.968 0 1 0 58.144-26.72l-153.312-333.76zM599.68 680l47.264-118.72 54.528 118.72H599.68z" p-id="4227"></path></svg>
                
                <span>AI</span>
            </a>
        </li>
        
        
        <li >
            <a href='/cs' >
                
                
                
                    <?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1738342654057" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="9507" xmlns:xlink="http://www.w3.org/1999/xlink" width="48" height="48"><path d="M954.1888 786.3552h-332.032v178.176h130.8416c16.0256 0 29.4912 12.928 29.4912 29.4656 0 16.5632-12.928 30.0032-29.4912 30.0032H271.0016a29.4912 29.4912 0 0 1-29.4912-29.4912c0-16.5376 12.928-29.9776 29.4912-29.9776h130.3296v-178.176H69.8112A69.8368 69.8368 0 0 1 0 716.544V69.8112A70.1696 70.1696 0 0 1 69.8112 0h884.3776A69.8368 69.8368 0 0 1 1024 69.8112V716.544a69.8368 69.8368 0 0 1-69.8112 69.8112z m-492.8768 178.176h100.864v-178.176h-100.864v178.176zM964.5312 59.4688H59.4688v667.4176h905.0624V59.4688zM175.3344 364.8512l107.5456-108.0832c5.7088-5.6832 12.9536-8.7808 21.2224-8.7808 7.7568 0 15.5136 3.0976 21.1968 8.7808 11.904 11.392 11.3664 30.5152 0 41.8816l-92.0576 92.0832 92.0576 92.032a30.1568 30.1568 0 0 1-0.512 42.4192 28.16 28.16 0 0 1-20.6848 8.7808c-7.7568 0-15.0016-3.0976-20.6848-8.7808l-107.0592-107.0592-1.024-1.024a59.52 59.52 0 0 1-5.7088-4.6592 29.2864 29.2864 0 0 1-8.7808-21.1968v-1.0496c0-8.2688 3.0976-15.5136 8.7808-21.1968a23.3472 23.3472 0 0 1 5.7088-4.1472z m391.4752-112.7424a30.9248 30.9248 0 0 1 25.8816-15.488c5.1712 0 10.3424 1.536 14.976 4.1216 7.2448 4.1216 11.904 10.3424 13.9776 18.0992 2.0736 7.7568 1.024 16.0256-3.0976 22.7584l-160.8448 278.2464a30.1312 30.1312 0 0 1-40.3456 10.8544 29.2352 29.2352 0 0 1-11.3664-40.3456l160.8192-278.2464z m123.1104 41.9072c0-8.2944 3.0976-15.5136 8.7808-21.2224 5.6832-5.6832 13.44-8.7808 21.1968-8.7808 7.7568 0 15.0016 3.0976 20.6848 8.7808l107.0592 107.0592 1.0496 0.512c2.048 1.0496 4.1216 2.5856 5.6832 4.1472 5.6832 5.6832 8.7808 13.44 8.7808 21.1968v3.1232c-0.512 7.2448-3.6096 13.952-8.7808 19.1232-1.5616 1.536-3.0976 2.5856-5.1712 5.6832l-108.1088 108.0832a30.08 30.08 0 0 1-21.1968 8.8064c-8.2688 0-15.5136-3.0976-21.1968-8.8064a30.1568 30.1568 0 0 1-8.7808-21.1968c0-8.2688 3.0976-15.5136 8.7808-21.1968l92.0576-92.0576-92.0576-92.0576a30.1568 30.1568 0 0 1-8.7808-21.1968z" fill="#7C848E" p-id="9508"></path></svg>
                
                <span>CS基础</span>
            </a>
        </li>
        
        
        <li >
            <a href='/ops' >
                
                
                
                    <?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1738342768989" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="11403" xmlns:xlink="http://www.w3.org/1999/xlink" width="48" height="48"><path d="M685.1 727.2h4.1c77.5-19 135.9-69.3 150.9-133.2 8.2-36.7 1.4-66.6-1.4-77.5-17.7-69.3-77.5-119.6-150.9-126.4h-19c-32.6-59.8-95.2-96.5-163.1-96.5-91.1 0-168.6 66.6-183.5 153.6-73.4 4.1-130.5 63.9-130.5 138.7 0 77.5 63.9 140 141.4 140h334.4M656.4 672H346.7c-51.6 0-93.2-41.6-93.2-91.9 0-51.6 41.6-93.2 93.2-93.2h2.5l20.1 1.3 1.3-18.9c5-70.5 64.2-124.7 134.7-124.7 52.9 0 100.7 31.5 123.4 79.3l6.3 12.6 13.9-1.3c7.6-1.3 13.9-1.3 22.7-1.3 51.6 5 93.2 40.3 105.8 89.4 2.5 7.6 7.6 29 1.3 54.1-10.1 44.1-52.9 79.3-108.3 94.4" fill="#444444" p-id="11404"></path><path d="M949.7 475c-13.3 0-25-9.7-27.1-23.3C908 357.5 860 271 787.4 208.2c-35.9-31.1-76.6-55.3-120.8-72.1-45.8-17.4-94-26.2-143.4-26.2-136.4 0-262.7 68.1-337.8 182.3-8.4 12.7-25.4 16.2-38.1 7.9-12.7-8.4-16.2-25.4-7.9-38.1C224.7 132.4 368.2 55 523.2 55c56.1 0 110.9 10 162.9 29.8 50.2 19.1 96.4 46.6 137.2 81.9 82.4 71.3 137 169.5 153.6 276.7 2.3 15-8 29.1-23 31.4-1.4 0.1-2.8 0.2-4.2 0.2zM523.2 973.2c-62 0-122.1-12.1-178.7-36.1-54.7-23.1-103.8-56.2-145.9-98.4-42.2-42.2-75.3-91.3-98.4-145.9-24-56.6-36.1-116.8-36.1-178.7 0-15.5 0.8-31.2 2.3-46.6 1.5-15.1 15-26.1 30.1-24.6 15.1 1.5 26.1 15 24.6 30.1-1.4 13.6-2.1 27.4-2.1 41.1 0 54.6 10.7 107.5 31.7 157.3 20.4 48.1 49.5 91.3 86.6 128.5 37.1 37.1 80.3 66.3 128.5 86.6 49.8 21.1 102.7 31.7 157.3 31.7 86.1 0 168.4-26.7 237.7-77.3 33.4-24.4 62.8-53.6 87.4-86.8 24.9-33.6 44.3-70.7 57.6-110.1 4.9-14.4 20.5-22.1 34.9-17.2 14.4 4.9 22.1 20.5 17.2 34.9-15.2 44.8-37.3 86.9-65.5 125.1-27.9 37.7-61.3 70.9-99.3 98.6-78.6 57.5-172 87.8-269.9 87.8z" fill="#444444" p-id="11405"></path><path d="M75.5 405.1l-56.6 98c-6.2 10.8 1.6 24.2 14 24.2H146c12.4 0 20.2-13.5 14-24.2l-56.6-98c-6.2-10.7-21.7-10.7-27.9 0zM952.3 501.2l54.4-94.2c6.7-11.6-1.7-26.1-15.1-26.1H882.8c-13.4 0-21.8 14.5-15.1 26.1l54.4 94.2c6.8 11.6 23.5 11.6 30.2 0z" fill="#444444" p-id="11406"></path></svg>
                
                <span>运维&amp;工具</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        
        <li >
            <a href='/links/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>Links</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>暗色模式</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">目录</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#计算量评估">计算量评估</a>
      <ol>
        <li>
          <ol>
            <li><a href="#前馈神经网络计算量">前馈神经网络计算量</a></li>
          </ol>
        </li>
        <li><a href="#模型计算量分析">模型计算量分析</a></li>
        <li><a href="#transformer块中每个transformer块都由多头注意力结构和mlp两种结构构成">transformer块中，每个transformer块都由多头注意力结构和MLP两种结构构成：</a>
          <ol>
            <li><a href="#多头注意力机制所需计算量">多头注意力机制所需计算量：</a></li>
          </ol>
        </li>
        <li><a href="#mlp层所需计算量">MLP层所需计算量</a></li>
        <li><a href="#其他结构所需计算量">其他结构所需计算量</a></li>
        <li><a href="#模型训练时间估算">模型训练时间估算</a></li>
        <li><a href="#一显存需求计算公式"><strong>一、显存需求计算公式</strong></a></li>
        <li><a href="#二典型场景硬件需求"><strong>二、典型场景硬件需求</strong></a></li>
        <li><a href="#三关键影响因素"><strong>三、关键影响因素</strong></a></li>
        <li><a href="#四实战示例7b模型--qlora"><strong>四、实战示例（7B模型 + QLoRA）</strong></a></li>
        <li><a href="#五资源估算工具"><strong>五、资源估算工具</strong></a>
          <ol>
            <li><a href="#显存计算">显存计算</a></li>
          </ol>
        </li>
      </ol>
    </li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/ai/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86%E8%B5%84%E6%BA%90%E4%B8%8E%E8%AE%A1%E7%AE%97%E9%87%8F%E8%AF%84%E4%BC%B0/">大模型训练推理资源与计算量评估</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Feb 19, 2025</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    阅读时长: 8 分钟
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h2 id="计算量评估">计算量评估
</h2><h4 id="前馈神经网络计算量">前馈神经网络计算量
</h4><p>前馈神经网络：Y = XW +B</p>
<img width="200" height="auto" src="https://cdn.jsdelivr.net/gh/Williamyzd/pub-imgs@main/2025-02/d1bf250b40fa27686da6046c2e7ef5df7570826e956971577473587fe83bdb02.png" >
<p>在这个过程中，完成的运算如下：</p>
<p>$$
\begin{bmatrix}
x_0 &amp; x_1 &amp; x_2
\end{bmatrix}
\times
\begin{bmatrix}
w_{00} &amp; w_{01} \
w_{10} &amp; w_{11} \
w_{20} &amp; w_{21}
\end{bmatrix} +
\begin{bmatrix}
b_0 \
b_1
\end{bmatrix}
$$</p>
<p>其中，W是权重矩阵，X是输入向量，B是偏置项。即：</p>
<p>$$
y = \begin{bmatrix}
y_0 \
y_1
\end{bmatrix}
\begin{matrix}
y_0 = w_{00}x_0 + w_{01}x_1 + b_0\
y_1 = w_{10}x_0 + w_{11}x_1 + b_1\<br>
\end{matrix}
$$</p>
<p>上述2个公式中，每个公式都有3个加法、3个乘法计算。所以[1,3]的矩阵和[3,2]的矩阵相乘共包含 2<em>1</em>3<em>2 =12 次浮点计算量(称为FLOPs)。从而推广到一般：[m,k]的矩阵和[k,n]的矩阵相乘，需要2</em>m<em>k</em>n FLOPs。而GPU在不同精度单位下，浮点数运算的效率是不同的。例如，A100在16位精度(FP16或BF16)下每秒可以进行$314<em>10^{12}$次浮点运算。而在32位精度下，每秒只能进行$156</em>10^{12}$次浮点运算。所以，我们可以理所当然的想到一个提速思路 - 让所有运算在低精度下进行来提高训练/推理效率。但是，这里有个无法忽略的问题：某些计算必须要在高精度下进行。例如全程在低精度下进行模型训练，往往会出现由于精度不够导致数值溢出和无法收敛的问题。所以目前大模型训练通常采用的是混合精度方式进行：前向传播、反向传播这些无需高精度的计算采用16位精度(FP16或BF16)进行，优化器状态更新在32位精度(FP32)进行。而模型推理的要求就比较低，可以在16位、8位甚至4位精度进行，整个任务的计算效率非常高。</p>
<h3 id="模型计算量分析">模型计算量分析
</h3><p>$$
训练计算量 = F_{前向传播} + F_{反向传播}\
推理计算量 = F_{前向传播} \
F_{反向传播} = 2*F_{前向传播}
$$</p>
<p>接下来进行$F_{前向传播}$的计算推导：</p>
<p>先确定符号：我们令B对应batch size，s对应sequence length，h对应hidden dimension，l对应layers number；接着，我们再来观察大模型结构：大模型本身由l个transformer块串联而成。此外，首transformer块前和尾transformer块后还有一些结构。所以大模型的算力估算可以分为两大块：一、l个transformer块中的FLOPs；二、其他结构中的FLOPs：</p>
<h3 id="transformer块中每个transformer块都由多头注意力结构和mlp两种结构构成">transformer块中，每个transformer块都由多头注意力结构和MLP两种结构构成：
</h3><img width="200" height="auto" src="https://cdn.jsdelivr.net/gh/Williamyzd/pub-imgs@main/2025-02/e4470333445feb151d73a37d79377254174c96c1240b57b1a0c883e24b551025.png">
<h4 id="多头注意力机制所需计算量">多头注意力机制所需计算量：
</h4><p><img src="https://cdn.jsdelivr.net/gh/Williamyzd/pub-imgs@main/2025-02/250da86b2e1b6fb734121a17b5125e55f7d9f8eac87875c3ac6ba3654b8827da.png"
	
	
	
	loading="lazy"
	
		alt="250da86b2e1b6fb734121a17b5125e55f7d9f8eac87875c3ac6ba3654b8827da"
	
	
></p>
<p><strong>QKV计算量：</strong></p>
<p>$$
Q = XW^Q, K=XW^K, V=XW^V, 其中：
x \in [B,s,h], W^Q\in[h,h/a]
$$</p>
<p>涉及计算量大致如下：</p>
<p>$$
3<em>2</em>B<em>s</em>h<em>h/a</em>a=6Bsh^2,其中a是多头注意力机制的head数。
$$</p>
<p><strong>注意力计算量</strong>：<code>&lt;br&gt;</code></p>
<ol>
<li>注意力分数计算
单头注意力分数一般采用如下计算：</li>
</ol>
<p>$$
attention = QK^T / \sqrt(d_k)
$$</p>
<p>计算量为：</p>
<p>$$
2B<em>s</em>h/a*s=2Bs^2h/a,其中Q \in [B,s,h/a], K^T \in[B,h/a,s]
$$</p>
<p>因此多头注意力分数整体计算量：</p>
<p>$$
2Bs^2h/a*a=2Bsh^2
$$</p>
<ol>
<li>注意力输出层计算量：
单头注意力输出层计算量：
计算公式为：</li>
</ol>
<p>$$
Z= attention \times V, 其中attention \in [B,s,s], V\in[B,s,h/a]
$$</p>
<p>计算量为：</p>
<p>$$
2B<em>s</em>s*h/a=2Bs^2h/a
$$</p>
<p>因此多头注意力输出层计算量为：$2Bs^2h/a*a=2Bs^2h$</p>
<p><strong>多头注意力合并层计算量</strong><code>&lt;br&gt;</code>
计算公式为：</p>
<p>$$
y = W^OZ + B \
其中,Z\in[B,s,h], W^O \in [h, h], Z为多头注意力输出层结果拼接
$$</p>
<p>计算量为：</p>
<p>$$
2B<em>s</em>h*h=2Bsh^2
$$</p>
<p>因此整个自注意力层的计算量为：</p>
<p>$$
4Bs^2h+8Bsh^2
$$</p>
<h3 id="mlp层所需计算量">MLP层所需计算量
</h3><ul>
<li>激活值和第一个线性层相乘：$[B,s,h]*[h,4h] 共8Bsh^2$ FLOPs；</li>
<li>激活值和第二个线性层相乘：$[B,s,4h]*[4h,h] 共8Bsh^2$ FLOPs；</li>
<li>激活层为约0FLOPs：GeLU是确定的数学缩放公式，可认为无需矩阵乘；</li>
</ul>
<p>单个MLP共有包含$16Bsh^2$$ FLOPs。<code>&lt;br&gt;</code>
因此，l个transformer块的FLOPs为：$24Bsh^2+4Bs^2h$，所有transformer块的FLOPs为：$l(24Bsh^2+4Bs^2h)$。</p>
<h3 id="其他结构所需计算量">其他结构所需计算量
</h3><p>主要是最终输出层，计算公式为：</p>
<p>$$
y = XW^O + B, 其中X\in[B,s,h], W^O \in [h,V], V为字典大小
$$</p>
<p>其计算量大小为：</p>
<p>$$
2B<em>s</em>h*V=2BsVh
$$</p>
<p>综上，整个模型的前向传播计算量为：</p>
<p>$$
F_{前向传播} = l(24Bsh^2+4Bs^2h)+2BsVh \
= 24Bsh^2l+4Bs^2hl+2BsVh \
= 24Bsh^2l(1+\frac{s}{6h}+\frac{V}{12lh})
$$</p>
<p>而通常在大模型中，6h远大于s，12lh远大于V，所以还可以进一步简化：</p>
<p>$$
F_{前向传播} = 24Bsh^2l
$$</p>
<p>因此可推断，训练与推理所需的计算量为：</p>
<p>$$
F_{训练} = 3F_{前向传播} = 72Bsh^2l \
F_{训练} = 4F_{前向传播} = 96Bsh^2l,全量参数计算时 \
F_{推理} = F_{前向传播} = 24Bsh^2l
$$</p>
<p>粗略估算一般基于transformer的大模型的参数量为$12lh^2$,可以发现：
1）模型训练中：如果没有应用激活值重计算，单模型副本处理每个tokens时，单参数上算力需求约为6FLOPs($\frac{72Bsh^2l}{12lh^2<em>B</em>s}$)。如果应用全激活值重计算，提升至8FLOPs；
2）模型推理中：单模型副本处理每个tokens时，单参数上算力需求约为6FLOPs；</p>
<p><img src="https://cdn.jsdelivr.net/gh/Williamyzd/pub-imgs@main/2025-02/b7530fd66541fc9ea2132a8dd205a7011dda6210ab3ac10ac39d42ed4cc6c04c.png"
	
	
	
	loading="lazy"
	
		alt="b7530fd66541fc9ea2132a8dd205a7011dda6210ab3ac10ac39d42ed4cc6c04c"
	
	
></p>
<p>最常用来测量每秒浮点运算次数的基准程序（benchmark）之一，就是Linpack。
一个MFLOPS（megaFLOPS）等于每秒一百万（=10^6）次的浮点运算，
一个GFLOPS（gigaFLOPS）等于每秒十亿（=10^9）次的浮点运算，
一个TFLOPS（teraFLOPS）等于每秒一万亿（=10^12）次的浮点运算，
一个PFLOPS（petaFLOPS）等于每秒一千万亿（=10^15）次的浮点运算，
一个EFLOPS（exaFLOPS）等于每秒一百亿亿（=10^18）次的浮点运算。</p>
<h3 id="模型训练时间估算">模型训练时间估算
</h3><p>根据阿姆达尔定律，由于整个系统中有网络通信等无法并行加速项的存在，集群中存在加速比上限。通常，训练集群中的GPU利用率通常约30%～55%之间。集群训练耗时计算公式如下：
<img src="https://cdn.jsdelivr.net/gh/Williamyzd/pub-imgs@main/2025-02/c3b91e8f5a779118ff3f119f3152c5aec3f0bc54e4712e757a9e828bdd53d74b.png"
	
	
	
	loading="lazy"
	
		alt="c3b91e8f5a779118ff3f119f3152c5aec3f0bc54e4712e757a9e828bdd53d74b"
	
	
></p>
<p><a class="link" href="https://blog.csdn.net/weixin_43925843/article/details/145626893"  target="_blank" rel="noopener"
    >https://blog.csdn.net/weixin_43925843/article/details/145626893</a>
<a class="link" href="https://www.nvidia.cn/data-center/v100/"  target="_blank" rel="noopener"
    >https://www.nvidia.cn/data-center/v100/</a>
<img src="https://cdn.jsdelivr.net/gh/Williamyzd/pub-imgs@main/2025-02/f048f9cb18ca409edeeb27e838ce9a011cf21bd80c82afa9ff3ca00fa33136ee.png"
	
	
	
	loading="lazy"
	
		alt="f048f9cb18ca409edeeb27e838ce9a011cf21bd80c82afa9ff3ca00fa33136ee"
	
	
>
<a class="link" href="https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md#%E8%BD%AF%E7%A1%AC%E4%BB%B6%E4%BE%9D%E8%B5%96"  target="_blank" rel="noopener"
    >https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md#%E8%BD%AF%E7%A1%AC%E4%BB%B6%E4%BE%9D%E8%B5%96</a></p>
<p>LLaMA Factory 微调所需的硬件资源计算主要基于<strong>模型参数量</strong>、<strong>微调方法</strong>和<strong>训练配置</strong>三个核心维度。以下是具体计算方法与典型场景示例：</p>
<hr>
<h3 id="一显存需求计算公式"><strong>一、显存需求计算公式</strong>
</h3><p>显存占用（GB）主要包含以下部分：</p>
<p>$$
\text{显存} = \text{模型参数显存} + \text{梯度显存} + \text{优化器状态显存} + \text{激活值显存}
$$</p>
<p><strong>1. 模型参数显存</strong></p>
<ul>
<li><strong>全参数微调（FP16）</strong>：参数量 × 2 bytes（例如 7B 模型：7×10⁹ × 2 / 10⁹ = 14GB）</li>
<li><strong>LoRA（FP16）</strong>：仅需存储低秩矩阵（约 0.1%-1% 参数量）（例如 7B 模型：14GB × 1% ≈ 0.14GB）</li>
<li><strong>QLoRA（4-bit）</strong>：参数量 × 0.5 bytes
（例如 7B 模型：7×10⁹ × 0.5 / 10⁹ = 3.5GB）</li>
</ul>
<p><strong>2. 梯度与优化器状态</strong></p>
<ul>
<li><strong>全参数微调（Adam优化器）</strong>：参数量 × 4 bytes（梯度+优化器状态）</li>
<li><strong>LoRA/QLoRA</strong>：仅需存储少量梯度（可忽略）</li>
</ul>
<p><strong>3. 激活值显存</strong>
与批次大小（batch size）和序列长度（seq_len）相关，计算公式：</p>
<p>$$
\text{激活值显存} \approx \text{batch_size} \times \text{seq_len} \times \text{hidden_dim} \times 2 \text{ bytes}
$$</p>
<hr>
<h3 id="二典型场景硬件需求"><strong>二、典型场景硬件需求</strong>
</h3><div class="table-wrapper"><table>
  <thead>
      <tr>
          <th>模型规模</th>
          <th>微调方法</th>
          <th>显存需求（估算）</th>
          <th>推荐 GPU 配置</th>
          <th>数据来源</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>7B</strong></td>
          <td>全参数（FP16）</td>
          <td>60GB</td>
          <td>2×A100 80GB</td>
          <td></td>
      </tr>
      <tr>
          <td><strong>7B</strong></td>
          <td>QLoRA（4-bit）</td>
          <td>6GB</td>
          <td>单卡 RTX 3090/4090</td>
          <td></td>
      </tr>
      <tr>
          <td><strong>13B</strong></td>
          <td>LoRA（FP16）</td>
          <td>12GB</td>
          <td>单卡 A100 40GB</td>
          <td></td>
      </tr>
      <tr>
          <td><strong>70B</strong></td>
          <td>全参数（FP16）</td>
          <td>600GB</td>
          <td>8×H100 80GB（NVLink）</td>
          <td></td>
      </tr>
  </tbody>
</table></div>
<hr>
<h3 id="三关键影响因素"><strong>三、关键影响因素</strong>
</h3><ol>
<li>
<p><strong>微调算法选择</strong></p>
<ul>
<li><strong>全参数微调</strong>：显存需求最高，适合高算力集群。</li>
<li><strong>LoRA/QLoRA</strong>：显存需求降低 50%-90%，适合单卡训练。</li>
<li><strong>GaLore/Freeze</strong>：通过梯度压缩或参数冻结进一步优化显存。</li>
</ul>
</li>
<li>
<p><strong>训练配置优化</strong></p>
<ul>
<li><strong>批次大小（batch_size）</strong>：增大 batch_size 会线性增加激活值显存。</li>
<li><strong>梯度累积（gradient_accumulation）</strong>：通过累积梯度减少单步显存需求。</li>
<li><strong>量化与混合精度</strong>：4-bit/8-bit 量化可降低显存 30%-70%。</li>
</ul>
</li>
<li>
<p><strong>分布式训练支持</strong>
LLaMA Factory 支持 DeepSpeed ZeRO 和模型并行，可将大模型拆分到多卡。</p>
</li>
</ol>
<hr>
<h3 id="四实战示例7b模型--qlora"><strong>四、实战示例（7B模型 + QLoRA）</strong>
</h3><ul>
<li><strong>硬件配置</strong>：RTX 4090（24GB 显存）</li>
<li><strong>训练命令</strong>：
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">llamafactory-cli train <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --model_name_or_path meta-llama/Llama-3-8b <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --finetuning_type qlora <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --quantization_bit <span class="m">4</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --per_device_train_batch_size <span class="m">4</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --gradient_accumulation_steps <span class="m">8</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li><strong>显存占用</strong>：约 10-12GB（含激活值）。</li>
</ul>
<hr>
<h3 id="五资源估算工具"><strong>五、资源估算工具</strong>
</h3><p>LLaMA Factory 提供显存估算工具，可通过以下命令生成报告：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">llamafactory-cli estimate --model_name_or_path meta-llama/Llama-3-8b --finetuning_type lora
</span></span></code></pre></td></tr></table>
</div>
</div><p>如需更精确的计算，可参考 Hugging Face 的 <a class="link" href="https://huggingface.co/docs/transformers/v4.40.0/en/main_classes/trainer#memory-estimator"  target="_blank" rel="noopener"
    >Transformer 显存计算器</a>。
<a class="link" href="https://mp.weixin.qq.com/s/CJxA-PxF_lvSpMr_7uHVVg"  target="_blank" rel="noopener"
    >https://mp.weixin.qq.com/s/CJxA-PxF_lvSpMr_7uHVVg</a></p>
<p>推理所用显存：
Total GPU Memory = 模型大小 + KV Cache + Memory Overhead
最后还是以 LLaMa-2 13B 来举例。假设有 10 个并发请求，同时请求 LLaMa-2 13B 以最大 Token数(4096) 进行模型推理。 那最终需要的 GPU Memory 计算过程如下：
模型大小= 13 Billion * 2 Bytes = 26 GB</p>
<p>Total KV cache= 800 KB * 4096 Tokens * 10 并发请求 = 32 GB</p>
<p>Memory Overhead= 0.1 * (26 GB + 32 GB) = 5.8 GB</p>
<p>所以最终需要总 GPU memory为: 26 GB + 32 GB + 5.8 GB = 63.8 GB。需要 2 块英伟达的 A100 芯片才可以。
<a class="link" href="https://github.com/manuelescobar-dev/LLM-Tools"  target="_blank" rel="noopener"
    >https://github.com/manuelescobar-dev/LLM-Tools</a></p>
<p><a class="link" href="https://medium.com/@manuelescobar-dev/memory-requirements-for-llm-training-and-inference-97e4ab08091b"  target="_blank" rel="noopener"
    >https://medium.com/@manuelescobar-dev/memory-requirements-for-llm-training-and-inference-97e4ab08091b</a></p>
<h4 id="显存计算">显存计算
</h4><p>推理显存评估：</p>
<p>$$
Total\ Inference\ Memory = Model Size + KV Cache + Activations
$$</p>
<p>模型大小：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Williamyzd/pub-imgs@main/2025-02/e21334b55e5c3e6692bf0d25103d42c561d602963061639d2981a314a86b2838.png"
	
	
	
	loading="lazy"
	
		alt="e21334b55e5c3e6692bf0d25103d42c561d602963061639d2981a314a86b2838"
	
	
></p>
<p>kv cache：</p>
<p>$$
KV\ Cache =2 × Batch Size × Sequence Length × Number of Layers × Hidden Size × Precision
$$</p>
<p>激活值：</p>
<p>$$
Activation Memory = Batch Size × Sequence Length × Hidden Size × ( 34 + 5 × \frac{Sequence Length ×Number of attention heads}{Hidden Size})
$$</p>
<p>推理显存评估：</p>
<p>$$
Total Memory=Model Size+KV Cache+Activations+(Optimizer States+Gradients)× Number of Trainable Parameters
$$</p>
<p>Optimizer States
Optimization algorithms require resources to store the parameters and auxiliary variables. These variables include momentum and variance used by algorithms such as Adam (2 states) or SGD (1 state). The precision and type of optimizer affect memory usage.</p>
<p>Formula [1]</p>
<p>AdamW (2 states): 8 Bytes per parameter
AdamW (bitsandbytes Quantized): 2 Bytes per parameter
SGD (1 state): 4 Bytes per parameter
Gradients
Gradient values are computed during the backward pass of the model. They represent the rate of change of the loss function with respect to each model parameter and are crucial for updating the parameters during optimization. As with activations, they must be stored in FP32 for numerical stability.</p>
<p>Formula [1]</p>
<p>4 Bytes per parameter</p>

</section>


    <footer class="article-footer">
    

    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
	const mainArticleElement = document.querySelector(".main-article");
        renderMathInElement(mainArticleElement, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>

    
</article>

    

    

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2025 流年
    </section>
    
    <section class="powerby">
        使用 <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> 构建 <br />
        主题 <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.30.0">Stack</a></b> 由 <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a> 设计
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>

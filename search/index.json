[{"content":"需求背景 将已有的网络图片地址转换为github镜像仓库的地址，插入markdown文档中\n使用框架 mcp.server.fastmcp\n代码逻辑 创建github仓库并获取对应仓库的token\n判断为网络图片还是本地图片。\n网络图片则首选获取其二进制内容 本地图片直接读取二进制内容 从地址或url中获取文件的格式并保存，如获取不到则默认为png\n获取文件二进制内容的哈希值，作为文件名。\n获取当前年月，按照’年-月/文件hash‘格式上传到代码仓库路径\n上传文件前首先尝试调用githu api获取文件在githu仓库的hash值\n如果上一步的hash存在，则是更新文件，需要在body体中添加’sha‘字段， 如果hash不存在则为新文件 发送文件上传请求，获取返回值\n结果拼接\ndownload_url为文件的直接下载地址 cdn_url用于展示加速，其拼接逻辑为：https://cdn.jsdelivr.net/gh/{repo}@{branch}/{target_path} 基于cdn_url拼接一个markdown格式文件标签 天机mcp协议支持\n实例化mcp: mcp = FastMCP() 使用 @mcp.tool将需要转换的函数工具化 在main函数中实现mcp server,建议使用stdio本地调用方式，相对安全 在客户端实现调用 以 cherry studio为例，分别填下如下内容： 撰写prompt\n1 2 3 4 5 6 7 8 9 任务：调用github upload 工具，将用户输入的图片地址或传入的图片附件传入到github仓库，并将返回的结果以j字符串形式输出。 执行要求： 如果传入的是图片那么将图片地址直接传给工具进行使用，且不需要将图片传给模型。 输出要求： 不展示图片，返回工具的json格式执行结果 /no_think 效果测试 代码全文 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 import argparse import urllib.request import urllib.error import urllib.parse import json import base64 import os import re import hashlib from datetime import datetime from mcp.server.fastmcp import FastMCP mcp = FastMCP() # 内置配置 DEFAULT_REPO = \u0026#34;\u0026#34; # 替换为你的仓库名 DEFAULT_TOKEN = \u0026#34;\u0026#34; # 替换为你的GitHub个人访问令牌 def is_url(path): \u0026#34;\u0026#34;\u0026#34;改进的URL检测函数\u0026#34;\u0026#34;\u0026#34; try: result = urllib.parse.urlparse(path) return all([result.scheme in (\u0026#39;http\u0026#39;, \u0026#39;https\u0026#39;), result.netloc]) except: return False def get_image_content(image_path): \u0026#34;\u0026#34;\u0026#34;改进的内容获取函数\u0026#34;\u0026#34;\u0026#34; try: if is_url(image_path): # 对URL进行安全编码处理 safe_url = urllib.parse.quote(image_path, safe=\u0026#39;:/?\u0026amp;=\u0026#39;) with urllib.request.urlopen(safe_url) as response: return response.read() else: with open(image_path, \u0026#34;rb\u0026#34;) as f: return f.read() except Exception as e: raise ValueError(f\u0026#34;读取失败: {str(e)}\u0026#34;) def get_file_sha(repo, token, branch, path): \u0026#34;\u0026#34;\u0026#34; 获取指定GitHub仓库中文件的SHA值。 Args: repo (str): GitHub仓库的名称，格式为\u0026#39;用户名/仓库名\u0026#39;。 token (str): GitHub的Personal Access Token。 branch (str): 仓库的分支名。 path (str): 文件在仓库中的路径。 Returns: str or None: 文件的SHA值，如果获取失败则返回None。 \u0026#34;\u0026#34;\u0026#34; url = f\u0026#34;https://api.github.com/repos/{repo}/contents/{path}\u0026#34; headers = { \u0026#34;Authorization\u0026#34;: f\u0026#34;token {token}\u0026#34;, \u0026#34;Accept\u0026#34;: \u0026#34;application/vnd.github.v3+json\u0026#34; } params = {\u0026#34;ref\u0026#34;: branch} # 手动拼接参数到URL query_string = urllib.parse.urlencode(params) url_with_params = f\u0026#34;{url}?{query_string}\u0026#34; rs =None req = urllib.request.Request(url_with_params, headers=headers, method=\u0026#34;GET\u0026#34;, data=None) try: with urllib.request.urlopen(req) as response: data = json.loads(response.read().decode(\u0026#39;utf-8\u0026#39;)) rs = data[\u0026#34;sha\u0026#34;] if \u0026#34;sha\u0026#34; in data else None finally: return rs def calculate_sha1(content): \u0026#34;\u0026#34;\u0026#34;计算文件内容的SHA1哈希值\u0026#34;\u0026#34;\u0026#34; sha1 = hashlib.sha1() sha1.update(content) return sha1.hexdigest() @mcp.tool() def upload_file(image_path, repo=DEFAULT_REPO, token=DEFAULT_TOKEN, branch=\u0026#34;main\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Uploads an image file to a specified GitHub repository. Args: image_path (str): The local file path or URL of the image to upload. repo (str, optional): The GitHub repository in the format \u0026#39;owner/repo\u0026#39;. Defaults to DEFAULT_REPO. token (str, optional): The GitHub personal access token for authentication. Defaults to DEFAULT_TOKEN. branch (str, optional): The branch to upload the file to. Defaults to \u0026#34;main\u0026#34;. Returns: dict: The JSON response from the GitHub API after uploading the file. Raises: RuntimeError: If the upload fails or the GitHub API returns an error. \u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;上传图片到GitHub仓库\u0026#34;\u0026#34;\u0026#34; try: content = get_image_content(image_path) # 计算SHA1哈希值 file_hash = calculate_sha1(content) if is_url(image_path): # 尝试从URL中提取扩展名 parsed = urllib.parse.urlparse(image_path) base_name = os.path.basename(parsed.path) _, ext = os.path.splitext(base_name) filename = f\u0026#34;{file_hash}.{ext}\u0026#34; if ext else f\u0026#34;{file_hash}.png\u0026#34; else: # 从本地路径获取扩展名 _, ext = os.path.splitext(image_path) filename = f\u0026#34;{file_hash}.{ext}\u0026#34; if ext else f\u0026#34;{file_hash}.png\u0026#34; now = datetime.now() year_month = now.strftime(\u0026#34;%Y-%m\u0026#34;) target_path = f\u0026#34;{year_month}/{filename}\u0026#34; url = f\u0026#34;https://api.github.com/repos/{repo}/contents/{target_path}\u0026#34; headers = { \u0026#34;Authorization\u0026#34;: f\u0026#34;token {token}\u0026#34;, \u0026#34;Accept\u0026#34;: \u0026#34;application/vnd.github.v3+json\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;Python-urllib\u0026#34; } data = { \u0026#34;message\u0026#34;: f\u0026#34;Add {filename}\u0026#34;, \u0026#34;content\u0026#34;: base64.b64encode(content).decode(\u0026#39;utf-8\u0026#39;), \u0026#34;branch\u0026#34;: branch } current_sha = get_file_sha(repo, token, branch, target_path) # 如果文件不存在，current_sha 可能为 None，需要根据业务逻辑处理 # 例如，可以决定创建一个新文件而不是更新一个不存在的文件 if current_sha : data[\u0026#34;sha\u0026#34;] = current_sha req = urllib.request.Request( url, data=json.dumps(data).encode(\u0026#34;utf-8\u0026#34;), headers=headers, method=\u0026#34;PUT\u0026#34; ) with urllib.request.urlopen(req) as response: rs = json.loads(response.read().decode(\u0026#34;utf-8\u0026#34;)) file_url = rs[\u0026#39;content\u0026#39;][\u0026#39;download_url\u0026#39;] cdn_url= f\u0026#34;https://cdn.jsdelivr.net/gh/{repo}@{branch}/{target_path}\u0026#34; return { \u0026#39;file_url\u0026#39; : file_url, \u0026#39;cdn_url\u0026#39;:cdn_url, \u0026#39;md_cdn\u0026#39;:f\u0026#39;![{filename}]({cdn_url})\u0026#39; } except urllib.error.HTTPError as e: error_msg = e.read().decode(\u0026#39;utf-8\u0026#39;) raise RuntimeError(f\u0026#34;GitHub API错误: {error_msg}\u0026#34;) except Exception as e: raise RuntimeError(f\u0026#34;上传失败: {str(e)}\u0026#34;) def main(): parser = argparse.ArgumentParser(description=\u0026#34;GitHub图片上传工具\u0026#34;) parser.add_argument(\u0026#34;image_path\u0026#34;, help=\u0026#34;要上传的图片路径(本地路径或URL)\u0026#34;) args = parser.parse_args() try: result = upload_file(args.image_path) print(result) print(f\u0026#34;图片上传成功: {result[\u0026#39;content\u0026#39;][\u0026#39;html_url\u0026#39;]}\u0026#34;) except Exception as e: print(f\u0026#34;错误: {str(e)}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: mcp.run(transport=\u0026#39;stdio\u0026#39;) ","date":"2025-06-29T11:38:25Z","permalink":"https://blog.liu-nian.top/ai/mcp-server%E5%AE%9E%E7%8E%B0%E5%9B%BE%E7%89%87%E8%87%AA%E5%8A%A8%E8%BD%AC%E6%8D%A2/","title":"mcp-server实现图片自动转换"},{"content":"1. 准备基础镜像 一般情况，可以参考：[GitHub Action] 自动化创建，vllm构建过程所需的磁盘和内存资源较大，github action提供的免费资源不足（4c16g,14 ssd）会导致镜像构建失败。需要使用自己的服务器\n参考https://github.com/vllm-project/vllm/blob/main/Dockerfile\n可知，所需的基础镜像如下： 如目标cuda版本为12.1.1,则需要的基础镜像为：\nnvidia/cuda:12.1.1-devel-ubuntu20.04\nnvidia/cuda:12.1.1-devel-ubuntu22.04\n查询该镜像是否存在 https://explore.ggcr.dev/\n以上镜像组合式存在的。如果不存在，可在该网站查询相近的镜像组合\n如镜像列表：https://explore.ggcr.dev/?repo=nvidia%2Fcuda\n如找不到目标基础镜像或相邻镜像也不满足要求，可以考虑构建自己的基础镜像，见nvidia/cuda代码库：https://gitlab.com/nvidia/container-images/cuda 。 这里暂不展开\n2. 同步基础镜像到国内镜像仓库（可选，如果具备外网服务器或可访问官方的镜像仓库，可不做） 使用github工作流自动同步：\n所有工作流yaml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # 工作流名称 name: Sync-Images-to-Aliyun-CR # 工作流运行时显示名称 run-name: ${{ github.actor }} is Sync Images to Aliyun CR. # 怎样触发工作流 on: push: branches: [ \u0026#34;main\u0026#34; ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch: # 工作流程任务（通常含有一个或多个步骤） jobs: syncimages: runs-on: ubuntu-latest steps: - name: Checkout Repos uses: actions/checkout@v4 - name: Login to Aliyun CR uses: docker/login-action@v3 with: registry: ${{ vars.ALY_REGISTRY}} username: ${{ secrets.ALY_UNAME }} password: ${{ secrets.ALY_PASSWD }} logout: false - uses: actions/setup-python@v5 with: python-version: \u0026#39;3.10\u0026#39; - run: python scrips/pull_image.py 对应处理镜像代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # -*- coding: utf-8 -*- import os,re,subprocess,datetime,json lines = [] rs = [] ds_reg = \u0026#39;registry.cn-hangzhou.aliyuncs.com/reg_pub/\u0026#39; with open(\u0026#39;images.txt\u0026#39;, \u0026#39;r\u0026#39;) as f: lines = f.read().split(\u0026#39;\\n\u0026#39;) for img in lines: # ims = re.split(\u0026#39;\\s+\u0026#39;, img) # p =None # if len(ims) \u0026gt; 1 and len(ims[1]) \u0026gt; 0: # p = ims[1] if len(img) \u0026gt; 0: ims = img.replace(\u0026#39;/\u0026#39;, \u0026#39;_\u0026#39;) ds = ds_reg + ims cmds = \u0026#39;skopeo copy --all docker://\u0026#39; + img + \u0026#39; docker://\u0026#39; + ds code,rss = subprocess.getstatusoutput(cmds) print(\u0026#39;*\u0026#39;*20+\u0026#39;\\n\u0026#39;+\u0026#34;copy {} to {}\u0026#34;.format(img, ds) + \u0026#39;\\n\u0026#39; + \u0026#34;rs:\\n code:{}\\n rs:{}\u0026#34;.format(code, rss) ) if code != 0: cmds = \u0026#39;docker pull \u0026#39; + img + \u0026#39; \u0026amp;\u0026amp; docker tag \u0026#39; + img + \u0026#39; \u0026#39; + ds + \u0026#39; \u0026amp;\u0026amp; docker push \u0026#39; + ds + \u0026#39; \u0026amp;\u0026amp; docker rmi {} {} \u0026#39;.format(img,ds) code,rss = subprocess.getstatusoutput(cmds) print(\u0026#39;skopeo copy error, try docker pull and push:\u0026#39; + \u0026#34;\\n\u0026#34; + cmds + \u0026#39;\\n\u0026#39; + \u0026#34;rs:\\n code:{}\\n rs:{}\u0026#34;.format(code, rss)) r = { \u0026#39;src\u0026#39;: img, \u0026#39;ds\u0026#39;: ds, \u0026#39;code\u0026#39;: code, \u0026#39;rs\u0026#39;: rss } rs.append(r) ctime =datetime.datetime.now().strftime(\u0026#39;%Y-%m-%d-%H:%M:%S\u0026#39;) with open(\u0026#39;result-{}.json\u0026#39;.format(ctime), encoding=\u0026#39;utf-8\u0026#39;, mode=\u0026#39;w\u0026#39;) as f: json.dump(rs, f) print(r) 镜像同步代码仓库结构： 3. 准备Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 注意确认镜像中的操作系统版本+cuda版本是否存在，查看：https://explore.ggcr.dev/?repo=nvidia%2Fcuda FROM nvidia/cuda:12.1.1-devel-ubuntu22.04 AS vllm-base # cuda版本：vllm目前只支持cuda 12.4、12.1、11.8 其他版本需要自己编译 ARG CUDA_VERSION=121 # vllm版本 ARG VLLM_VERSION=0.8.1 # 防止需要交互卡主打包过程 ENV DEBIAN_FRONTEND=noninteractive # 更新环境 RUN apt update \u0026amp;\u0026amp; \\ apt install -y build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libsqlite3-dev libreadline-dev libffi-dev libbz2-dev liblzma-dev tk-dev wget # 安装python，如需变更python版本这里调整 RUN wget https://www.python.org/ftp/python/3.12.9/Python-3.12.9.tgz \u0026amp;\u0026amp; \\ tar -zxvf Python-3.12.9.tgz \u0026amp;\u0026amp; \\ cd Python-3.12.9 \u0026amp;\u0026amp; \\ ./configure --enable-optimizations --prefix=/usr/local/python3.12 \u0026amp;\u0026amp; make -j$(nproc) \u0026amp;\u0026amp; \\ make altinstall \u0026amp;\u0026amp; \\ ln -s /usr/local/python3.12/bin/python3.12 /usr/bin/python3 \u0026amp;\u0026amp; \\ ln -s /usr/local/python3.12/bin/pip3.12 /usr/bin/pip3 \u0026amp;\u0026amp; \\ ln -s /usr/local/python3.12/bin/pip3.12 /usr/bin/pip \u0026amp;\u0026amp; \\ rm -rf Python-3.12.9.tgz Python-3.12.9 # 设置PATH ENV PATH=\u0026#34;/usr/bin:/usr/local/python3.12/bin:$PATH\u0026#34; # 安装vllm,注意所需版本，如无合适的whl需要考虑自己编译源码 RUN wget https://github.com/vllm-project/vllm/releases/download/v${VLLM_VERSION}/vllm-${VLLM_VERSION}+cu${CUDA_VERSION}-cp38-abi3-manylinux1_x86_64.whl RUN pip install vllm-${VLLM_VERSION}+cu${CUDA_VERSION}-cp38-abi3-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu${CUDA_VERSION} RUN rm -f vllm-${VLLM_VERSION}+cu${CUDA_VERSION}-cp38-abi3-manylinux1_x86_64.whl ENTRYPOINT [\u0026#34;python3\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;vllm.entrypoints.openai.api_server\u0026#34;] ﻿\n4. 打镜像并推送 核心命令如下：\n1 2 3 docker build --tag vllm/vllm-openai . # 如服务器配置较高，可以使用该参数--build-arg max_jobs=8 --build-arg nvcc_threads=2 或针对性调整，如资一般，不建议使用 docker tag vllm/vllm-openai registry.cn-hangzhou.aliyuncs.com/reg_pub/vllm_vllm-openai:cuda-12.1.1-20250320 docker push registry.cn-hangzhou.aliyuncs.com/reg_pub/vllm_vllm-openai:cuda-12.1.1-20250320 ﻿\n﻿\n﻿\n","date":"2025-03-11T08:38:25Z","permalink":"https://blog.liu-nian.top/ai/%E6%9E%84%E5%BB%BA%E6%8C%87%E5%AE%9Acuda%E7%89%88%E6%9C%AC%E7%9A%84vllm%E9%95%9C%E5%83%8F/","title":"构建指定cuda版本的vllm镜像"},{"content":"计算量评估 前馈神经网络计算量 前馈神经网络：Y = XW +B\n在这个过程中，完成的运算如下：\n$$ \\begin{bmatrix} x_0 \u0026amp; x_1 \u0026amp; x_2 \\end{bmatrix} \\times \\begin{bmatrix} w_{00} \u0026amp; w_{01} \\ w_{10} \u0026amp; w_{11} \\ w_{20} \u0026amp; w_{21} \\end{bmatrix} + \\begin{bmatrix} b_0 \\ b_1 \\end{bmatrix} $$\n其中，W是权重矩阵，X是输入向量，B是偏置项。即：\n$$ y = \\begin{bmatrix} y_0 \\ y_1 \\end{bmatrix} \\begin{matrix} y_0 = w_{00}x_0 + w_{01}x_1 + b_0\\ y_1 = w_{10}x_0 + w_{11}x_1 + b_1\\\n\\end{matrix} $$\n上述2个公式中，每个公式都有3个加法、3个乘法计算。所以[1,3]的矩阵和[3,2]的矩阵相乘共包含 2132 =12 次浮点计算量(称为FLOPs)。从而推广到一般：[m,k]的矩阵和[k,n]的矩阵相乘，需要2mkn FLOPs。而GPU在不同精度单位下，浮点数运算的效率是不同的。例如，A100在16位精度(FP16或BF16)下每秒可以进行$31410^{12}$次浮点运算。而在32位精度下，每秒只能进行$15610^{12}$次浮点运算。所以，我们可以理所当然的想到一个提速思路 - 让所有运算在低精度下进行来提高训练/推理效率。但是，这里有个无法忽略的问题：某些计算必须要在高精度下进行。例如全程在低精度下进行模型训练，往往会出现由于精度不够导致数值溢出和无法收敛的问题。所以目前大模型训练通常采用的是混合精度方式进行：前向传播、反向传播这些无需高精度的计算采用16位精度(FP16或BF16)进行，优化器状态更新在32位精度(FP32)进行。而模型推理的要求就比较低，可以在16位、8位甚至4位精度进行，整个任务的计算效率非常高。\n模型计算量分析 $$ 训练计算量 = F_{前向传播} + F_{反向传播}\\ 推理计算量 = F_{前向传播} \\ F_{反向传播} = 2*F_{前向传播} $$\n接下来进行$F_{前向传播}$的计算推导：\n先确定符号：我们令B对应batch size，s对应sequence length，h对应hidden dimension，l对应layers number；接着，我们再来观察大模型结构：大模型本身由l个transformer块串联而成。此外，首transformer块前和尾transformer块后还有一些结构。所以大模型的算力估算可以分为两大块：一、l个transformer块中的FLOPs；二、其他结构中的FLOPs：\ntransformer块中，每个transformer块都由多头注意力结构和MLP两种结构构成： 多头注意力机制所需计算量： QKV计算量：\n$$ Q = XW^Q, K=XW^K, V=XW^V, 其中： x \\in [B,s,h], W^Q\\in[h,h/a] $$\n涉及计算量大致如下：\n$$ 32Bshh/aa=6Bsh^2,其中a是多头注意力机制的head数。 $$\n注意力计算量：\u0026lt;br\u0026gt;\n注意力分数计算 单头注意力分数一般采用如下计算： $$ attention = QK^T / \\sqrt(d_k) $$\n计算量为：\n$$ 2Bsh/a*s=2Bs^2h/a,其中Q \\in [B,s,h/a], K^T \\in[B,h/a,s] $$\n因此多头注意力分数整体计算量：\n$$ 2Bs^2h/a*a=2Bsh^2 $$\n注意力输出层计算量： 单头注意力输出层计算量： 计算公式为： $$ Z= attention \\times V, 其中attention \\in [B,s,s], V\\in[B,s,h/a] $$\n计算量为：\n$$ 2Bss*h/a=2Bs^2h/a $$\n因此多头注意力输出层计算量为：$2Bs^2h/a*a=2Bs^2h$\n多头注意力合并层计算量\u0026lt;br\u0026gt; 计算公式为：\n$$ y = W^OZ + B \\ 其中,Z\\in[B,s,h], W^O \\in [h, h], Z为多头注意力输出层结果拼接 $$\n计算量为：\n$$ 2Bsh*h=2Bsh^2 $$\n因此整个自注意力层的计算量为：\n$$ 4Bs^2h+8Bsh^2 $$\nMLP层所需计算量 激活值和第一个线性层相乘：$[B,s,h]*[h,4h] 共8Bsh^2$ FLOPs； 激活值和第二个线性层相乘：$[B,s,4h]*[4h,h] 共8Bsh^2$ FLOPs； 激活层为约0FLOPs：GeLU是确定的数学缩放公式，可认为无需矩阵乘； 单个MLP共有包含$16Bsh^2$$ FLOPs。\u0026lt;br\u0026gt; 因此，l个transformer块的FLOPs为：$24Bsh^2+4Bs^2h$，所有transformer块的FLOPs为：$l(24Bsh^2+4Bs^2h)$。\n其他结构所需计算量 主要是最终输出层，计算公式为：\n$$ y = XW^O + B, 其中X\\in[B,s,h], W^O \\in [h,V], V为字典大小 $$\n其计算量大小为：\n$$ 2Bsh*V=2BsVh $$\n综上，整个模型的前向传播计算量为：\n$$ F_{前向传播} = l(24Bsh^2+4Bs^2h)+2BsVh \\ = 24Bsh^2l+4Bs^2hl+2BsVh \\ = 24Bsh^2l(1+\\frac{s}{6h}+\\frac{V}{12lh}) $$\n而通常在大模型中，6h远大于s，12lh远大于V，所以还可以进一步简化：\n$$ F_{前向传播} = 24Bsh^2l $$\n因此可推断，训练与推理所需的计算量为：\n$$ F_{训练} = 3F_{前向传播} = 72Bsh^2l \\ F_{训练} = 4F_{前向传播} = 96Bsh^2l,全量参数计算时 \\ F_{推理} = F_{前向传播} = 24Bsh^2l $$\n粗略估算一般基于transformer的大模型的参数量为$12lh^2$,可以发现： 1）模型训练中：如果没有应用激活值重计算，单模型副本处理每个tokens时，单参数上算力需求约为6FLOPs($\\frac{72Bsh^2l}{12lh^2Bs}$)。如果应用全激活值重计算，提升至8FLOPs； 2）模型推理中：单模型副本处理每个tokens时，单参数上算力需求约为6FLOPs；\n最常用来测量每秒浮点运算次数的基准程序（benchmark）之一，就是Linpack。 一个MFLOPS（megaFLOPS）等于每秒一百万（=10^6）次的浮点运算， 一个GFLOPS（gigaFLOPS）等于每秒十亿（=10^9）次的浮点运算， 一个TFLOPS（teraFLOPS）等于每秒一万亿（=10^12）次的浮点运算， 一个PFLOPS（petaFLOPS）等于每秒一千万亿（=10^15）次的浮点运算， 一个EFLOPS（exaFLOPS）等于每秒一百亿亿（=10^18）次的浮点运算。\n模型训练时间估算 根据阿姆达尔定律，由于整个系统中有网络通信等无法并行加速项的存在，集群中存在加速比上限。通常，训练集群中的GPU利用率通常约30%～55%之间。集群训练耗时计算公式如下： https://blog.csdn.net/weixin_43925843/article/details/145626893 https://www.nvidia.cn/data-center/v100/ https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md#%E8%BD%AF%E7%A1%AC%E4%BB%B6%E4%BE%9D%E8%B5%96\nLLaMA Factory 微调所需的硬件资源计算主要基于模型参数量、微调方法和训练配置三个核心维度。以下是具体计算方法与典型场景示例：\n一、显存需求计算公式 显存占用（GB）主要包含以下部分：\n$$ \\text{显存} = \\text{模型参数显存} + \\text{梯度显存} + \\text{优化器状态显存} + \\text{激活值显存} $$\n1. 模型参数显存\n全参数微调（FP16）：参数量 × 2 bytes（例如 7B 模型：7×10⁹ × 2 / 10⁹ = 14GB） LoRA（FP16）：仅需存储低秩矩阵（约 0.1%-1% 参数量）（例如 7B 模型：14GB × 1% ≈ 0.14GB） QLoRA（4-bit）：参数量 × 0.5 bytes （例如 7B 模型：7×10⁹ × 0.5 / 10⁹ = 3.5GB） 2. 梯度与优化器状态\n全参数微调（Adam优化器）：参数量 × 4 bytes（梯度+优化器状态） LoRA/QLoRA：仅需存储少量梯度（可忽略） 3. 激活值显存 与批次大小（batch size）和序列长度（seq_len）相关，计算公式：\n$$ \\text{激活值显存} \\approx \\text{batch_size} \\times \\text{seq_len} \\times \\text{hidden_dim} \\times 2 \\text{ bytes} $$\n二、典型场景硬件需求 模型规模 微调方法 显存需求（估算） 推荐 GPU 配置 数据来源 7B 全参数（FP16） 60GB 2×A100 80GB 7B QLoRA（4-bit） 6GB 单卡 RTX 3090/4090 13B LoRA（FP16） 12GB 单卡 A100 40GB 70B 全参数（FP16） 600GB 8×H100 80GB（NVLink） 三、关键影响因素 微调算法选择\n全参数微调：显存需求最高，适合高算力集群。 LoRA/QLoRA：显存需求降低 50%-90%，适合单卡训练。 GaLore/Freeze：通过梯度压缩或参数冻结进一步优化显存。 训练配置优化\n批次大小（batch_size）：增大 batch_size 会线性增加激活值显存。 梯度累积（gradient_accumulation）：通过累积梯度减少单步显存需求。 量化与混合精度：4-bit/8-bit 量化可降低显存 30%-70%。 分布式训练支持 LLaMA Factory 支持 DeepSpeed ZeRO 和模型并行，可将大模型拆分到多卡。\n四、实战示例（7B模型 + QLoRA） 硬件配置：RTX 4090（24GB 显存） 训练命令： 1 2 3 4 5 6 llamafactory-cli train \\ --model_name_or_path meta-llama/Llama-3-8b \\ --finetuning_type qlora \\ --quantization_bit 4 \\ --per_device_train_batch_size 4 \\ --gradient_accumulation_steps 8 显存占用：约 10-12GB（含激活值）。 五、资源估算工具 LLaMA Factory 提供显存估算工具，可通过以下命令生成报告：\n1 llamafactory-cli estimate --model_name_or_path meta-llama/Llama-3-8b --finetuning_type lora 如需更精确的计算，可参考 Hugging Face 的 Transformer 显存计算器。 https://mp.weixin.qq.com/s/CJxA-PxF_lvSpMr_7uHVVg\n推理所用显存： Total GPU Memory = 模型大小 + KV Cache + Memory Overhead 最后还是以 LLaMa-2 13B 来举例。假设有 10 个并发请求，同时请求 LLaMa-2 13B 以最大 Token数(4096) 进行模型推理。 那最终需要的 GPU Memory 计算过程如下： 模型大小= 13 Billion * 2 Bytes = 26 GB\nTotal KV cache= 800 KB * 4096 Tokens * 10 并发请求 = 32 GB\nMemory Overhead= 0.1 * (26 GB + 32 GB) = 5.8 GB\n所以最终需要总 GPU memory为: 26 GB + 32 GB + 5.8 GB = 63.8 GB。需要 2 块英伟达的 A100 芯片才可以。 https://github.com/manuelescobar-dev/LLM-Tools\nhttps://medium.com/@manuelescobar-dev/memory-requirements-for-llm-training-and-inference-97e4ab08091b\n显存计算 推理显存评估：\n$$ Total\\ Inference\\ Memory = Model Size + KV Cache + Activations $$\n模型大小：\nkv cache：\n$$ KV\\ Cache =2 × Batch Size × Sequence Length × Number of Layers × Hidden Size × Precision $$\n激活值：\n$$ Activation Memory = Batch Size × Sequence Length × Hidden Size × ( 34 + 5 × \\frac{Sequence Length ×Number of attention heads}{Hidden Size}) $$\n推理显存评估：\n$$ Total Memory=Model Size+KV Cache+Activations+(Optimizer States+Gradients)× Number of Trainable Parameters $$\nOptimizer States Optimization algorithms require resources to store the parameters and auxiliary variables. These variables include momentum and variance used by algorithms such as Adam (2 states) or SGD (1 state). The precision and type of optimizer affect memory usage.\nFormula [1]\nAdamW (2 states): 8 Bytes per parameter AdamW (bitsandbytes Quantized): 2 Bytes per parameter SGD (1 state): 4 Bytes per parameter Gradients Gradient values are computed during the backward pass of the model. They represent the rate of change of the loss function with respect to each model parameter and are crucial for updating the parameters during optimization. As with activations, they must be stored in FP32 for numerical stability.\nFormula [1]\n4 Bytes per parameter\n","date":"2025-02-19T08:38:25Z","permalink":"https://blog.liu-nian.top/ai/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86%E8%B5%84%E6%BA%90%E4%B8%8E%E8%AE%A1%E7%AE%97%E9%87%8F%E8%AF%84%E4%BC%B0/","title":"大模型训练推理资源与计算量评估"},{"content":"目标 docker-compose、k8s-yaml\n准备镜像 这里以llama.cpp的基础镜像作为参考。选择llama.cpp的原因是他支持非常多种类的加速卡，同时也支持cpu运行大模型。 使用官方已有镜像 如果机器环境的CUDA版本与官方镜像的兼容，可以直接使用官方镜像。 参考链接：https://github.com/ggerganov/llama.cpp/blob/master/docs/docker.md 如果有GPU，建议使用：ghcr.io/ggerganov/llama.cpp:server-cuda 如果没有gpu，建议使用：ghcr.io/ggerganov/llama.cpp:server 如果想要拉取默认的基础镜像为最新版本，check下最新的dockerfile文件，确认下是否与自己环境的cuda版本兼容。链接如下：https://github.com/ggerganov/llama.cpp/tree/master/.devops 如果最新的镜像不能满足需求，可以尝试搜索下历史镜像 可访问页面进行搜索:https://github.com/ggerganov/llama.cpp/pkgs/container/llama.cpp\n以最新的镜像为例，具体命令如下：\n1 2 docker pull ghcr.io/ggerganov/llama.cpp:server-cuda --platform linux/amd64 #如果镜像拉操作系统与最终操作系统不一致，需要加上--platform参数。如果一致，无需添加。 docker save -o llama.cpp.tar ghcr.io/ggerganov/llama.cpp:server-cuda #如果需要离线导入目标机器，可将镜像保存为tar包 自定义镜像 历史镜像可能不满足需求，可以尝试自定义镜像。这里以cuda 11.8.0为例。 查看nvidia/cuda镜像的tag，选择合适的版本。参考地址：https://hub.docker.com/r/nvidia/cuda/tags\n准备llama.cpp镜像 1 2 3 4 5 6 7 8 git clone https://github.com/ggerganov/llama.cpp cd llama.cpp vim .devops/cuda.Dockerfile # 修改对应的cuda版本，注意操作系统的版本也要和刚查到的镜像保持一致 docker build -t llama.cpp:server-cuda-11.8 .devops/cuda.Dockerfile # 保存为tar包，方便离线导入目标机器 docker save -o llama.cpp-server-cuda-11.8.tar llama.cpp:server-cuda-11.8 # 也可选择push到自己的镜像仓库 docker push ${ your hub }/llama.cpp:server-cuda-11.8 Dockerfile示例:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 ARG UBUNTU_VERSION=20.04 # 修改为目标版本 # 如果目标版本为20.04，会出现时区问题和cmake版本过低问题，需要单独调整 # This needs to generally match the container host\u0026#39;s environment. ARG CUDA_VERSION=11.5.2 # 修改为目标版本 # Target the CUDA build image ARG BASE_CUDA_DEV_CONTAINER=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu${UBUNTU_VERSION} # 确保dockerhub中有该镜像 ARG BASE_CUDA_RUN_CONTAINER=nvidia/cuda:${CUDA_VERSION}-runtime-ubuntu${UBUNTU_VERSION} # 确保dockerhub中有该镜像 ARG DEBIAN_FRONTEND=noninteractive FROM ${BASE_CUDA_DEV_CONTAINER} AS build # CUDA architecture to build for (defaults to all supported archs) ARG CUDA_DOCKER_ARCH=default # 版本为20.04时，需要单独调整时区问题，需要调整部分 ########################################### #ENV TZ=Etc/UTC # RUN apt-get update \u0026amp;\u0026amp; \\ # apt-get install -y tzdata wget tar build-essential python3 python3-pip git libcurl4-openssl-dev libgomp1 \u0026amp;\u0026amp; \\ # wget https://github.com/Kitware/CMake/releases/download/v3.31.5/cmake-3.31.5-linux-x86_64.tar.gz \u0026amp;\u0026amp; \\ # tar -zxvf cmake-3.31.5-linux-x86_64.tar.gz \u0026amp;\u0026amp; mv cmake-3.31.5-linux-x86_64 /opt/cmake \u0026amp;\u0026amp; \\ # ln -s /opt/cmake/bin/cmake /usr/bin/cmake \u0026amp;\u0026amp; cmake --version ########################################### RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y build-essential cmake python3 python3-pip git libcurl4-openssl-dev libgomp1 WORKDIR /app COPY . . RUN if [ \u0026#34;${CUDA_DOCKER_ARCH}\u0026#34; != \u0026#34;default\u0026#34; ]; then \\ export CMAKE_ARGS=\u0026#34;-DCMAKE_CUDA_ARCHITECTURES=${CUDA_DOCKER_ARCH}\u0026#34;; \\ fi \u0026amp;\u0026amp; \\ cmake -B build -DGGML_NATIVE=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON ${CMAKE_ARGS} -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined . \u0026amp;\u0026amp; \\ cmake --build build --config Release -j$(nproc) RUN mkdir -p /app/lib \u0026amp;\u0026amp; \\ find build -name \u0026#34;*.so\u0026#34; -exec cp {} /app/lib \\; RUN mkdir -p /app/full \\ \u0026amp;\u0026amp; cp build/bin/* /app/full \\ \u0026amp;\u0026amp; cp *.py /app/full \\ \u0026amp;\u0026amp; cp -r gguf-py /app/full \\ \u0026amp;\u0026amp; cp -r requirements /app/full \\ \u0026amp;\u0026amp; cp requirements.txt /app/full \\ \u0026amp;\u0026amp; cp .devops/tools.sh /app/full/tools.sh ## Base image FROM ${BASE_CUDA_RUN_CONTAINER} AS base RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install -y libgomp1 curl\\ \u0026amp;\u0026amp; apt autoremove -y \\ \u0026amp;\u0026amp; apt clean -y \\ \u0026amp;\u0026amp; rm -rf /tmp/* /var/tmp/* \\ \u0026amp;\u0026amp; find /var/cache/apt/archives /var/lib/apt/lists -not -name lock -type f -delete \\ \u0026amp;\u0026amp; find /var/cache -type f -delete COPY --from=build /app/lib/ /app # 此处删除了light full 等不必要的镜像信息，仅留了server信息 ### Server, Server only FROM base AS server ENV LLAMA_ARG_HOST=0.0.0.0 COPY --from=build /app/full/llama-server /app WORKDIR /app HEALTHCHECK CMD [ \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost:8080/health\u0026#34; ] ENTRYPOINT [ \u0026#34;/app/llama-server\u0026#34; ] 也可选择将llama.cpp仓库fork,修改为目标版本的Dockerfile，然后使用github的工作流去构建镜像。可参考镜像仓库：https://github.com/Williamyzd/appbuilder.git 可直接使用的tag:https://github.com/Williamyzd/appbuilder/pkgs/container/appbuilder%2Fllama.cpp\n准备模型 docker-compose撰写 k8s-yaml撰写 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 apiVersion: apps/v1 kind: Deployment metadata: name: qwen2-5-14b-gpu namespace: llms labels: app: qwen2-5-14b-gpu spec: replicas: 1 # 副本数量 selector: matchLabels: app: qwen2-5-14b-gpu template: metadata: labels: app: qwen2-5-14b-gpu spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/cluster-role operator: In values: - slave - master-compute # - key: kubernetes.io/gpu-type-name # operator: In # values: # - Tesla-P4 containers: - name: qwen2-5-14b-gpu-gpu image: 10.233.0.100:5000/llama_cpp_gpu_12_2:cuda12.2_img-csbhtgdrp9ts131y # 使用的镜像 imagePullPolicy: IfNotPresent # 镜像拉取策略 #command: [\u0026#34;nvidia-smi\u0026#34;,\u0026#34;\u0026amp;\u0026amp;\u0026#34;,\u0026#34;tail\u0026#34;, \u0026#34;-f\u0026#34;,\u0026#34;/dev/null\u0026#34;] env: - name: LLAMA_ARG_MODEL value: \u0026#34;/models/qwen2.5-14b-instruct-q5_k_m.gguf\u0026#34; - name: LLAMA_ARG_PORT value: \u0026#34;8000\u0026#34; - name: LLAMA_ARG_UBATCH value: \u0026#34;4096\u0026#34; - name: LLAMA_ARG_HOST value: \u0026#34;0.0.0.0\u0026#34; - name: LLAMA_ARG_N_PARALLEL value: \u0026#34;2\u0026#34; - name: LLAMA_ARG_N_PREDICT value: \u0026#34;4096\u0026#34; - name: LLAMA_ARG_CTX_SIZE value: \u0026#34;12000\u0026#34; - name: LLAMA_ARG_N_GPU_LAYERS value: \u0026#34;10\u0026#34; # # vgpu 资源限制 # - name: USE_GPU # value: \u0026#34;1\u0026#34; # - name: GPU_NUM # value: \u0026#34;100\u0026#34; resources: requests: memory: \u0026#34;1Gi\u0026#34; # 请求的内存大小 cpu: \u0026#34;1\u0026#34; # 请求的 CPU 大小（以毫核为单位） limits: memory: \u0026#34;64Gi\u0026#34; # 限制的内存大小 cpu: \u0026#34;8\u0026#34; securityContext: privileged: false runAsGroup: 0 runAsUser: 0 ports: - containerPort: 8000 # 容器暴露的端口 volumeMounts: - mountPath: /dev/shm name: cache-volume - mountPath: /models # subPath: v-wa9c73y3ge0wrvmm/org/william/qwen name: llm-models volumes: - name: cache-volume emptyDir: medium: Memory sizeLimit: \u0026#34;4Gi\u0026#34; # - name: gluster-llms # persistentVolumeClaim: # claimName: gluster-llms ### 此处根据实际填写 - hostPath: path: /models type: Directory name: llm-models --- apiVersion: v1 kind: Service metadata: name: qwen2-5-14b-gpu namespace: llms spec: selector: app: qwen2-5-14b-gpu # 确保这与 Deployment 中 Pod 的标签相匹配 ports: - protocol: TCP port: 8000 # Service 监听的端口 targetPort: 8000 # Pod 内应用程序监听的端口 nodePort: 8512 # Node 上开放的端口 type: NodePort # Service 类型设置为 NodePort 此处使用hostpath挂载模型，可根据实际环境选择更适合的存储方式。 注意对hostpath挂载的位置做调整 ","date":"2025-02-05T17:29:32+08:00","permalink":"https://blog.liu-nian.top/ai/%E5%A6%82%E4%BD%95%E9%83%A8%E7%BD%B2%E7%94%9F%E4%BA%A7%E7%BA%A7%E5%88%AB%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A1%E7%90%86/","title":"如何部署生产级别的大模型推理服务"},{"content":"发票合规验证助手 1.1 背景： 发票报销是每个公司员工常见的工作之一。一般公司的报销制度都有比较复杂的规定和流程。员工在报销时没注意到销制度的一些细节，便容易出现所报销的发票不符合公司的规定的情况，导致了许多额外的工作量。同时很多公司采用纸质发票或者电子发票打印的方式进行报销，在报销审核流程上也要耗费很多的时间。 发票合规验证助手智能体用于解决以上问题。利用大模型调用文本识别接口，自动识别电子发票，方便线上审核。根据发票信息，大模型查询公司的报销制度手册，判断发票是否满足公司的报销规范，并告知用户。 1.2 方案设计 1.3 Prompt编写 https://console.bce.baidu.com/ai_apaas/personalSpace/app/debugger/assistant/a694c560-b3b9-4bc4-b6ae-95a4ca317174/831cec36-e8e1-464f-b802-38ea6dbc45b6\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 # 角色 你是一位公司的财务人员，负责公司的报销制度制定与员工报销，以及回答员工问题。 # 工具能力 使用通用文字识别技术，特别是高精度版，以准确识别并解析发票上的文字信息。 利用知识库中的员工报销手册来判断发票的识别结果是否合规。 # 要求与限制 根据数据源的报销制度，验证发票是否可用于报销 确保识别的准确性，以避免任何因识别错误导致的验证失败。重点识别发票的关键信息，如日期、金额、商品或服务描述等。 请如实根据知识库与实际情况作答，不要假设场景。 # 任务 1. 接受用户发票图片，系统使用通用文字识别技术解析并验证发票上的关键信息 2. 根据发票解析结果，根据员工报销手册，判断发票是否可报销 # 输出规范 如果发票可用，则输出： 该发票是否可用：输出是 如果发票不可用，则输出： 该发票是否可用：否 不可用原因：说明不能用的原因 # 示例1 发票可用的示例： 该发票是否可用：是 发票不可用示例： 该发票是否可用：否 不可用原因： 1. 报销金额超限 2. 发票抬头公司不符 # 数据源 员工报销手册 前言 本手册旨在明确百度在线网络技术(北京)有限公司（以下简称“公司”）员工在移动通讯费、工作交通费、交通费、业务洽谈费等方面的报销制度及流程，确保报销工作的规范化、透明化。请各位员工仔细阅读，并严格按照手册规定执行报销事宜。 第一章 总则 目的与原则 本手册的制定旨在规范员工报销行为，提高报销效率，确保公司财务的合规性与准确性。报销应遵循“实事求是、合理合规、及时准确”的原则。 适用范围 本手册适用于公司全体员工，包括但不限于全职员工、兼职员工、实习生等。 报销流程 员工需先垫付相关费用，并保留好原始发票、收据等凭证。然后，通过公司报销系统提交报销申请，附上相关凭证及说明。经部门主管审核、财务部门复核后，方可进行报销。 第二章 报销类型及限额 一、移动通讯费 报销范围 移动通讯费包括手机话费、上网流量费等因工作需要产生的通讯费用。 报销限额 每月移动通讯费报销限额为人民币200元。超出部分需员工自行承担。 开票要求 发票抬头需为“百度在线网络技术(北京)有限公司”。 二、工作交通费 报销范围 工作交通费包括因工作需要而产生的出租车费、地铁费、公交费等公共交通费用。 报销条件 工作交通费只允许报销晚上10点后至早上7点前、以及周末节假日全天所发生的费用。其他时间段产生的费用不予报销。 报销限额 每月工作交通费报销限额为人民币500元。超出部分需员工自行承担。 开票要求 发票抬头需为“百度在线网络技术(北京)有限公司”。 三、交通费 报销范围 交通费包括因工作需要而产生的长途汽车费、火车票、飞机票等费用。 报销限额 交通费报销需根据出差地点、出差天数等因素综合考虑，具体限额如下： 市内短途出差：每次不超过人民币100元； 省内长途出差：每次不超过人民币500元； 跨省长途出差：根据出差地点及交通工具类型，每次不超过人民币2000元。 超出限额部分需员工自行承担，并需提前向部门主管申请并获得批准。 开票要求 发票抬头需为“百度在线网络技术(北京)有限公司”。同时，需附上出差申请单、行程单等相关凭证。 四、业务洽谈费 报销范围 业务洽谈费为与客户吃饭产生的餐费，旨在促进业务合作与交流。 报销限额 每次业务洽谈费报销限额为人民币1000元。超出部分需员工自行承担，并需提前向部门主管申请并获得批准。 开票要求 发票抬头需为“百度在线网络技术(北京)有限公司”。同时，需附上业务洽谈邀请函、客户名单、菜单等相关凭证。 第三章 报销限制条件 费用日期限制 费用日期超过3个月以上不允许报销。请员工务必在费用发生后及时提交报销申请。 报销凭证要求 员工需保留好原始发票、收据等凭证，并确保凭证的真实性与完整性。无凭证或凭证不全者，不予报销。 特殊事项处理 对于因特殊原因产生的费用（如紧急出差、突发事件等），员工需提前向部门主管申请并获得批准。未经批准而产生的费用，不予报销。 第四章 报销审核与支付 审核流程 员工提交报销申请后，由部门主管进行初审。初审通过后，由财务部门进行复核。复核通过后，方可进行报销支付。 支付方式 公司采用银行转账方式支付报销款项。员工需在报销系统中填写正确的银行账户信息。因信息错误而导致的支付失败或延误，由员工自行承担。 支付时效 财务部门在收到完整的报销申请及凭证后，将在5个工作日内完成审核并支付报销款项。如遇特殊情况需延迟支付，财务部门将提前通知员工并说明原因。 1.4 Prompt优化策略 基于思维连策略细化任务内容\n在判断是否满足员工报销制度要求时，为了方便大模型判断的准确性，使用Cot方式将处理过程进行拆解。任务部分的Prompt修改如下：\n1 2 3 4 5 6 7 8 9 10 11 12 # 任务 首先尝试回答以下问题： 1. 发票中的购买方（开票抬头公司）是谁？销售方是谁？ 2. 发票中消费或购买的是什么服务？ 3. 发票中的消费金额是多少？ 4. 发票的开票日期是多少？ 基于上一步的判断，根据员工报销制度手册，回答以下问题： 1. 根据发票信息，将该发票划分为公司报销类型中的一种，并说明是哪一种？ 2. 今天是哪天，开票日期距今多久，是否在公司许可时效范围内？ 3. 开票金额是否在公司需要范围内？ 4. 购买方公司名称是什么？是否满足公司发票抬头要求？ 根据对以上问题的回答和理解，判断该发票是否可用于报销 使用知识库来存储员工手册\n使用外部知识库的方式可以更好地对员工手册进行切片，增强大模型的检索和理解能力。\n1.5 最终Prompt与效果展示 最终的Prompt如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # 角色 你是一位公司的财务人员，负责公司的报销制度制定与员工报销，以及回答员工问题。 # 工具能力 使用通用文字识别技术，特别是高精度版，以准确识别并解析发票上的文字信息。 利用知识库中的员工报销手册来判断发票的识别结果是否合规。 # 要求与限制 根据知识库的报销制度，验证发票是否可用于报销 确保识别的准确性，以避免任何因识别错误导致的验证失败。重点识别发票的关键信息，如日期、金额、商品或服务描述等。 请如实根据知识库与实际情况作答，不要假设场景。 # 任务 首先尝试回答以下问题： 1. 发票中的购买方（开票抬头公司）是谁？销售方是谁？ 2. 发票中消费或购买的是什么服务？ 3. 发票中的消费金额是多少？ 4. 发票的开票日期是多少？ 基于上一步的判断，根据知识库中的员工报销制度手册，回答以下问题： 1. 根据发票信息和员工报销手册，将该发票划分为公司报销类型中的一种，并说明是哪一种？ 2. 今天是哪天，开票日期距今多久，是否在公司许可时效范围内？ 3. 开票金额是否在公司需要范围内？ 4. 购买方公司名称是什么？是否满足公司发票抬头要求？ 根据对以上问题的回答和理解，判断该发票是否可用于报销 # 输出规范 如果发票可用，则输出： 该发票是否可用：输出是或者否 如果发票不可用，则输出： 该发票是否可用：否 不可用原因：说明不能用的原因 # 示例1 该发票是否可用：是 # 示例 2 该发票是否可用：否 不可用原因： 1. 报销金额超限 2. 发票抬头公司不符 在APPBuilder中的设置效果如下：\n智能报销智能体 2.1 背景 上一个案例仅仅使用智能体实现了一个发票的合规性检查工作。实际上我们还可以做得更多。基于以上的验证结果，我们还可以进行如下的工作：\n调用外部的api，对发票的真实性进行验证； 抽取信息形成报销表单并对接到公司的审批流中； 同时我们也可以利用智能体的记忆功能保存员工手册中的知识、报销单据，并进行询问和统计； 2.2 方案设计 https://console.bce.baidu.com/ai_apaas/personalSpace/app/debugger/chatflow/1832bf3c-d051-490b-bc64-70c91e404214/27702d2a-038e-468d-98f3-b217d4258e31\n2.3 Prompt编写 我们需要再在这个项目中单独创建几个单独的部分：\n发票识别组件 发票信息合规判断 发票验真组件 串联工作流和组件的Prompt和代码 2.3.1 发票识别组件 https://console.bce.baidu.com/ai_apaas/workflow/cfe5049f-f9db-4669-b7fa-e1a2e402fe6e\n其中发票识别组件使用外部API接入的方式创建。如下图展示的发票识别组件在输入层了数据转换以方便获取图片的url地址。同样，在输出层也做了一层转换，以方便外部其他组件的调用时，可以直接获区拼装好的文字内容。\n2.3.2 发票信息合规判断组件 https://console.bce.baidu.com/ai_apaas/workflow/bd8c8e51-5b8f-4819-a517-8b7fcd0d8cc7\n发票信息合规判断组件以query改写、知识库组件、发票类型判断和合规判断四部分为核心。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 角色 你是一位公司的财务人员，帮助员工回答报销问题 # 任务 基于{{content}}，来回答{{query}}，以json格式输出结果 # 输出要求 输出格式：json 输出字段以及含义如下： bx_type：公司报销类型中最有可能报销类型 xz_reason：选择这种报销类型的原因 # 输出示例 { \u0026#34;bx_type\u0026#34;:\u0026#34;业务洽谈费\u0026#34;, \u0026#34;xz_reason\u0026#34;:\u0026#34;该发票属于餐饮费，可用于公司业务洽谈费的报销范围\u0026#34; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 角色 你是一位公司的财务人员，负责公司的报销制度制定与员工报销，以及回答员工问题。 # 任务 基于{{content}}，以及提交报销的日期为{{c_date}},来回答{{query}} # 特别注意 在判断发票是否可报销时要特别注意以今天为提交报销申请的时间节点，报销是否合规。 # 输出要求 输出字段以及含义 code：0或1。0表示满足公司报销合规要求或找到用户咨询的答案，1表示不满足公司报销规定或没有找到用户合适的答案。 reason：满足或不满足公司合规要求的原因，或是用户问题的答案。 # 输出示例 { \u0026#34;code\u0026#34;:0, \u0026#34;reason\u0026#34;:\u0026#34;发票的金额、抬头、报销日期均符合报销要求\u0026#34; } 知识库生成Prompt:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #角色： 你是一名公司的财务人员，主要负责处理员工的报销制度编制和员工报销事宜。 #任务： 1. 编制一份员工报销手册，主要包括移动通讯费、工作交通费、交通费、业务洽谈费几种类型的报销。 2. 对报销添加限制条件，包括但不限于： 2.1 每种报销的限额以及月总费用的限额； 2.2 费用日期超过3个月以上不允许报销 2.3 工作交通费只允许报销晚上10点后至早上7点前、以及周末节假日全天所发生的费用 2.4 业务洽谈费为与客户吃饭产生的餐费 2.5 每种费用的开票抬头公司为：百度在线网络技术(北京)有限公司 3. 本手册内容不少于2000字 #输出要求： 手册通俗易通，结构清晰，无歧义。 [附件]\n2.3.3 发票验真组件 https://console.bce.baidu.com/ai_apaas/workflow/d0e9467e-b1fc-4ee8-bf8f-a09706d9a0a6\n发票验真API准备\n这里生成一个模拟发票验证的接口，生成接口的Prompt如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #角色 你是一名python研发工程师 #任务 基于python3，使用flask框架生成一个发票验证接口，尽可能少的使用第三方框架。 具体要求如下： 1. 接口为post请求 2. 入参接受json格式，具体参数如下： nomber:发票号 pay:购买方 sale:销售方 date:开票日期 account:发票金额 3. 返回参数： code:0表示验证成功，1表示验证失败 msg:验证结果，主要是success和fail，分别对应code的0和1两种结果 4. 处理逻辑 以80%的概率返回成功，20%的概率返回失败。 # 输出要求 1. 所有代码在一个文件中。每个函数需要有详细的注释 2. 输入和输出都为json格式 3. 生成一个requrements.txt文件，包含所有依赖的第三方框架 #样例： 输入样例： { \u0026#34;nomber\u0026#34;:\u0026#34;24112000003220018474944\u0026#34;, \u0026#34;pay\u0026#34;:\u0026#34;xxxx有限公司\u0026#34;, \u0026#34;sale\u0026#34;:\u0026#34;北京恒泰人合火锅店\u0026#34;, \u0026#34;date\u0026#34;:\u0026#34;2024年04月03日\u0026#34;, \u0026#34;account\u0026#34;:\u0026#34;246.23\u0026#34; } 输出样例： { \u0026#34;code\u0026#34;:0, \u0026#34;msg\u0026#34;:\u0026#34;success\u0026#34; } 代码文件：\n[附件] 用于构建、发布服务的镜像Dockerfile如下\n1 2 3 4 5 from python:3.10.0-slim-buster run pip install flask \u0026amp;\u0026amp; mkdir -p /opt/fpyz copy app.py /opt/fpyz workdir /opt/fpyz cmd python /opt/fpyz/app.py 发票验真组件搭建\n组件调试 2.4 整体智能体工作流如下： 2.5 智能体优化策略\n添加记忆变量，优化不同组件之间的变量传递 对接外部接口或数据库，方便查询报销单的处理流程 ","date":"2024-12-01T17:29:32+08:00","permalink":"https://blog.liu-nian.top/ai/%E6%99%BA%E8%83%BD%E4%BD%93%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B-%E6%8A%A5%E9%94%80%E5%8A%A9%E6%89%8B/","title":"基于APPBuildr的智能体实战案例-报销助手"},{"content":"llama.cpp推理-树莓派4b\n参考链接：https://github.com/ggerganov/llama.cpp\nhttps://qwen.readthedocs.io/zh-cn/latest/run_locally/llama.cpp.html\n裸机方式运行，非特殊框架，可以直接拉取官方镜像：\n硬件信息：\n平台架构：aarch64\n操作系统: Ubuntu 22.04.3 LTS\nCPU: 2\nMEM:4G\n创建环境 主要包括拉取代码、下载模型或生成模型等步骤\n1 2 3 4 5 6 7 8 9 10 11 # 1. 拉取代码 git clone https://github.com/ggerganov/llama.cpp llmama.cpp # 2. 编译代码 cd llama.cpp make make llama-cli # 如无需本地交互测试，不需要执行 # 3. 手动下载模型 #https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/tree/main https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/blob/main/qwen2-1_5b-instruct-q5_k_m.gguf # 或自己生成，需要python环境 python convert-hf-to-gguf.py Qwen/Qwen2-7B-Instruct --outfile qwen2-1_5b-instruct-f16.gguf 交互式测试 参数说明：\n-m 指模型地址 -cnv 指会话模式 -p 指的输入信息，必须要传入 1 ./llama-cli -m models/qwen2-1_5b-instruct-q5_k_m.gguf -cnv -p \u0026#39;你是一个人工智能专家\u0026#39; 运行成功的截图如下：\nAPI 发布测试 1 ./llama-server -m /opt/codes/models/qwen2-1_5b-instruct-q5_k_m.gguf --port 8000 --host 0.0.0.0 前台页面访问测试\nhttp://192.168.2.189:8080/\n","date":"2024-09-25T08:38:25Z","permalink":"https://blog.liu-nian.top/ai/llama.cpp/","title":"llama.cpp推理-树莓派4b"},{"content":"硬件信息：\nBCC: 2核8G\n创建推理环境 参考链接\nhttps://docs.vllm.ai/en/stable/getting_started/cpu-installation.html\nhttps://github.com/Williamyzd/vllm/blob/main/Dockerfile.cpu\nhttps://github.com/vllm-project/vllm/blob/main/requirements-cpu.txt\nhttps://www.cnblogs.com/obullxl/p/18353447/NTopic2024081101\n准备镜像\n1 2 3 4 5 6 7 8 9 10 # 拉取代码 #如果没有git需要先安装 yum install git git clone https://github.com/vllm-project/vllm.git vllm-project cd vllm-project # 打镜像 # 以下命令在 Docker version 26.1.4 中测试 # 可编辑Dockerfile.cpu做优化 # 1. 修改基础镜像为：registry.cn-hangzhou.aliyuncs.com/reg_pub/ubuntu:22.04 # 2. 调整pip源和source为国内源 docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g . 如果执行速度慢可以考虑加上国内源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #!/bin/bash # 添加清华pip源 pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple # 备份原有的sources.list文件 sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak # 创建一个新的sources.list文件，并添加清华源 cat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;EOF # 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释 deb https://mirrors.aliyun.com/ubuntu/ jammy main restricted universe multiverse deb-src https://mirrors.aliyun.com/ubuntu/ jammy main restricted universe multiverse deb https://mirrors.aliyun.com/ubuntu/ jammy-security main restricted universe multiverse deb-src https://mirrors.aliyun.com/ubuntu/ jammy-security main restricted universe multiverse deb https://mirrors.aliyun.com/ubuntu/ jammy-updates main restricted universe multiverse deb-src https://mirrors.aliyun.com/ubuntu/ jammy-updates main restricted universe multiverse # deb https://mirrors.aliyun.com/ubuntu/ jammy-proposed main restricted universe multiverse # deb-src https://mirrors.aliyun.com/ubuntu/ jammy-proposed main restricted universe multiverse deb https://mirrors.aliyun.com/ubuntu/ jammy-backports main restricted universe multiverse deb-src https://mirrors.aliyun.com/ubuntu/ jammy-backports main restricted universe multiverse EOF dockerfile样例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 # https://github.com/Williamyzd/vllm/blob/main/Dockerfile.cpu # This vLLM Dockerfile is used to construct image that can build and run vLLM on x86 CPU platform. # \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;更换为国内镜像 FROM registry.cn-hangzhou.aliyuncs.com/reg_pub/ubuntu:22.04 AS cpu-test-1 ENV CCACHE_DIR=/root/.cache/ccache ENV CMAKE_CXX_COMPILER_LAUNCHER=ccache # \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;添加国内源 RUN cp /etc/apt/sources.list /etc/apt/sources.list.bak \\ \u0026amp;\u0026amp; echo \u0026#34;deb https://mirrors.aliyun.com/ubuntu/ jammy main restricted universe multiverse\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list \\ \u0026amp;\u0026amp; echo \u0026#34;deb-src https://mirrors.aliyun.com/ubuntu/ jammy main restricted universe multiverse\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list \\ \u0026amp;\u0026amp; echo \u0026#34;deb https://mirrors.aliyun.com/ubuntu/ jammy-security main restricted universe multiverse\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list \\ \u0026amp;\u0026amp; echo \u0026#34;deb-src https://mirrors.aliyun.com/ubuntu/ jammy-security main restricted universe multiverse\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list \\ \u0026amp;\u0026amp; echo \u0026#34;deb https://mirrors.aliyun.com/ubuntu/ jammy-updates main restricted universe multiverse\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list \\ \u0026amp;\u0026amp; echo \u0026#34;deb-src https://mirrors.aliyun.com/ubuntu/ jammy-updates main restricted universe multiverse\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list \\ \u0026amp;\u0026amp; echo \u0026#34;deb https://mirrors.aliyun.com/ubuntu/ jammy-backports main restricted universe multiverse\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list \\ \u0026amp;\u0026amp; echo \u0026#34;deb-src https://mirrors.aliyun.com/ubuntu/ jammy-backports main restricted universe multiverse\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list # \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;这里添加了清华pip源 RUN --mount=type=cache,target=/var/cache/apt \\ apt-get update -y \\ \u0026amp;\u0026amp; apt-get install -y curl ccache git wget vim numactl gcc-12 g++-12 python3 python3-pip libtcmalloc-minimal4 libnuma-dev \\ \u0026amp;\u0026amp; apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\ \u0026amp;\u0026amp; update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12 \\ \u0026amp;\u0026amp; pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple # https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html # intel-openmp provides additional performance improvement vs. openmp # tcmalloc provides better memory allocation efficiency, e.g, holding memory in caches to speed up access of commonly-used objects. RUN --mount=type=cache,target=/root/.cache/pip \\ pip install intel-openmp ENV LD_PRELOAD=\u0026#34;/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4:/usr/local/lib/libiomp5.so\u0026#34; RUN echo \u0026#39;ulimit -c 0\u0026#39; \u0026gt;\u0026gt; ~/.bashrc RUN pip install https://intel-extension-for-pytorch.s3.amazonaws.com/ipex_dev/cpu/intel_extension_for_pytorch-2.4.0%2Bgitfbaa4bc-cp310-cp310-linux_x86_64.whl ENV PIP_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cpu RUN --mount=type=cache,target=/root/.cache/pip \\ --mount=type=bind,src=requirements-build.txt,target=requirements-build.txt \\ pip install --upgrade pip \u0026amp;\u0026amp; \\ pip install -r requirements-build.txt # install oneDNN RUN git clone -b rls-v3.5 https://github.com/oneapi-src/oneDNN.git RUN --mount=type=cache,target=/root/.cache/ccache \\ cmake -B ./oneDNN/build -S ./oneDNN -G Ninja -DONEDNN_LIBRARY_TYPE=STATIC \\ -DONEDNN_BUILD_DOC=OFF \\ -DONEDNN_BUILD_EXAMPLES=OFF \\ -DONEDNN_BUILD_TESTS=OFF \\ -DONEDNN_BUILD_GRAPH=OFF \\ -DONEDNN_ENABLE_WORKLOAD=INFERENCE \\ -DONEDNN_ENABLE_PRIMITIVE=MATMUL \u0026amp;\u0026amp; \\ cmake --build ./oneDNN/build --target install --config Release FROM cpu-test-1 AS build WORKDIR /workspace/vllm RUN --mount=type=cache,target=/root/.cache/pip \\ --mount=type=bind,src=requirements-common.txt,target=requirements-common.txt \\ --mount=type=bind,src=requirements-cpu.txt,target=requirements-cpu.txt \\ pip install -v -r requirements-cpu.txt COPY ./ ./ # Support for building with non-AVX512 vLLM: docker build --build-arg VLLM_CPU_DISABLE_AVX512=\u0026#34;true\u0026#34; ... ARG VLLM_CPU_DISABLE_AVX512 ENV VLLM_CPU_DISABLE_AVX512=${VLLM_CPU_DISABLE_AVX512} RUN --mount=type=cache,target=/root/.cache/pip \\ --mount=type=cache,target=/root/.cache/ccache \\ VLLM_TARGET_DEVICE=cpu python3 setup.py bdist_wheel \u0026amp;\u0026amp; \\ pip install dist/*.whl WORKDIR /workspace/ RUN ln -s /workspace/vllm/tests \u0026amp;\u0026amp; ln -s /workspace/vllm/examples \u0026amp;\u0026amp; ln -s /workspace/vllm/benchmarks ENTRYPOINT [\u0026#34;python3\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;vllm.entrypoints.openai.api_server\u0026#34;] 下载模型文件 方法一：使用git 1 2 3 4 5 6 7 # 使用git 下载，文件比较大，因而使用git-lfs来支持断点续传 安装参考：https://github.com/git-lfs/git-lfs/blob/main/INSTALLING.md curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash yum install git-lfs mkdir -p ~/ModelSpace \u0026amp;\u0026amp; cd ~/ModelSpace git lfs install git clone https://www.modelscope.cn/qwen/qwen2-0.5b.git Qwen2-0.5B 方法二：直接使用modelscope工具 1 2 3 4 pip install modelscope modelscope download --model qwen/qwen2-0.5b --local_dir ~/ModelSpace/ #modelscope -h 查看使用说明 #https://www.modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD 运行测试 直接跑会报错，重新写一个dockerfile以方便测试\n1 2 3 4 # 注意这里的镜像名称为上一步所打镜像 FROM vllm-cpu-env # 这里是为了覆盖掉老的endpoint以方便做测试 ENTRYPOINT tail -f /dev/null 执行以下命令进行测试，指定了 8000端口暴漏，同时将下载的模型挂载到容器，并指定共享内存为4G\ndocker run -d \u0026ndash;name llms -v /root/llmvs/ModelSpace:/workspace/ModelSpace -p 8000:8000 \u0026ndash;shm-size 4g llms:v0.2\n正式运行 1 docker run -d --name llms -v /root/llmvs/ModelSpace:/workspace/ModelSpace -p 8000:8000 --shm-size 4g vllm-cpu-env:latest --model /workspace/ModelSpace/Qwen2-0.5B 参考链接：https://qwen.readthedocs.io/en/latest/deployment/vllm.html\n1 2 3 4 5 6 7 8 9 10 11 curl -i http://localhost:8080/v1/chat/completions -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;/workspace/ModelSpace/Qwen2-0.5B\u0026#34;, \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你是一名人工智能领域的专家\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;如何学习大模型应用？\u0026#34;} ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;top_p\u0026#34;: 0.8, \u0026#34;repetition_penalty\u0026#34;: 1.05, \u0026#34;max_tokens\u0026#34;: 300 }\u0026#39; 如果以上过程你都不想做，可以直接拉取我打好的cpu镜像：registry.cn-hangzhou.aliyuncs.com/reg_pub/vllm-cpu-env:latest ","date":"2024-09-23T08:38:25Z","permalink":"https://blog.liu-nian.top/ai/vllm-cpu%E6%8E%A8%E7%90%86-%E4%BB%A5qwen2-0.5b%E4%B8%BA%E4%BE%8B/","title":"vllm-cpu推理-以qwen2-0.5b为例"},{"content":"摘要： 本文档详细介绍了使用Hugo搭建Markdown博客的过程，包括本地编辑环境搭建、主题样式修改、远程部署及图床搭建等步骤。通过本地编辑Markdown文档并预览，成功后可推送至GitHub私有仓库。Hugo将Markdown文档渲染为HTML文件，可利用GitHub Pages服务或VPS中的容器对外暴露。同时，文档还介绍了如何配置SSH Key、克隆主题仓库、修改主题样式，以及通过GitHub Pages或自建服务器进行远程部署，并提供了基于GitHub的图床搭建方法。\nPowered by 百度Comate\n基本思路：\n使用在本地编辑Markdown文档，并测试预览。成功后推送到github私有仓库。hugo可以将markdown文档渲染为html文件。可以使用github的pages服务对外暴漏，也可在vps中启动一个容器来对外暴漏。\n1. 本地编辑环境搭建 1.1 二进制安装hugo 直接安装二进制文件，可参考：https://github.com/gohugoio/hugo/releases 下载对应版本并安装 以mac客户端为例，下载对应版本后解压到指定目录。\n1 2 3 4 5 6 7 # 文件解压 tar -zxvf hugo_0.143.0_darwin-universal.tar.gz cd hugo_0.143.0_darwin-universal chmod +x hugo mv hugo /usr/local/bin # 验证测试 hugo version 1.2 源码安装hugo(可选) 如需源码安装，以mac客户端为例，需要安装git、hugo。\n安装git mac自带git，无需安装。如需安装或更新，可参考：https://git-scm.com/downloads/mac\n安装go 可参考链接：https://go.dev/dl/选择对应版本的hugo，下载后解压到指定目录。\n安装hugo go install github.com/gohugoio/hugo@latest\n2. 修改主题样式 2.1 配置ssh key 此处为本地开发环境需要，方便免密提交文件到git仓库。这里以提交到github仓库为例。如果主要在国内访问，且有备案域名，建议使用gitee或自有git仓库。 可参考链接： ssh本地秘钥生成\n1 2 3 4 5 6 7 ssh-keygen -t ed25519 -C \u0026#34;your_email@example.com\u0026#34; # 邮箱为github注册邮箱,一路输入enter即可 # 如果你使用的是不支持 Ed25519 算法的旧系统，请使用以下命令： # ssh-keygen -t rsa -b 4096 -C \u0026#34;your_email@example.com\u0026#34; # 将 SSH 私钥添加到 ssh-agent。 ssh-add ~/.ssh/id_ed25519 # 查看公钥内容，将其复制 cat ~/.ssh/id_ed25519.pub 访问自己的github账号，在账号设置中添加ssh公钥 2.2 克隆主题仓库到本地 将自己喜欢的主题克隆到本地，和hugo一起使用。因涉及到对主题的更改和维护，建议使用git 子模块来维护。可以将该主题fork到自己的空间，然后将自己fork的主题作为子模块添加。这里以stack主题为例。\n1 2 3 4 5 6 7 8 9 10 # 创建第一个站点 hugo new site pages # pages 为站点文件夹名字 cd pages git init # 注意初始化的相对位置 # 添加主题，以stack为例。注意github仓库地址不要使用https协议的，否则会出现子模块的更改无法提交的情况 git submodule add --name stack git@github.com:Williamyzd/hugo-theme-stack.git themes/stack # 添加一个测试页面 hugo new content content/posts/my-first-post.md echo \u0026#34;hello,word!\u0026#34; \u0026gt;\u0026gt; content/posts/my-first-post.md hugo server -D -p 8000 # 可查看处于草稿状态下的文档内容 -p 制定可对外暴漏端口，默认是1314 浏览器访问页面：http://localhost:8000/posts/my-first-post/，可看到发布内容 2.3 修改主题样式 修改主题样式，主要在hugo.toml(yaml)文件中，或者在主题文件夹中修改。可将themes/stack/exampleSite/hugo.yaml 复制到站点根目录，并修改为自己的配置。\nparams-sidebar-avatar修改logo params-mainSections 指定主页默认获取的最新文章 menu-main 指定导航模块 menu-social 指定社交模块 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 baseurl: https://example.com/ languageCode: en-us # 主题名称，注意要和子模块的文件夹名一致 theme: stack # 网站标题 title: Example Site # 版权信息 copyright: Example Person # 多语言支持相关 # Theme i18n support # Available values: ar, bn, ca, de, el, en, es, fr, hu, id, it, ja, ko, nl, pt-br, th, uk, zh-cn, zh-hk, zh-tw DefaultContentLanguage: en # Set hasCJKLanguage to true if DefaultContentLanguage is in [zh-cn ja ko] # This will make .Summary and .WordCount behave correctly for CJK languages. hasCJKLanguage: false languages: en: languageName: English title: Example Site weight: 1 params: sidebar: subtitle: Example description zh-cn: languageName: 中文 title: 演示站点 weight: 2 params: sidebar: subtitle: 演示说明 pagination: pagerSize: 3 permalinks: post: /p/:slug/ page: /:slug/ params: # 这个参数比较关键，决定主页回去哪些目录下去取最新的文章来显示 mainSections: - ai - cs - ops - tools featuredImageField: image rssFullContent: true favicon: # e.g.: favicon placed in `static/favicon.ico` of your site folder, then set this field to `/favicon.ico` (`/` is necessary) footer: since: 2020 customText: dateFormat: published: Jan 02, 2006 lastUpdated: Jan 02, 2006 15:04 MST sidebar: emoji: 🍥 subtitle: Lorem ipsum dolor sit amet, consectetur adipiscing elit. avatar: enabled: true local: true src: img/avatar.png # logo存放的位置，在`static/img/`下 article: math: false toc: true readingTime: true license: enabled: true default: Licensed under CC BY-NC-SA 4.0 # 是否开启评论，建议不开 comments: enabled: false provider: disqus # 默认会开启的几个组件：搜索、归档、分类、标签云 widgets: homepage: - type: search - type: archives params: limit: 5 - type: categories params: limit: 10 - type: tag-cloud params: limit: 10 page: - type: toc opengraph: twitter: # Your Twitter username site: # Available values: summary, summary_large_image card: summary_large_image defaultImage: opengraph: enabled: false local: false src: colorScheme: # Display toggle toggle: true # Available values: auto, light, dark default: auto imageProcessing: cover: enabled: true content: enabled: true ### Custom menu ### See https://stack.jimmycai.com/config/menu ### To remove about, archive and search page menu item, remove `menu` field from their FrontMatter # 自定义的菜单项 menu: main: - identifier: ai #标识符，唯一值 name: AI # 显示的名称 url: /ai # 文件路径，相对于content文件夹 weight: -90 #权重 #权重越小，越靠前显示 params: ### For demonstration purpose, the home link will be open in a new tab newTab: false #是否新开标签页 icon: ai # 图标信息，查看和自定义图标，可修改 themes/stack/assets/icons 下的文件，将文件名写在这里 - identifier: ops name: 运维\u0026amp;工具 url: /ops weight: -90 #权重 params: ### For demonstration purpose, the home link will be open in a new tab newTab: false icon: ops #更多的图标信息见后 - identifier: cs name: CS基础 url: /cs weight: -90 #权重 params: ### For demonstration purpose, the home link will be open in a new tab newTab: true icon: cs #更多的图标信息见后 social: - identifier: github name: GitHub url: https://github.com/CaiJimmy/hugo-theme-stack params: icon: brand-github - identifier: twitter name: Twitter url: https://twitter.com params: icon: brand-twitter related: includeNewer: true threshold: 60 toLower: false indices: - name: tags weight: 100 - name: categories weight: 200 markup: goldmark: extensions: passthrough: enable: true delimiters: block: - - \\[ - \\] - - $$ - $$ inline: - - \\( - \\) renderer: ## Set to true if you have HTML content inside Markdown unsafe: true tableOfContents: endLevel: 4 ordered: true startLevel: 2 highlight: noClasses: false codeFences: true guessSyntax: true lineNoStart: 1 lineNos: true lineNumbersInTable: true tabWidth: 4 现在可以在content目录下创建文档了。 最终效果如下： 3. 远程部署 hugo 生成的静态html文件在public目录下，可以考虑单独创建一个仓库来管理，在主git仓库下再建一个子模块。\n3.1 添加静态文件子模块 添加子模块主要使用命令如下：\n新建远程仓库：git@github.com:Williamyzd/blogs-html.git 在主仓库下添加子模块： 1 git submodule add -n pub-html git@github.com:Williamyzd/blogs-html.git blog/public 如果 子模块添加错误，可用如下命令删除：\n1 2 3 git rm --cached pub-html # pub-html:子模块名称 rm -rf blog/public # 删除本地文件夹 vim .gitmodules # 删除.gitmodules文件中的子模块信息 3.2 远程服务器部署(可选) 创建Dockerfile文件，参考如下： 如无服务器可用，可以不考虑这一步。 通过vps（服务器）托管一个nginx服务。dockerfile参考： 1 2 3 4 5 6 7 8 9 10 11 12 FROM nginx:1.27.3-alpine WORKDIR /home COPY blog.conf /etc/nginx/conf.d/default.conf # 准备证书,如有可打开 # COPY cert /etc/nginx/cert RUN apk update \u0026amp;\u0026amp; apk add git \u0026amp;\u0026amp; \\ git clone https://github.com/Williamyzd/blogs-html.git /home/public \u0026amp;\u0026amp; \\ echo \u0026#34;echo \\$(date +%Y-%m-%d) \\$(date +%H:%M:%S) [info]: \\$(cd /home/public \u0026amp;\u0026amp; git pull) \u0026gt;\u0026gt; /var/log/blog/\\$(date +%Y-%m-%d).log\u0026#34; \u0026gt;\u0026gt; /home/blog.sh \u0026amp;\u0026amp; \\ chmod +x /home/blog.sh \u0026amp;\u0026amp; \\ mkdir -p /var/log/blog \u0026amp;\u0026amp; \\ echo \u0026#34;*/1 * * * * /home/blog.sh\u0026#34; \u0026gt;\u0026gt; /etc/crontabs/root CMD crond \u0026amp;\u0026amp; nginx \u0026amp;\u0026amp; tail -f /dev/null Dockerfile 内容说明： 这里使用了更加轻量版的alpine版本nginx，并安装了git。这个镜像本身具备定时功能，创建了一个定时任务，每天执行一次git pull命令。因仓库是只读且对外公开，所以这里不需要认证。最后的启动命令是先启动定时任务，再启动nginx服务。同时保留一个主进程，保证容器不会退出。 nginx配置文件参考： blog.conf参考(未包含https配置)： 1 2 3 4 5 6 7 8 server { listen 80; server_name blog.xxx.com; location / { root /home/public; index index.html index.htm; } } ssl配置的例子,需要提前准备下证书，并将dockerfile 中的注释去掉。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 以下属性中以ssl开头的属性代表与证书配置有关，其他属性请根据自己的需要进行配置。 server { listen 80; # 需要调整为您证书绑定的域名。 server_name blog.xxx.com return 301 https://$host/$request_uri; } server { listen 443 ssl; server_name blog.xxx.com; # localhost修改为您证书绑定的域名。 ssl_certificate /etc/nginx/cert/domain_ca.crt; # 将domain_ca.crt替换成您证书链的文件名(如下载文件没有domain_ca.crt则使用domain.crt或domain.pem)。 ssl_certificate_key /etc/nginx/cert/domain.key; # 将domain.key替换成您证书的密钥文件名。 ssl_session_timeout 5m; # 指定SSL/TLS会话的超时时间 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; #使用此加密套件。 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #使用该协议进行配置。 ssl_prefer_server_ciphers on; location / { root /home/public; #站点目录。 index index.html index.htm; } }\t创建docker-compose.yml文件，参考如下： 1 2 3 4 5 6 7 8 9 10 11 12 version: \u0026#39;3\u0026#39; services: blog: build: context: . dockerfile: Dockerfile image: blog:v1.0 container_name: blog restart: always ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; 构建并启动容器： 将ssl证书（可选）、blog.conf、Dockerfile、Dockerfile文件放到同一目录下，执行如下命令： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 一键构建并启动脚本 docker-compose up -d # 如果镜像版本不存在，会自动构建镜像，然后启动镜像 # 如需在启动时，都重新构建镜像可执行如下命令 docker-compose up -d --build # 单独构建镜像并启动容器 # 如当前操作环境的架构与目标机器的架构不同，可以构建镜像，并指定操作系统架构，如： docker build --platform linux/amd64,linux/arm64 -t blog:v1.0 . docker compose up -d # 停止脚本 docker-compose down 3.3 使用github pages托管 3.2 自建容器的方式部署，需要自备服务器、域名等，成本较高。不具备条件或者想节约成本，可以考虑白嫖github，使用github pages托管静态文件。只需要一些简单的配置即可。 上文我们已经创建了html文件的仓库：https://github.com/Williamyzd/blogs-html.git 我们只需要在仓库的设置中，开启github pages功能即可。\n进入仓库设置页面，选择pages选项卡 在Source中选择main分支，并选择/（根目录） 稍等片刻，即可通过域名访问了。 其中的自定义域名（标红的3），需要有自己的域名，并在域名解析中添加一条解析记录，指向github pages提供的域名。如本例中，域名pages.liu-nian.top需要添加的记录是：williamyzd.github.io。其模式为[github用户名].github.io。当前域名在百度智能云托管，其添加记录如下： 至此可以愉快地访问自己的博客了。\n4. 图床搭建-基于github 本人主要使用vscode撰写博客，vscode有丰富的插件。关于markdown相关的，当前使用到的如下： 在博客撰写过程中,不免要复制、粘贴图片，如果直接上传到博客中，涉及到图片存储为止的考虑。而huogo中如果想在编写和最终生产环境中能够展示图片，涉及到几处复杂配置。做过几次尝试便放弃了。博主之前使用过picgo等图床工具，可以直接将复制的图片上传到对应的文件服务器，同时生成一个文件的链接，直接插入文档即可。可惜picgo已经停止维护了。常见的图床常见的托管方如七牛、微博、腾讯云、阿里云等是免费的或一定限额的免费。处于不愿受制与人的考量，这里考虑使用github搭建一个自己的图床。\n4.1 创建仓库 选择创建一个公开的仓库，用于存放图片。 进入设置中，创建token，用于插件上传图片时使用。可直接点击链接配置：https://github.com/settings/personal-access-tokens github的开发者token可用api的方式对git仓库、账号进行控制。token分为两种，一种是账号级权限控制，另外一种是项目级别的控制。建议选用项目级别控制的 fine-grained personal token。 红框部分注意填写，其中过期时间根据自己需要填写。 注意选中repo权限，勾选repo的写入权限。 创建后页面会显示token信息，进行复制保存，在后续插件配置时使用。 4.2 安装vscode插件 这里使用markdown-image插件，安装后配置如下：\nbase 配置，主要是选github为默认存储方式、规定图片存储的格式等，其他内容酌情设置 File Name Format 这个选项比较关键，建议使用年月方式创建文件夹，当月的文件放在一起，以防止文件过多，导致github仓库管理麻烦货加载缓慢。 github配置，主要是填写仓库地址、token等信息。token信息来源于github账号中生成的token。 至此，图片上传配置完成。 参考链接：\n图标生成：https://www.iconfont.cn/search/index 博客搭建：https://gohugo.io/getting-started/quick-start/ ssh设置：https://docs.github.com/zh/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account 博客设置参考：https://yidude.com/html/15.html ","date":"2024-05-16T02:54:56+08:00","permalink":"https://blog.liu-nian.top/ops/%E6%90%AD%E5%BB%BAhugo%E5%8D%9A%E5%AE%A2/","title":"搭建Markdown博客--基于hugo"},{"content":"摘要: 本文详细介绍了Linux Volume Manager (LVM) 的相关命令，包括磁盘分区、物理卷创建、卷组管理、逻辑卷创建与管理、文件系统挂载与自动挂载等步骤。LVM 是一种逻辑卷管理技术，允许用户在物理磁盘上创建、扩展、缩减和移动分区，而不必重新分区或重新格式化磁盘。通过LVM，可以更灵活地管理存储空间，提高存储利用率和灵活性。\n参考链接：https://support.huaweicloud.com/bestpractice-evs/evs_02_0002.html\n磁盘分区 新挂载硬盘或者扩充硬盘\nparted相关命令 按照如下命令建立分区、文件系统格式、打上lvm标签\n1 2 3 4 5 6 7 8 9 lsblk # 查看挂载磁盘 parted /dev/vdd # 回车后按照以下操作： mklabel gpt # 大于2T以上磁盘使用该格式 mkpart p1 ext4 0 100GB # p1 分区名称,ext4 文件系统格式 0 起始位置（百分比or数字），100GB结束位置（百分比or数字） print #查看分区结果 toggle 3 lvm # 3 新创建分区的编号，lvm 标签，从而可以使用lvm来管理 quit # 退出parted命令 lsblk #查看分区情况 通过新增磁盘来创建物理卷 pvcreate /dev/vdd3\n物理卷 查看磁盘 1 2 3 fdisk -l | grep /dev/vd | grep -v vda Disk /dev/vdb: 10.7 GB, 10737418240 bytes, 20971520 sectors Disk /dev/vdc: 10.7 GB, 10737418240 bytes, 20971520 sectors 执行以下命令，将云硬盘创建为物理卷。 pvcreate 磁盘设备名1 磁盘设备名2 磁盘设备名3\n参数说明如下：\n磁盘设备名：此处需要填写磁盘的设备名称，如果需要批量创建，可以填写多个设备名称，中间以空格间隔。\n命令示例:\npvcreate /dev/vdb /dev/vdc\n1 2 3 pvcreate /dev/vdb /dev/vdc Physical volume \u0026#34;/dev/vdb\u0026#34; successfully created. Physical volume \u0026#34;/dev/vdc\u0026#34; successfully created. 执行如下命令，查看系统中物理卷的详细信息。 pvdisplay\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 pvdisplay \u0026#34;/dev/vdc\u0026#34; is a new physical volume of \u0026#34;10.00 GiB\u0026#34; --- NEW Physical volume --- PV Name /dev/vdc VG Name PV Size 10.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID dypyLh-xjIj-PvG3-jD0j-yup5-O7SI-462R7C \u0026#34;/dev/vdb\u0026#34; is a new physical volume of \u0026#34;10.00 GiB\u0026#34; --- NEW Physical volume --- PV Name /dev/vdb VG Name PV Size 10.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID srv5H1-tgLu-GRTl-Vns8-GfNK-jtHk-Ag4HHB 卷组 创建卷组 执行以下命令，创建卷组。 vgcreate 卷组名 物理卷名称1 物理卷名称2 物理卷名称3\u0026hellip;\n参数说明如下：\n​\t卷组名：可自定义，此处以vgdata为例。\n​\t物理卷名称：此处需要填写待添加进卷组的所有物理卷名称，中间以空格隔开。\n命令示例:\nvgcreate vgdata /dev/vdb /dev/vdc\n回显类似如下信息：\n1 2 vgcreate vgdata /dev/vdb /dev/vdc Volume group \u0026#34;vgdata\u0026#34; successfully created 执行如下命令，查看系统中卷组的详细信息。 vgdisplay\n卷组扩/缩容 查看磁盘，并如创建物理卷\nfdisk -l | grep /dev/vd | grep -v vda\npvcreate 磁盘**设备名\n添加新的物理卷\nvgextend 卷组名称 物理卷名称\n命令示例:\nvgextend vgdata /dev/vdd\n删除物理卷\nvgreduce 卷组名称 物理卷名称\n命令示例:\nvgreduce vgdata /dev/vdd\n逻辑卷 创建逻辑卷 lvcreate** -L 逻辑卷大小 -n 逻辑卷名称 卷组名称 参数说明如下：\n逻辑卷大小：该值应小于卷组剩余可用空间大小，单位可以选择“MB”或“GB”。\n逻辑卷名称：可自定义，此处以lvdata1为例。\n卷组名称：此处需要填写逻辑卷所在的卷组名称。\n命令示例:\nlvcreate -L 15GB -n lvdata1 vgdata\n1 2 lvcreate -L 15GB -n lvdata1 vgdata Logical volume \u0026#34;lvdata1\u0026#34; created. 执行如下命令，查询系统中逻辑卷的详细信息。\nlvdisplay\n扩充逻辑卷 执行如下命令，扩展逻辑卷的容量。 lvextend -L +增加容量 逻辑卷路径\n参数说明如下：\n增加容量：该值应小于组卷剩余可用空间大小，单位可以选择“MB”或“GB”。\n逻辑卷路径：此处需要填写待扩容的逻辑卷的路径。\n命令示例: lvextend -L +4GB /dev/vgdata/lvdata1\n回显类似如下信息：\n1 2 lvextend -L +4GB /dev/vgdata/lvdata1 Size of logical volume vgdata/lvdata1 changed from 15.00 GiB (3840 extents) to 19.00 GiB (4864 extents). Logical volume vgdata/lvdata1 successfully resized. 此时只是扩展的逻辑卷的容量，在其之上的文件系统也要随之进行扩展才能使用。\n执行如下命令，扩展文件系统的容量。\nresize2fs 逻辑卷路径\n命令示例: resize2fs /dev/vgdata/lvdata1\n回显类似如下信息：\n1 2 resize2fs /dev/vgdata/lvdata1 resize2fs 1.42.9 (28-Dec-2013) Filesystem at /dev/vgdata/lvdata1 is mounted on /Data1; on-line resizing required old_desc_blocks = 4, new_desc_blocks = 28 The filesystem on /dev/vgdata/lvdata1 is now 3657728 blocks long. 执行如下命令，查看文件系统容量是否增加。\ndf -h 回显类似如下信息：\n1 2 3 4 5 6 7 8 9 10 df -h Filesystem Size Used Avail Use% Mounted on /dev/vda2 39G 1.5G 35G 5% / devtmpfs 487M 0 487M 0% /dev tmpfs 496M 0 496M 0% /dev/shm tmpfs 496M 6.7M 490M 2% /run tmpfs 496M 0 496M 0% /sys/fs/cgroup /dev/vda1 976M 131M 779M 15% /boot tmpfs 100M 0 100M 0% /run/user/0 /dev/mapper/vgdata-lvdata1 19G 44M 18G 1% /Data1 可以看到，文件系统“/dev/mapper/vgdata-lvdata1”的容量相比之前增加了4GB。\n逻辑卷缩容 不可在线操作，需要先umount 挂载\n主要命令\numount /Data1 # /data1为挂载路径 e2fsck -f /dev/vgdata/lvdata1 # 进行磁盘检查 resize2fs /dev/vgdata/lvdata1 1G # 调整文件系统 lvreduce -L 1GB /dev/vgdata/lvdata # 通过lvduce命令进行缩 mount /dev/vgdata/lvdata1 /Data1 # 重新挂载 1 2 3 4 5 6 7 8 9 10 umount /Data1 # /data1为挂载路径 mount |grep Data1|wc -l # 查看是否成功卸载 lvs |grep Data1 # 查看磁盘状态 e2fsck -f /dev/vgdata/lvdata1 # 进行磁盘检查 resize2fs /dev/vgdata/lvdata1 1G # 调整文件系统 lvs |grep Data1 # 查看磁盘状态，未发生变化 lvreduce -L 1GB /dev/vgdata/lvdata # 通过lvduce命令进行缩 lvs |grep Data1 # 查看磁盘状态，可发现减少 mount /dev/vgdata/lvdata1 /Data1 # 重新挂载 df -lh |grep Data1 # 查看当前状态 挂载文件系统 执行如下命令，创建文件系统。 **mkfs.**文件格式 逻辑卷路径\n命令示例:\nmkfs.ext4 /dev/vgdata/lvdata1\n回显类似如下信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 mkfs.ext4 /dev/vgdata/lvdata1 mke2fs 1.42.9 (28-Dec-2013) Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) Stride=0 blocks, Stripe width=0 blocks 983040 inodes, 3932160 blocks 196608 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=2151677952 120 block groups 32768 blocks per group, 32768 fragments per group 8192 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208 Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): done Writing superblocks and filesystem accounting information: done 手动挂载 具备文件系统后才可以挂载\n创建挂载目录。 mkdir 挂载目录 命令示例: mkdir /Data1 执行如下命令，将文件系统挂载到目录下。 mount 逻辑卷路径 挂载目录 命令示例： mount /dev/vgdata/lvdata1 /Data1 执行如下命令，查询文件系统挂载信息。 mount | grep 挂载目录 命令示例： mount | grep /Data1 ​\t回显类似如下信息：\n1 2 mount | grep /Data1 /dev/mapper/vgdata-lvdata1 on /Data1 type ext4 (rw,relatime,data=ordered) 开机自动挂载 执行以下步骤，设置云服务器系统启动时自动挂载文件系统。\n执行如下命令，查询文件系统的UUID。\nblkid 文件系统路径\n以查询“dev/mapper/vgdata-lvdata1”的UUID为例：\nblkid /dev/mapper/vgdata-lvdata1\n1 2 blkid /dev/mapper/vgdata-lvdata1 /dev/mapper/vgdata-lvdata1: UUID=\u0026#34;c6a243ce-5150-41ac-8816-39db54d1a4b8\u0026#34; TYPE=\u0026#34;ext4\u0026#34; 执行以下命令，打开“/etc/fstab”文件。\nvi /etc/fstab\n1 2 3 4 5 6 7 8 9 10 11 vi /etc/fstab # # /etc/fstab # Created by anaconda on Tue Nov 7 14:28:26 2017 # # Accessible filesystems, by reference, are maintained under \u0026#39;/dev/disk\u0026#39; # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info # UUID=27f9be47-838b-4155-b20b-e4c5e013cdf3 / ext4 defaults 1 1 UUID=2b2000b1-f926-4b6b-ade8-695ee244a901 /boot ext4 defaults 1 2 按“i”进入编辑模式，将光标移至文件末尾，按“Enter”，添加如下内容。\n1 UUID=c6a243ce-5150-41ac-8816-39db54d1a4b8 /Data1 ext4 defaults 0 0 内容说明如下：\n第一列：UUID，此处填写1查询的UUID；\n第二列：文件系统的挂载目录；\n第三列：文件系统的文件格式，如文件格式“ext4”;\n第四列：挂载选项，此处以“defaults”为例；\n第五列：备份选项，设置为“1”时，系统自动对该文件系统进行备份；设置为“0”时，不进行备份。此处以“0”为例；\n第六列：扫描选项，设置为“1”时，系统在启动时自动对该文件系统进行扫描；设置为“0”时，不进行扫描。此处以“0”为例。\n按“Esc”，输入“:wq!”，并按“Enter”。保存设置并退出vi编辑器。\n执行以下步骤，验证自动挂载功能\n执行如下命令，卸载文件系统。\numount 逻辑卷路径\n命令示例： umount /dev/vgdata/lvdata1\n执行如下命令，将/etc/fstab文件所有内容重新加载。\nmount -a\n执行如下命令，查询文件系统挂载信息。\nmount | grep 挂载目录\n命令示例： mount | grep /Data1\n回显类似如下信息，说明自动挂载功能生效：\n1 2 mount | grep /Data1 /dev/mapper/vgdata-lvdata1 on /Data1 type ext4 (rw,relatime,data=ordered) ​\n","date":"2024-05-11T08:38:47Z","permalink":"https://blog.liu-nian.top/ops/lvm%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4/","title":"LVM相关命令"},{"content":"资源要求： 4C/8G\nhttps://kind.sigs.k8s.io/docs/user/known-issues/#failure-to-create-cluster-with-cgroups-v2\ncentos 7 在线升级内核 默认内核版本太低\n![image-20240513154841260](/Users/yangzedong/Library/Application Support/typora-user-images/image-20240513154841260.png)\n参考链接：https://www.cnblogs.com/liugp/p/16950443.html\n安装docker/kubectl kubectl :https://kubernetes.io/zh-cn/docs/tasks/tools/install-kubectl-linux/\n1 curl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; docker：https://www.runoob.com/docker/centos-docker-install.html\n1 curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun Helm:https://helm.sh/zh/docs/intro/install/\n安装k8s 参考文档：https://github.com/kubernetes-sigs/kind/tree/v0.22.0\n安装kind 二进制文件\n1 [ $(uname -m) = x86_64 ] \u0026amp;\u0026amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.21.0/kind-$(uname)-amd64 *可能比较慢，可以选择手动下载安装，二进制包下载地址：https://github.com/kubernetes-sigs/kind/releases\n1 2 3 4 # 执行安装操作 chmod +x ./kind chmod +x ./kind mv ./kind /usr/local/bin/ 拉取镜像（可选）\n1 2 3 4 5 6 7 8 # 选一个版本 $ docker search kindest/node NAME DESCRIPTION STARS OFFICIAL kindest/node https://sigs.k8s.io/kind node image 104 kindest/node-amd64 2 kindest/node-arm64 0 # 镜像拉取 docker pull kindest/node 安装集群\n1 2 3 4 # 在线创建集群 kind create cluster # 指定 配置文件 # kind create cluster --config kind-example-config.yaml 配置文件地址：https://kind.sigs.k8s.io/docs/user/configuration/\n安装ingress\n1 2 3 4 5 6 7 8 9 10 11 # 1. 获取yaml wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml # 2. 因服务器无外网，mac上进行下载，指定架构：一般是amd64和arm64、aarch64 # registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1 # registry.k8s.io/ingress-nginx/controller:v1.10.1 # registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1 docker pull --platform amd64 xxx # 3. 装载image kind load docker-image img:v1 # 或者 kind load image-archive my-image-archive.tar ![image-20240513220418988](/Users/yangzedong/Library/Application Support/typora-user-images/image-20240513220418988.png)\nkind架构 参考文档：https://kind.sigs.k8s.io/docs/design/initial/\nk8s中的端口映射\n参考地址：https://www.cnblogs.com/baixiaoyong/p/16051137.html\nk8s中的pod:https://fly-luck.github.io/2018/04/15/Kubernetes%20Ports/\ncontainerPort vs. hostPort 出现在如Deployment、Pod等资源对象描述文件中的容器部分，针对容器端口起类似于docker run -p \u0026lt;containerPort\u0026gt;:\u0026lt;hostPort\u0026gt;的作用： containerPort：容器暴露的端口。 hostPort：容器暴露的端口直接映射到的主机端口。\nport vs. targetPort vs. nodePort 出现在Service描述文件中，当Service的类型为ClusterIP时： port：Service中ClusterIP对应的端口。 targetport：clusterIP作为负载均衡， 后端目标实例（容器）的端口，与上述containerPort保持一致。\n当Service的类型为NodePort时： nodePort：由于ClusterIP只能集群内访问，配置nodePort会在每个运行kubelet节点的宿主机打开一个端口，用于集群外部访问。\n","date":"2024-05-11T08:38:25Z","permalink":"https://blog.liu-nian.top/ai/kind-%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4/","title":"kind 安装k8s集群"},{"content":"https://blog.csdn.net/qq836825331/article/details/136846624\n下载镜像 Kustomize安装 1 2 curl -s \u0026#34;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\u0026#34; | bash chmod +x kustomize \u0026amp;\u0026amp; mv kustomize /usr/local/bin/ 下载镜像 1 2 3 4 5 6 7 wget https://github.com/kubeflow/manifests/archive/refs/tags/v1.8.1.tar.gz tar -zxvf v1.8.1.tar.gz cd v1.8.1 # 过滤出镜像，注意需要手动做下排查 kustomize build example |grep \u0026#39;image: \u0026#39;|awk \u0026#39;$2 != \u0026#34;\u0026#34; { print $2}\u0026#39; |sort -u # 下载镜像，并push到dockerhub \u0026#34;for i in `cat images.txt`; do docker pull $i ;tag_w=`echo \u0026#34;$i\u0026#34; | sed \u0026#39;s/\\//_/g\u0026#39;`;tag_new=\u0026#34;williamyang1/kubeflow_$tag_w\u0026#34;;docker tag $i $tag_new ;docker push $tag_new ;docker rmi $tag_new $i;echo \u0026#34;$tag_new\u0026#34; \u0026gt;\u0026gt; n_images.txt; done\u0026#34; 最终的镜像列表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 williamyang1/busybox:1.28 williamyang1/docker.io_istio_pilot:1.17.5 williamyang1/docker.io_istio_proxyv2:1.17.5 williamyang1/docker.io_kubeflowkatib_earlystopping-medianstop:v0.16.0 williamyang1/docker.io_kubeflowkatib_enas-cnn-cifar10-cpu:v0.16.0 williamyang1/docker.io_kubeflowkatib_file-metrics-collector:v0.16.0 williamyang1/docker.io_kubeflowkatib_katib-controller:v0.16.0 williamyang1/docker.io_kubeflowkatib_katib-db-manager:v0.16.0 williamyang1/docker.io_kubeflowkatib_katib-ui:v0.16.0 williamyang1/docker.io_kubeflowkatib_mxnet-mnist:v0.16.0 williamyang1/docker.io_kubeflowkatib_pytorch-mnist-cpu:v0.16.0 williamyang1/docker.io_kubeflowkatib_suggestion-darts:v0.16.0 williamyang1/docker.io_kubeflowkatib_suggestion-enas:v0.16.0 williamyang1/docker.io_kubeflowkatib_suggestion-goptuna:v0.16.0 williamyang1/docker.io_kubeflowkatib_suggestion-hyperband:v0.16.0 williamyang1/docker.io_kubeflowkatib_suggestion-hyperopt:v0.16.0 williamyang1/docker.io_kubeflowkatib_suggestion-optuna:v0.16.0 williamyang1/docker.io_kubeflowkatib_suggestion-pbt:v0.16.0 williamyang1/docker.io_kubeflowkatib_suggestion-skopt:v0.16.0 williamyang1/docker.io_kubeflowkatib_tfevent-metrics-collector:v0.16.0 williamyang1/docker.io_kubeflowmanifestswg_oidc-authservice:e236439 williamyang1/docker.io_kubeflownotebookswg_centraldashboard:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_jupyter-web-app:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_kfam:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_notebook-controller:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_poddefaults-webhook:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_profile-controller:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_pvcviewer-controller:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_tensorboard-controller:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_tensorboards-web-app:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_volumes-web-app:v1.8.0 williamyang1/docker.io_metacontrollerio_metacontroller:v2.0.4 williamyang1/docker.io_seldonio_mlserver:1.3.2 williamyang1/gcr.io_knative-releases_knative.dev_eventing_cmd_controller@sha256:92967bab4ad8f7d55ce3a77ba8868f3f2ce173c010958c28b9a690964ad6ee9b williamyang1/gcr.io_knative-releases_knative.dev_eventing_cmd_mtping@sha256:6d35cc98baa098fc0c5b4290859e363a8350a9dadc31d1191b0b5c9796958223 williamyang1/gcr.io_knative-releases_knative.dev_eventing_cmd_webhook@sha256:ebf93652f0254ac56600bedf4a7d81611b3e1e7f6526c6998da5dd24cdc67ee1 williamyang1/gcr.io_knative-releases_knative.dev_net-istio_cmd_controller@sha256:421aa67057240fa0c56ebf2c6e5b482a12842005805c46e067129402d1751220 williamyang1/gcr.io_knative-releases_knative.dev_net-istio_cmd_webhook@sha256:bfa1dfea77aff6dfa7959f4822d8e61c4f7933053874cd3f27352323e6ecd985 williamyang1/gcr.io_knative-releases_knative.dev_serving_cmd_activator@sha256:c2994c2b6c2c7f38ad1b85c71789bf1753cc8979926423c83231e62258837cb9 williamyang1/gcr.io_knative-releases_knative.dev_serving_cmd_autoscaler@sha256:8319aa662b4912e8175018bd7cc90c63838562a27515197b803bdcd5634c7007 williamyang1/gcr.io_knative-releases_knative.dev_serving_cmd_controller@sha256:98a2cc7fd62ee95e137116504e7166c32c65efef42c3d1454630780410abf943 williamyang1/gcr.io_knative-releases_knative.dev_serving_cmd_domain-mapping@sha256:f66c41ad7a73f5d4f4bdfec4294d5459c477f09f3ce52934d1a215e32316b59b williamyang1/gcr.io_knative-releases_knative.dev_serving_cmd_domain-mapping-webhook@sha256:7368aaddf2be8d8784dc7195f5bc272ecfe49d429697f48de0ddc44f278167aa williamyang1/gcr.io_knative-releases_knative.dev_serving_cmd_queue@sha256:dabaecec38860ca4c972e6821d5dc825549faf50c6feb8feb4c04802f2338b8a williamyang1/gcr.io_knative-releases_knative.dev_serving_cmd_webhook@sha256:4305209ce498caf783f39c8f3e85dfa635ece6947033bf50b0b627983fd65953 williamyang1/gcr.io_kubebuilder_kube-rbac-proxy:v0.13.1 williamyang1/gcr.io_kubebuilder_kube-rbac-proxy:v0.8.0 williamyang1/gcr.io_ml-pipeline_api-server:2.0.5 williamyang1/gcr.io_ml-pipeline_cache-server:2.0.5 williamyang1/gcr.io_ml-pipeline_frontend williamyang1/gcr.io_ml-pipeline_frontend:2.0.5 williamyang1/gcr.io_ml-pipeline_metadata-writer:2.0.5 williamyang1/gcr.io_ml-pipeline_minio:RELEASE.2019-08-14T20-37-41Z-license-compliance williamyang1/gcr.io_ml-pipeline_mysql:8.0.26 williamyang1/gcr.io_ml-pipeline_persistenceagent:2.0.5 williamyang1/gcr.io_ml-pipeline_scheduledworkflow:2.0.5 williamyang1/gcr.io_ml-pipeline_viewer-crd-controller:2.0.5 williamyang1/gcr.io_ml-pipeline_visualization-server williamyang1/gcr.io_ml-pipeline_workflow-controller:v3.3.10-license-compliance williamyang1/gcr.io_tfx-oss-public_ml_metadata_store_server:1.14.0 williamyang1/ghcr.io_dexidp_dex:v2.36.0 williamyang1/kserve_kserve-controller:v0.11.2 williamyang1/kserve_lgbserver:v0.11.2 williamyang1/kserve_models-web-app:v0.10.0 williamyang1/kserve_paddleserver:v0.11.2 williamyang1/kserve_pmmlserver:v0.11.2 williamyang1/kserve_sklearnserver:v0.11.2 williamyang1/kserve_xgbserver:v0.11.2 williamyang1/kubeflow_training-operator:v1-855e096 williamyang1/mysql:8.0.29 williamyang1/nvcr.io_nvidia_tritonserver:23.05-py3 williamyang1/python:3.7 williamyang1/pytorch_torchserve-kfs:0.8.2 williamyang1/quay.io_jetstack_cert-manager-cainjector:v1.12.2 williamyang1/quay.io_jetstack_cert-manager-controller:v1.12.2 williamyang1/quay.io_jetstack_cert-manager-webhook:v1.12.2 williamyang1/tensorflow_serving:2.6.2 安装k8s Kubesphere 安装k8s\u0026ndash;v1.27.10 https://kubesphere.io/zh/docs/v3.4/quick-start/all-in-one-on-linux/\nhttps://kubesphere.io/zh/blogs/using-kubekey-deploy-k8s-v1.28.8/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 安装基础软件 yum install -y socat conntrack ebtables ipset ipvsadm # 安装工具 # shell安装 或者离线下载 curl -sfL https://get-kk.kubesphere.io | sh - ## 离线下载 https://github.com/kubesphere/kubekey/releases/download/v3.1.1/kubekey-v3.1.1-linux-amd64.tar.gz chmod +x kk mv kk /usr/local/bin/ # 确认是否包含想要安装的版本 kk version --show-supported-k8s # 创建manifest 导出供离线下载使用 kk create manifest --with-kubernetes v1.27 # 下载基础环境镜像-manifest用到的工具，单独下载--可选 # 镜像所在地址 https://github.com/kubesphere/kubekey/releases/tag/v3.1.1 # 找一台访问速度快的机器下载 https://github.com/kubesphere/kubekey/releases/download/v3.1.1/centos7-rpms-amd64.iso # 这一步骤会下载manifest-sample.yaml中涉及的的工具如kubelet/containerd/helm等 ./kk artifact export -m manifest-sample.yaml -o kubesphere.tar.gz # 此步骤为方便没有外网的机器创建集群。 ./kk create cluster -f config-sample.yaml -a kubesphere.tar.gz --with-packages 安装kubeflow https://github.com/kubeflow/manifests\n1 2 # 下载镜像 for i in `cat images.txt`; do ctr imgaes pull $i ;done ","date":"2024-05-11T08:38:25Z","permalink":"https://blog.liu-nian.top/ai/kubeflow%E5%AE%89%E8%A3%85/","title":"Kubeflow 安装"},{"content":"参考文献：https://www.jianshu.com/p/d78fff321005\n数据存储格式：WKT VS GeoJSON WKT是什么？ WKT(well-know text)是开放地理空间联盟OGC制定的一种文本标记语言，用于表示矢量几何对象、空间参照系统及空间参照系统之间的转换。\nWKB是什么？ WKB(well-know binary)是WKT的二进制表现形式，解决WKT表达冗余的问题，便于传输和存储在数据库中。\nGeoJSON是什么？ 以JSON的格式输出空间数据，便于被javascript等脚本调用。\nWKT与GeoJSON WKT与GeoJSON分为点、线、面、几何集合四种： 点 线 面 组合 Point MultiPoint LineString MultiLineString Polygon MultiPolygon GeometryCollection 类型 WKT GeoJSON Point POINT(10 10) { \u0026ldquo;type\u0026rdquo;: \u0026ldquo;Point\u0026rdquo;, \u0026ldquo;coordinates\u0026rdquo;: [10, 10] } LineString LINESTRING(10 10, 20 30) { \u0026ldquo;type\u0026rdquo;: \u0026ldquo;Point\u0026rdquo;, \u0026ldquo;coordinates\u0026rdquo;:[ [10, 10], [20, 30] ] } Polygon POLYGON(10 10, 15 16, 22 10, 30 32) { \u0026ldquo;type\u0026rdquo;: \u0026ldquo;Polygon\u0026rdquo;, \u0026ldquo;coordinates\u0026rdquo;: [ [ [10, 10], [10, 10], [10, 10], [10, 10] ] ] } MultiPoint MULTIPOINT(*) { \u0026ldquo;type\u0026rdquo;: \u0026ldquo;MultiPoint\u0026rdquo;, \u0026ldquo;coordinates\u0026rdquo;: [*] } MultiLineString MULTILINESTRING (*) { \u0026ldquo;type\u0026rdquo;: \u0026ldquo;MultiLineString\u0026rdquo;, \u0026ldquo;coordinates\u0026rdquo;: [*] } MultiPolygon MULTIPOLYGON (*) { \u0026ldquo;type\u0026rdquo;: \u0026ldquo;MultiPolygon\u0026rdquo;, \u0026ldquo;coordinates\u0026rdquo;: [*] } GEOMETRYCOLLECTION GEOMETRYCOLLECTION(POINT(2 3),LINESTRING(2 3,3 4)) - WKT与GeoJSON的区别 WKT是用来单独表示空间点、线、面数据，GeoJSON还可以用来表示空间数据和属性数据的集合 （crs、bbox属性）。 使用GeoPandas 读取 wkt字符串数据 获取山东省地图土地轮廓并使用GeoPandas加载展示\n通过百度地图api获取数据信息 将形状字符串转换为MultiPolygon类型数据 通过GeoPandas加载数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # encoding:utf-8 # 根据您选择的AK已为您生成调用代码 # 检测到您当前的AK设置了IP白名单校验 # 您的IP白名单中的IP非公网IP，请设置为公网IP，否则将请求失败 # 请在IP地址为0.0.0.0/0 外网IP的计算发起请求，否则将请求失败 import requests sds =None # 服务地址 host = \u0026#34;https://api.map.baidu.com\u0026#34; # 接口地址 uri = \u0026#34;/api_region_search/v1/\u0026#34; # 此处填写你在控制台-应用管理-创建应用后获取的AK ak = \u0026#34;\u0026#34; params = { \u0026#34;keyword\u0026#34;: \u0026#34;山东省\u0026#34;, \u0026#34;sub_admin\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;ak\u0026#34;: ak, \u0026#34;extensions_code\u0026#34;:1, \u0026#34;boundary\u0026#34;:1, } response = requests.get(url = host + uri, params = params) sds =None if response: # print(response.json()) sds = response.json() print(sds[\u0026#39;districts\u0026#39;]) # pd.DataFrame(sds[\u0026#39;districts\u0026#39;]) ","date":"2024-05-11T08:38:25Z","permalink":"https://blog.liu-nian.top/ai/%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E5%8F%AF%E8%A7%86%E5%8C%96/","title":"地理信息可视化"},{"content":"AI vs ML vs DL AI:人工智能 人工智能是在1956年达特茅斯会议上首先提出的。该会议确定了人工智能的目标是“实现能够像人类一样利用知识去解决问题的机器”。\n像人一样思考：认知科学\n像人一样行动：图灵测试（1950）的途径\n自然语言处理 知识表示 自动推理 机器学习 完全图灵测试：\n计算视觉 机器人学 合理地思考：逻辑学\n合理地行动：强化学习\n机器学习 学习的定义： Herbert A. Simon：如果一个系统能够通过执行某个过程改进他的性能，这就是学习。\nTom M. Mitchell：\nStudy of algorithms that\nimprove their performance P at some task T withexperience E well-defined learning task: \u0026lt;P,T,E\u0026gt; 一个程序被认为能从经验E中学习，解决任务 T，达到性能度量值P，当且仅当，有了经验E后，经过P评判， 程序在处理 T 时的性能有所提升。\n**机器学习：**从原始数据中获取模式(规律) 深度学习 表示学习：自动地学习出有效特征, 并最终提升机器学习模型性能的方法。 局部学习：0ne-hot编码\n分布表示：连续表示（RGB）、词向量（词嵌入）\n特征工程 VS 表示学习：\n特征工程：人工提取特征\n表示学习：自动获取特征\n深度学习： 统计学 vs 统计学习 vs 机器学习 参考文献：http://brenocon.com/blog/2008/12/statistics-vs-machine-learning-fight/\n统计学 ：Statistics\n统计学习：Statistical learning\n统计机器学习:Statistical machine learning\n机器学习：Machine Learning\n机器学习 vs 数据挖掘 •机器学习是数据挖掘的重要工具。\n•数据挖掘不仅仅要研究、拓展、应用一些机器学习方法，还要通过许多非机器学习技术解决数据仓储、大规模数据、数据噪音等等更为实际的问题。\n•机器学习的涉及面更宽，常用在数据挖掘上的方法通常只是“从数据学习”，然则机器学习不仅仅可以用在数据挖掘上，一些机器学习的子领域甚至与数据挖掘关系不大，例如增强学习与自动控制等等。\n•数据挖掘试图从海量数据中找出有用的知识。\n•大体上看，数据挖掘可以视为机器学习和数据库的交叉，它主要利用机器学习界提供的技术来分析海量数据，利用数据库界提供的技术来管理海量数据。\n冯诺依曼机的现代观点：AI vs 大数据 vs 云计算 https://baike.baidu.com/item/3D%E7%A3%81%E5%AD%98%E5%82%A8%E5%99%A8/22383847\n机器学习的三要素 机器学习=模型+策略+算法(优化算法)\n[附件]\n模型:决策函数、概率模型 在监督学习中，模型就是所要学习的条件概率分布或决策函数。 模型的假设空间F 包含所有可能的条件概率分布或决策函数。 假设空间可以是决策函数的集合 $$ F = {f |Y = f_\\theta(X),\\theta\\in R^n $$\n也可以是条件概率函数的集合 $$ F = {F | P_\\theta(Y|X),\\theta\\in R^n $$\n简单理解：\n$Y= aX^2 +bX + c$\n策略:选择模型的准则 资源：https://mp.weixin.qq.com/s/LN1X4TVgfF-FRhMlqEUVpw\n损失函数：一次预测的好坏 0-1损失函数 $$ L(Y,f(X)) = \\begin{cases} 1 \u0026amp;Y=f(X) \\ 0 \u0026amp;Y{=}\\mathllap{/,} f(X) \\end{cases} $$\n平方损失函数 $$ L(Y,f(X)) = (Y-f(X))^2 $$\n绝对损失函数 $$ L(Y,f(X)) = |Y-f(X)| $$\n对数损失函数 $$ L(Y,f(X)) = -logP(Y|X) $$\n风险函数：平均意义下模型预测的好坏 经验风险最小化最优模型\n$$ min_{\\substack{f\\in F}}\\frac{1}{N}\\sum_{i=1}L(y_i,f(x_i)) $$\n算法:学习模型的具体算法 –解析解 –无解析解:数值计算\n参考链接：https://zh.d2l.ai/chapter_optimization/index.html\n梯度下降 梯度下降法的计算过程就是沿梯度下降的方向求解极小值，也可以沿梯度上升方向求解最大值。\n$$ x \\leftarrow x - \\eta f\u0026rsquo;(x) $$\n学习率（learning rate）决定目标函数能否收敛到局部最小值，以及何时收敛到最小值\n批量梯度下降：每进行1次参数更新，需要计算整个数据样本集 随机梯度下降：随机均匀选择一部分样本做梯度下降 小批量梯度下降法：随机选择一批样本 动量优化法 动量优化方法引入物理学中的动量思想，加速梯度下降，有Momentum和Nesterov两种算法。当我们将一个小球从山上滚下来，没有阻力时，它的动量会越来越大，但是如果遇到了阻力，速度就会变小，动量优化法就是借鉴此思想，使得梯度方向在不变的维度上，参数更新变快，梯度有所改变时，更新参数变慢，这样就能够加快收敛并且减少动荡。\n$$ V_t \\leftarrow \\beta V_{t-1} + g_{t,t-1} $$\n$$ X_t \\leftarrow x_{t-1} - \\eta_t V_t $$\n自适应学习率优化算法 在机器学习中，学习率是一个非常重要的超参数，但是学习率是非常难确定的，虽然可以通过多次训练来确定合适的学习率，但是一般也不太确定多少次训练能够得到最优的学习率，玄学事件，对人为的经验要求比较高，所以是否存在一些策略自适应地调节学习率的大小，从而提高训练速度。\nAdaGrad RMSprop Adadelta Adam：Adaptive Moment Estimation 机器学习的分类 模型选择：正则化、交叉验证 选择模型的方法主要包括正则化、交叉验证\n正则化：在经验风险上加一个正则化项(Regularizer)或罚项(Penalty term)。 一般形式：\n$$ min_{\\substack{f\\in F}}\\frac{1}{N}\\sum_{i=1}L(y_i,f(x_i)) + \\lambda J(f) $$\n二阶形式：\n$$ min_{\\substack{f\\in F}}\\frac{1}{N}\\sum_{i=1}L(y_i,f(x_i)) + \\lambda||w|| $$\n一阶形式：\n$$ min_{\\substack{f\\in F}}\\frac{1}{N}\\sum_{i=1}L(y_i,f(x_i)) + \\lambda||w|| $$\n交叉验证：将数据分为训练集、验证集、测试集 简单交叉验证：训练集、测试集\nS折交叉验证:S个互不相交的大小相同的子集，用S-1个子集训练模型\n留一交叉验证:S=N\n机器学习任务的一般流程 准备工作： 准备数据 框架问题-业务目标、解决方案 选择性能指标（均方差、交叉熵等）与评测方案 检验假设-与实际生产环境对齐 数据获取、分析、处理 获取数据 分析数据（可视化） 数据清洗 特征工程 模型训练 模型测试、评估 模型服务（上架） 房价预测案例\n","date":"2022-03-11T08:38:25Z","permalink":"https://blog.liu-nian.top/ai/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/","title":"机器学习概述"},{"content":"建议书籍： 实战书籍\n《机器学习实战（原书第2版）》 基于sklearn、Tesnsorflow\nhttps://book.douban.com/subject/35218199/﻿\n动手学深度学习》 (书籍、代码、视频-沐神出品，可以关注沐神的知乎和b站，经常会发干货)\nhttp://zh.d2l.ai/chapter_preface/index.html﻿\n漫画机器学 https://book.sciencereading.cn/shop/book/Booksimple/onlineRead.do?id=BBFFD36F5A94C6D7DE053010B0A0AB613000\u0026amp;readMark=0﻿\n理论书籍\n《机器学习》 （西瓜书-周志华-比较通俗易懂）\nhttps://book.douban.com/subject/26708119/﻿\n《统计学习方法（第2版）》 （大量公式-比较难啃，但是知识点非常全面，类似于知识清单）\nhttps://book.douban.com/subject/33437381/﻿\n*《深度学习》\n（花书-深度学习三巨头鸿篇巨制）\nhttps://book.douban.com/subject/27087503/﻿\nNLP相关\n《文本数据挖掘》 nlp-领域大佬新做\nhttps://book.douban.com/subject/34441323/﻿\n《Speech and Language Processing (3rd ed. draft)》 斯坦福大佬著作，不断更新中。中文译名，《自然语言处理综述》，对应版本为第二版。\nhttps://web.stanford.edu/~jurafsky/slp3/﻿\n参考资料：\nhttps://llmbook-zh.github.io/LLMBook.pdf\n标量、向量、张量 标量： 一个单独的数字 向量 ：一个向量是一列数。词向量就是一列数字。 张量： 从视觉角度： 一阶张量是一个一维数组，即一组数，我们可以将一组数表示为一个矢量\n二阶张量是一个矩阵，因此我们可以将张量作为矢量和矩阵概念的推广。\n三阶张量 对于 RGB 图片，我们可以理解为由三张分别表示 R,G,B 分量的图片堆叠而成，如下\n对于每个分量图片，我们都可以看成一个矩阵，那么一张 RGB 图片就可以用三阶的张量进行表示。\n四阶张量 对于多张 RGB 图片，我们可以用四阶张量进行表示，可以看作是三阶张量（单张RGB图像）的数组（多张 RGB 图片）。\n五阶张量 视频是由多张图片组成的，因此每个视频可以用一个四阶张量表示，显然，多个视频可以用五阶张量表示。\n从NLP角度： 一阶张量一个字或词\n二阶张量是一句话。\n三阶以上的张量：一段话或者一篇文章，或者是将大量的语句分成若干个批次。\n﻿\n分词与词向量 参考链接：AI大模型基础：1.分词_大模型分词-CSDN博客﻿\n为什么要分词？ 自然语言类任务的流程中分词是重要一环 分词是自然语言处理的基础，分词准确度直接决定了后面的词性标注、句法分析、词向量以及文本分析的质量。英文语句使用空格将单词进行分隔，除了某些特定词，如how many，New York等外，大部分情况下不需要考虑分词问题。但中文不同，天然缺少分隔符，需要读者自行分词和断句。故在做中文自然语言处理时，需要先进行分词。\n﻿\n﻿\n分词就是将句子、段落、文章这种长文本，分解为以字词甚至更小单位的数据结构\n﻿\n中文分词难点 中文分词不像英文那样，天然有空格作为分隔。而且中文词语组合繁多，分词很容易产生歧义。因此中文分词一直以来都是NLP的一个重点，也是一个难点。难点主要集中在分词标准，切分歧义和未登录词三部分。\n中文分词算法 传统的分词方法：\n主要分为两类，基于词典的规则匹配方法，和基于统计的机器学习方法。\n基于词典的分词算法 基于词典的分词算法，本质上就是字符串匹配。将待匹配的字符串基于一定的算法策略，和一个足够大的词典进行字符串匹配，如果匹配命中，则可以分词。根据不同的匹配策略，又分为正向最大匹配法，逆向最大匹配法，双向匹配分词，全切分路径选择等。\n最大匹配法主要分为三种：\n正向最大匹配法 ，从左到右对语句进行匹配，匹配的词越长越好。比如“商务处女干事”，划分为“商务处/女干事”，而不是“商务/处女/干事”。这种方式切分会有歧义问题出现，比如“结婚和尚未结婚的同事”，会被划分为“结婚/和尚/未/结婚/的/同事”。\n逆向最大匹配法 ，从右到左对语句进行匹配，同样也是匹配的词越长越好。比如“他从东经过我家”，划分为“他/从/东/经过/我家”。这种方式同样也会有歧义问题，比如“他们昨日本应该回来”，会被划分为“他们/昨/日本/应该/回来”。\n双向匹配分词 ，则同时采用正向最大匹配和逆向最大匹配，选择二者分词结果中词数较少者。但这种方式同样会产生歧义问题，比如“他将来上海”，会被划分为“他/将来/上海”。由此可见，词数少也不一定划分就正确。\n全切分路径选择 ，将所有可能的切分结果全部列出来，从中选择最佳的切分路径。分为两种选择方法\nn最短路径方法。将所有的切分结果组成有向无环图，切词结果作为节点，词和词之间的边赋予权重，找到权重和最小的路径即为最终结果。比如可以通过词频作为权重，找到一条总词频最大的路径即可认为是最佳路径。\nn元语法模型。同样采用n最短路径，只不过路径构成时会考虑词的上下文关系。一元表示考虑词的前后一个词，二元则表示考虑词的前后两个词。然后根据语料库的统计结果，找到概率最大的路径。\n子词分词 BPE: 在 1994 年,BPE 算法被提出,最早用于通用的数据压缩 [137]。随后,自然 语言处理领域的研究人员将其进行适配,并应用于文本分词 [138]。BPE 算法从一 组基本符号(例如字母和边界字符)开始,迭代地寻找语料库中的两个相邻词元, 并将它们替换为新的词元,这一过程被称为合并。合并的选择标准是计算两个连 续词元的共现频率,也就是每次迭代中,最频繁出现的一对词元会被选择与合并。 合并过程将一直持续达到预定义的词表大小。\n算法逻辑：\n准备足够大的训练语料 确定期望的subword词表大小 将单词拆分为字符序列并在末尾添加后缀“ \u0026lt;/ w\u0026gt;”，统计单词频率。 本阶段的subword的粒度是字符。 例如，“ low”的频率为5，那么我们将其改写为“ l o w \u0026lt;/ w\u0026gt;”：5 统计每一个连续字节对的出现频率，选择最高频者合并成新的subword 重复第4步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1 停止符\u0026quot;\u0026lt;/w\u0026gt;\u0026ldquo;的意义在于表示subword是词后缀。举例来说：\u0026ldquo;st\u0026quot;字词不加\u0026rdquo;\u0026lt;/w\u0026gt;\u0026ldquo;可以出现在词首如\u0026quot;st ar\u0026rdquo;，加了\u0026rdquo;\u0026lt;/w\u0026gt;\u0026ldquo;表明改字词位于词尾，如\u0026quot;wide st \u0026lt;/w\u0026gt;\u0026quot;，二者意义截然不同。\n每次合并后词表可能出现3种变化：\n+1，表明加入合并后的新字词，同时原来的2个子词还保留（2个字词不是完全同时连续出现） +0，表明加入合并后的新字词，同时原来的2个子词中一个保留，一个被消解（一个字词完全随着另一个字词的出现而紧跟着出现） -1，表明加入合并后的新字词，同时原来的2个子词都被消解（2个字词同时连续出现） 实际上，随着合并的次数增加，词表大小通常先增加后减小。\n﻿\n论文地址：\n﻿https://aclanthology.org/P18-1007.pdf﻿\n﻿https://zhuanlan.zhihu.com/p/86965595﻿\n﻿\nWordPiece ： WordPiece 是谷歌内部非公开的分词算法,最初是由谷歌研究人员在开发语音 搜索系统时提出的 [140]。随后,在 2016 年被用于机器翻译系统 [141],并于 2018 年被 BERT 采用作为分词器 [13]。WordPiece 分词和 BPE 分词的想法非常相似,都 是通过迭代合并连续的词元,但是合并的选择标准略有不同。在合并前,WordPiece 分词算法会首先训练一个语言模型,并用这个语言模型对所有可能的词元对进行 评分。然后,在每次合并时,它都会选择使得训练数据的似然性增加最多的词元 对。 由于谷歌并未发布 WordPiece 分词算法的官方实现,这里我们参考了 Hugging Face 在线自然语言课程中给出的 WordPiece 算法的一种实现。与 BPE 类似,WordPiece 分词算法也是从一个小的词汇表开始,其中包括模型使用的特殊词元和初始 词汇表。由于它是通过添加前缀(如 BERT 的##)来识别子词的,因此每个词的初 始拆分都是将前缀添加到词内的所有字符上。举例来说,“word”会被拆分为:“w ##o ##r ##d”。与 BPE 方法的另一个不同点在于,WordPiece 分词算法并不选择最 频繁的词对,而是使用下面的公式为每个词对计算分数:\nUnigram: BPE存在的问题：一个词可能有多个分法\n与 BPE 分词和 WordPiece 分词不同,Unigram 分词方法 [142] 从语料库的一 组足够大的字符串或词元初始集合开始,迭代地删除其中的词元,直到达到预期 的词表大小。它假设从当前词表中删除某个词元,并计算训练语料的似然增加情 况,以此来作为选择标准。这个步骤是基于一个训练好的一元语言模型来进行的。 为估计一元语言模型,它采用期望最大化(Expectation–Maximization, EM)算法: 在每次迭代中,首先基于旧的语言模型找到当前最优的分词方式,然后重新估计 一元概率从而更新语言模型。这个过程中一般使用动态规划算法(即维特比算法, Viterbi Algorithm)来高效地找到语言模型对词汇的最优分词方式。采用这种分词 方法的代表性模型包括 T5 和 mBART。\n论文地址：\n﻿https://aclanthology.org/P16-1162.pdf﻿\n﻿[UIM]论文解读：subword Regularization: Multiple Subword Candidates_unigram language model论文-CSDN博客﻿\n﻿\nByte-level BPE(BBPE) 基础知识：\nUnicode： Unicode 是一种字符集，旨在涵盖地球上几乎所有的书写系统和字符。它为每个字符分配了一个唯一的代码点（code point）用于标识字符。Unicode 不关注字符在计算机内部的具体表示方式，而只是提供了一种字符到代码点的映射。Unicode 的出现解决了字符集的碎片化问题，使得不同的语言和字符能够在一个共同的标准下共存。然而，Unicode 并没有规定如何在计算机内存中存储和传输这些字符。\nUTF-8： UTF-8（Unicode Transformation Format-8）是一种变长的字符编码方案，它将 Unicode 中的代码点转换为字节序列。UTF-8 的一个重要特点是它是向后兼容 ASCII 的，这意味着标准的 ASCII 字符在 UTF-8 中使用相同的字节表示，从而确保现有的 ASCII 文本可以无缝地与 UTF-8 共存。在 UTF-8 编码中，字符的表示长度可以是1到4个字节，不同范围的 Unicode 代码点使用不同长度的字节序列表示，这样可以高效地表示整个 Unicode 字符集。UTF-8 的编码规则是：\n单字节字符（ASCII 范围内的字符）使用一个字节表示，保持与 ASCII 编码的兼容性。 带有更高代码点的字符使用多个字节表示。UTF-8 使用特定的字节序列来指示一个字符所需的字节数，以及字符的实际数据。 例如，英文字母 \u0026ldquo;A\u0026rdquo; 的 Unicode 代码点是U+0041，在 UTF-8 中表示为 0x41（与 ASCII 编码相同）；而中文汉字 \u0026ldquo;你\u0026rdquo; 的 Unicode 代码点是U+4F60，在 UTF-8 中表示为0xE4 0xBD 0xA0三个字节的序列。\n所以简单的来说：\nUnicode 是字符集，为每个字符分配唯一的代码点。 UTF-8 是一种基于 Unicode 的字符编码方式，用于在计算机中存储和传输字符。 Byte(字节) ：计算机存储和数据处理时，字节是最小的单位。一个字节包含8个(Bit)二进制位，每个位可以是0或1，每位的不同排列和组合可以表示不同的数据，所以一个字节能表示的范围是256个。\n言归正传：\nByte-level BPE(BBPE)和Byte-Pair Encoding (BPE)区别就是BPE是最小词汇是字符级别，而BBPE是字节级别的，通过UTF-8的编码方式这一个字节的256的范围，理论上可以表示这个世界上的所有字符。\n所以实现的步骤和BPE就是实现的粒度不一样，其他的都是一样的。\n初始词表：构建初始词表，包含一个字节的所有表示(256)。 构建频率统计：统计所有子词单元对（两个连续的子词）在文本中的出现频率。 合并频率最高的子词对：选择出现频率最高的子词对，将它们合并成一个新的子词单元，并更新词汇表。 重复合并步骤：不断重复步骤 2 和步骤 3，直到达到预定的词汇表大小、合并次数，或者直到不再有有意义的合并（即，进一步合并不会显著提高词汇表的效益）。 分词：使用最终得到的词汇表对文本进行分词。 获取词向量： 独热表示（Onehotrepresentation） 在自然语言处理的各项任务中，需要将字、词进行编码，形成数字表达的向量的形式，以便进行运算。常见的一种编码方式是独热表示（Onehotrepresentation）。这种编码方式是很多编码方式的基础。具体做法是首先有一个词汇表V，并按指定规则对词汇进行排序。定义一个向量用来表示词汇，向量大小同词汇表大小。词汇w的向量表示为c(w)，其在词汇表中的次序是i。c(w)的值第i位为1，其他位置是0。以‘我爱NLP’未例，具体表示如下：\nw= ‘我‘，在词表中的次序是1；\nc(w) =[0,1,0,0,…,0,0,0]；\nW = ‘爱‘，在词表中的次序是3；\nc(w) =[0,0,0,1,…,0,0,0]；\nW = ‘NLP‘，在词表中的次序是2；\nc(w)=[0,1,0,0,…,0,0,0]；\n独热表示存在如下的问题：\n独热向量中的每一个位置都表示一个单词，但词汇之间没有相关性； 向量大小受词表的大小影响，词汇量大时便造成维数爆炸问题； 该词向量也无法表示未出现的词； 分布表示（DistributedRepresentation） 另外一种词汇的表示是Hinton提出的[49]分布表示（DistributedRepresentation）。主要做法是基于大量的文本数据学习，将语言中每个词汇对应到固定大小的向量。其长度一般远远小于词表大小。每个词汇可以再空间中映射成某一点，从而将距离概念引入词的表达，可以便捷地计算词汇之间在句法、语义上的相似性。\n在文本的表示方面，主要有两种不同的表示方式：上下文无关的文本表示和上下文相关的文本表示。上下文无关的文本表示以Word2vec为代表，每个词映射成固定的词向量；上下文相关的文本表示以ELMO为发端，每个词汇表示为动态的词向量，根据上下文的不同，词向量不同。Word2vec不能解决一词多义的问题，而且存在词表之外词的表示问题（OOV）。\nNNLM时代 Word2Vec时代 ELMO-基于上下文的动态词向量 ELMO[55]是一种基于语境的词汇表达模型，不但可以解决一词多义的问题，而且可以挖掘出词汇的复杂特征，如词性特征、语法特征等。模型的核心思想是将句子来作为上下文，来训练词向量，学习当前上下文环境下的词汇表征。\n大模型时代的词向量，以Bert为开端 额外补充：目前中文词嵌入效果最好的模型之一：bge:\n﻿https://arxiv.org/pdf/2309.07597﻿\n﻿https://www.modelscope.cn/models/AI-ModelScope/bge-large-zh#usage﻿\n﻿\n需要掌握的神经网络结构：RNN、LSTM、Seq2Seq 大模型之前的NLP模型、算法都在尝试解决的问题：更好的表示输入，更好地利用知识（记忆）\nRNN与Bi-RNN: NLP的某些任务如机器翻译、语音识别等都属于序列生成任务，每一时刻都输出都依赖与上一时刻以及之前的输出。RNN（RecurrentNeuralNetworks）是一种处理序列任务的神经网络[6]。\n循环神经网络由数据获取层、数据流出层、隐藏层构成，与前馈神经网络相似。不同的是，RNN的隐藏层之间相互连接，使得RNN有了记忆之前输入内容的能力。隐藏层包含着之前每个时刻的信息，因此使得输出能够获得充分的信息。RNN的基本结构示意如图2.1：\n在实际的应用中，只考虑输入的从左到右，并不能完全利用输入的信息。例如在文本摘要的任务重，某一时刻的输入不但与之前的输入信息有关，也与未输入的词汇有关。双向的神经网络从两个方向来考虑输入，能够对信息进行更充分的抽取和编码。其结构也比较简单，在原有的从左到右的隐藏层之上加上了一层从右到左的隐藏层，并将两者链接。双向神经网络可以获取两个方向的文本表示，增强了隐藏层的信息表达。图2.2是双向神经网络的示意图。ho层是从左到右的网络层，g0层是从由到左的网络层。这两层的信息拼接，作为下一层隐藏层的输入。\n﻿\n[流程图]### LSTM\nRNN模型存在两个问题：长期依赖问题和梯度消失问题[45]。长期依赖问题是指RNN网络无法记忆较长的输入内容，只能记住最近的内容，这是链式求导数导致的问题[7]。长短期记忆模型与标准的循环神经网络的差别是体现在隐藏层。LSTM通过隐藏单元的门控机制来筛选信息，从而减弱了长期依赖和梯度小时问题。LSTM的门控主要包括输入门，遗忘门，输出门三部分，具体的计算如下\n序列到序列（Seq2Seq） DNN(深度神经网络，多层神经网络的叠加)是非常强大的机器学习工具，在自然语言处理的多个任务上取得了不错成绩。但是DNN只能处理输入和输出维度为固定的任务[34]。但是如翻译，摘要等任务的输出的长度是不定的，这类问题需要输入与输出是域无关的方案。这类的任务需要解决数据对齐的问题。序列到序列（SequencetoSequence）便是这样一套方案。\n序列到序列结构的主要分为两部分：编码器和解码器。编码器和解码器一般都是用RNN。编码器通过RNN网络将数据编码为指定的维度的输出，解码器从编码器的输出中解码为需要的维度，从而解决了输入与输出的对齐问题。\n﻿\n以一个对话系统为例，主要的结构如图3.1。\n图3.1Seq2Seq结构示意图\n输入的句子是“我爱NLP”,经过编码器的序列，编码器的隐藏层链接输出层的隐藏层。\u0026lt;S\u0026gt;为输出的开始。“\u0026lt;S\u0026gt;”经过隐藏层，输出第一个预测：“Me”,输出值“Me”作为下一刻的输入，再进入隐藏层。当输出为 \u0026lt;/S\u0026gt;表示输出的结束。这时候整个输入输出的流程就结束了。\n循环神经网络因其本身的时序特性和前后依赖特性，适合用来做生成式任务。但是因为循环神经网络自身存在的长期依赖性问题，在某一时刻的预测若与该时刻较远的输入有很大相关关系时，RNN可能是不包含这些信息的，导致预测不准确。因此，在Seq2Seq的机构中，通常使用长短期记忆模型（LSTM）通过输入门、遗忘门和输出门来控制RNN要记住的内容，从而提升了对长距离内容的记忆效果。\n图3.1是最基本的Seq2Seq结构，在实际的使用中通常会在编码器和解码器之间有一个存储区域（context），用来存储编码器的所有隐藏层信息，而不是解码器仅仅依赖最后一层隐藏层。此时，编码器将输入映射为固定长度，解码器根据编码器的(context)来生成内容输出。户保田[75]等人的论文中证明，使用了context的Seq2Seq模型相比不使用context，生成的摘要在R-1,R-2,R-L都有较大幅度的提升。\nSeq2Seq结构有效地解决了源输入数据与输出数据的对齐问题。文本摘要的长度通常都小于文本的长度。但是仅仅依赖与循环神经网络的Seq2Seq模型的信息的编码能力仍有待加强。特别是解码器不考虑长度，将输入内容压缩为固定的长度带来信息的丢失。输入是特别长的文章时，这种信息的丢失将会更加明显。在输入的文本中，不同的句子对最终的摘要重要性是不同的，但在编码器中被压缩的文本并没有权重上的分别。\n注意力机制：注意力、自注意力、多头注意力 注意力 基本的序列到序列模型的编码器会把输入内容压缩到固定的长度，但是在输入的内容比较长的时候，就不能有效地记住足够的信息了，这是序列到序列模型的一个重要问题。Bahdanau等人[46]在2015年发表了一篇机器翻译的文章提到了一种注意力机制。注意力机制在编码器的部分，对输入的不同部分建立一个索引，在输出阶段可以随时来回看与输入相关的输入片段，这样编码器在输出时就不需要存储所有的信息了。加入注意力机制的序列到序列模型可以有效地解决这个问题。其基本的结构如图3.2。\n这里编码器使用了一个双向的LSTM模型，Attention机制主要是在解码器进行解码时产生作用。其中涉及到的公式如下：\n﻿**$a_t=attn(hs,s_i), i =t-1$**﻿\n﻿**$a_s = Softmax(a)_t$**﻿\n﻿\n解码器在t时刻时，根据编码器的隐藏层输出h,和当前的解码器的隐藏层输出s_t，来计算在不同的输入i(i表示输入的次序，h_i表示第i个输入的隐藏层输出，包括正反两个方向)的得分e_i^t。然后将得分归一化，生成Attention的权重a^t。Attention的权重与对应的编码器的隐藏层相乘，得到t时刻输出应该关注的输入内容h_t^*，进而根据输入h_t^*和解码器的隐藏层输出s_t得到当前的输出P_t (w)。\n注意力机制去掉了编码器与解码器之间的数据压缩过程，直接数据流通，可以使得Seq2Seq结构不必再映射成为一个固定的长度。同时解码器的每一时刻都直接关注与当前输出有关的编码端信息，不但加速了信息的流动速度，也增强了获取信息的质量。Nallapati[36]等人的实验也证明了注意力机制在文本摘要任务中的有效性。\n但是注意力机制的运用也使得另一个问题愈发明显：内容重复生成。这是因为注意力机制使得生成内容时，会重点关注输入中某一部分重点内容，从使得生成的句子冲出现重复的单词。\n﻿深度学习笔记：如何理解激活函数？（附常用激活函数）_什么是激活函数?举例说明几种常见的激活函数及其用途-CSDN博客﻿\n自注意力 注意机制关注的是t时刻的输出与输入序列各部分的关系，而自注意力机制关注的是输入序列中各个部分的相互关系。根据这些相互关系来重新表示每个词汇。如句子：“我爱自然语言处理，学习它我能得到很高的成就感”。句子中的“它”指的是“自然语言处理”，两者具有很强的关系；“我”和“自然语言处理”分别是“学习”的执行者和执行对象，也都有一定的关系。自注意力机制便是要挖掘词汇中的这些关系。具体的计算如下：\n﻿https://0809zheng.github.io/2020/04/24/self-attention.html﻿\n自注意力和注意力机制的区别： 注意力机制的权重参数是一个全局可学习参数，对于模型来说是固定的；而自注意力机制的权重参数是由输入决定的，即使是同一个模型，对于不同的输入也会有不同的权重参数。 注意力机制的输出序列长度与输入序列长度可以是不同的；而自注意力机制的的输出序列长度与输入序列长度必须是相同的。 注意力机制在一个模型中通常只使用一次，作为编码器和解码器之间的连接部分；而自注意力机制在同一个模型中可以使用很多次，作为网络结构的一部分。 注意力机制擅长捕捉两个序列之间的关系，如机器翻译任务中将一个序列映射为另一个序列；而自注意力机制擅长捕捉单个序列内部的关系，如作为预训练语言模型的基本结构。 编码器与解码器 ﻿Transformer(一)\u0026ndash;论文翻译：Attention Is All You Need 中文版-CSDN博客﻿\n位置编码 自注意力机使得模型可以有效地并行计算，但是却没有RNN的输入序列信息。为了解决这一问题，在输入词汇性自注意力计算之前加上一层位置编码，再进行运算，使得位置信息也能参与到自注意力的表示中。Transformers中使用使用正弦函数和余弦函数来构造位置编码:\n编码器 编码器由输入层和若干个叠加的编码层构成。输入层包括合并词向量和位置编码。每个编码层中又顺序包括自注意力层、正则化层、全连接的前馈神经网络层、正则化层。输入层的词向量首先进入自注意力层获取由相关关系表示的输入，进入正则化层进行处理，再进入前馈网络层，再进行正则化，最后输出，进入下一个编码层，直到最后输出。具体流程如图2.4所示\n残差链接层\u0026amp;归一化层（add \u0026amp;normalize） ﻿\n参考链接：\n﻿https://developer.volcengine.com/articles/7389519960881496075﻿\n基本上所有的归一化技术，都可以概括为如下的公式：\n对于隐层中某个节点的输出，即激活值a，进行非线性变换（如ReLU、tanh等）后得到h。\n****层归一化的过程就是先计算这一层所有激活值的均值μ和方差σ²，然后使用这些统计量对h进行分布调整。****这种调整就是把“高瘦”和“矮胖”的都调整回正常体型（深粉色），把偏离x=0的拉回中间来（淡紫色）。\n解码器： 类似于编码器，解码器也包含若干个叠加的解码层，最后通过一个线性连接层和输出层输出预测。在每个编码层中，首先是解码器的输入转化为输入向量（词向量+位置编码），通过注意力层和正则化层。这时候，需要将编码器的输出映射成为K,和V,来计算输入层在此刻输出的权重，并对输入进行加权计算。然后再进入一个正则化层，前向网络层，再进入一个正则化层，输出到下一个解码层。在所有的解码器层顶端通过线性连接层和softmax层输出结果。编码器到解码器的具体的流程如图2.5。\n综合自注意力机制、位置编码、编码器和解码器，Transformer的整体的模型结构如\n﻿\n﻿\nTransformer模型的计算量评估 ﻿万字长文解析：大模型需要怎样的硬件算力﻿\nTransformer模型可视化 ﻿https://bbycroft.net/llm﻿\n﻿\n﻿\n理解Transformer_从0到1.pdf\n","date":"0001-01-01T00:00:00Z","permalink":"https://blog.liu-nian.top/ai/%E7%90%86%E8%A7%A3transformer%E4%BB%8E0%E5%88%B01/","title":""},{"content":"","date":"0001-01-01T00:00:00Z","permalink":"https://blog.liu-nian.top/ops/github-mcp/","title":""}]
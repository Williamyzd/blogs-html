[{"content":"1. 准备基础镜像 一般情况，可以参考：[GitHub Action] 自动化创建，vllm构建过程所需的磁盘和内存资源较大，github action提供的免费资源不足（4c16g,14 ssd）会导致镜像构建失败。需要使用自己的服务器\n参考https://github.com/vllm-project/vllm/blob/main/Dockerfile\n可知，所需的基础镜像如下： 如目标cuda版本为12.1.1,则需要的基础镜像为：\nnvidia/cuda:12.1.1-devel-ubuntu20.04\nnvidia/cuda:12.1.1-devel-ubuntu22.04\n查询该镜像是否存在 https://explore.ggcr.dev/\n以上镜像组合式存在的。如果不存在，可在该网站查询相近的镜像组合\n如镜像列表：https://explore.ggcr.dev/?repo=nvidia%2Fcuda\n如找不到目标基础镜像或相邻镜像也不满足要求，可以考虑构建自己的基础镜像，见nvidia/cuda代码库：https://gitlab.com/nvidia/container-images/cuda 。 这里暂不展开\n2. 同步基础镜像到国内镜像仓库（可选，如果具备外网服务器或可访问官方的镜像仓库，可不做） 使用github工作流自动同步：\n所有工作流yaml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # 工作流名称 name: Sync-Images-to-Aliyun-CR # 工作流运行时显示名称 run-name: ${{ github.actor }} is Sync Images to Aliyun CR. # 怎样触发工作流 on: push: branches: [ \u0026#34;main\u0026#34; ] # Allows you to run this workflow manually from the Actions tab workflow_dispatch: # 工作流程任务（通常含有一个或多个步骤） jobs: syncimages: runs-on: ubuntu-latest steps: - name: Checkout Repos uses: actions/checkout@v4 - name: Login to Aliyun CR uses: docker/login-action@v3 with: registry: ${{ vars.ALY_REGISTRY}} username: ${{ secrets.ALY_UNAME }} password: ${{ secrets.ALY_PASSWD }} logout: false - uses: actions/setup-python@v5 with: python-version: \u0026#39;3.10\u0026#39; - run: python scrips/pull_image.py 对应处理镜像代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # -*- coding: utf-8 -*- import os,re,subprocess,datetime,json lines = [] rs = [] ds_reg = \u0026#39;registry.cn-hangzhou.aliyuncs.com/reg_pub/\u0026#39; with open(\u0026#39;images.txt\u0026#39;, \u0026#39;r\u0026#39;) as f: lines = f.read().split(\u0026#39;\\n\u0026#39;) for img in lines: # ims = re.split(\u0026#39;\\s+\u0026#39;, img) # p =None # if len(ims) \u0026gt; 1 and len(ims[1]) \u0026gt; 0: # p = ims[1] if len(img) \u0026gt; 0: ims = img.replace(\u0026#39;/\u0026#39;, \u0026#39;_\u0026#39;) ds = ds_reg + ims cmds = \u0026#39;skopeo copy --all docker://\u0026#39; + img + \u0026#39; docker://\u0026#39; + ds code,rss = subprocess.getstatusoutput(cmds) print(\u0026#39;*\u0026#39;*20+\u0026#39;\\n\u0026#39;+\u0026#34;copy {} to {}\u0026#34;.format(img, ds) + \u0026#39;\\n\u0026#39; + \u0026#34;rs:\\n code:{}\\n rs:{}\u0026#34;.format(code, rss) ) if code != 0: cmds = \u0026#39;docker pull \u0026#39; + img + \u0026#39; \u0026amp;\u0026amp; docker tag \u0026#39; + img + \u0026#39; \u0026#39; + ds + \u0026#39; \u0026amp;\u0026amp; docker push \u0026#39; + ds + \u0026#39; \u0026amp;\u0026amp; docker rmi {} {} \u0026#39;.format(img,ds) code,rss = subprocess.getstatusoutput(cmds) print(\u0026#39;skopeo copy error, try docker pull and push:\u0026#39; + \u0026#34;\\n\u0026#34; + cmds + \u0026#39;\\n\u0026#39; + \u0026#34;rs:\\n code:{}\\n rs:{}\u0026#34;.format(code, rss)) r = { \u0026#39;src\u0026#39;: img, \u0026#39;ds\u0026#39;: ds, \u0026#39;code\u0026#39;: code, \u0026#39;rs\u0026#39;: rss } rs.append(r) ctime =datetime.datetime.now().strftime(\u0026#39;%Y-%m-%d-%H:%M:%S\u0026#39;) with open(\u0026#39;result-{}.json\u0026#39;.format(ctime), encoding=\u0026#39;utf-8\u0026#39;, mode=\u0026#39;w\u0026#39;) as f: json.dump(rs, f) print(r) 镜像同步代码仓库结构： 3. 准备Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 注意确认镜像中的操作系统版本+cuda版本是否存在，查看：https://explore.ggcr.dev/?repo=nvidia%2Fcuda FROM nvidia/cuda:12.1.1-devel-ubuntu22.04 AS vllm-base # cuda版本：vllm目前只支持cuda 12.4、12.1、11.8 其他版本需要自己编译 ARG CUDA_VERSION=121 # vllm版本 ARG VLLM_VERSION=0.8.1 # 防止需要交互卡主打包过程 ENV DEBIAN_FRONTEND=noninteractive # 更新环境 RUN apt update \u0026amp;\u0026amp; \\ apt install -y build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libsqlite3-dev libreadline-dev libffi-dev libbz2-dev liblzma-dev tk-dev wget # 安装python，如需变更python版本这里调整 RUN wget https://www.python.org/ftp/python/3.12.9/Python-3.12.9.tgz \u0026amp;\u0026amp; \\ tar -zxvf Python-3.12.9.tgz \u0026amp;\u0026amp; \\ cd Python-3.12.9 \u0026amp;\u0026amp; \\ ./configure --enable-optimizations --prefix=/usr/local/python3.12 \u0026amp;\u0026amp; make -j$(nproc) \u0026amp;\u0026amp; \\ make altinstall \u0026amp;\u0026amp; \\ ln -s /usr/local/python3.12/bin/python3.12 /usr/bin/python3 \u0026amp;\u0026amp; \\ ln -s /usr/local/python3.12/bin/pip3.12 /usr/bin/pip3 \u0026amp;\u0026amp; \\ ln -s /usr/local/python3.12/bin/pip3.12 /usr/bin/pip \u0026amp;\u0026amp; \\ rm -rf Python-3.12.9.tgz Python-3.12.9 # 设置PATH ENV PATH=\u0026#34;/usr/bin:/usr/local/python3.12/bin:$PATH\u0026#34; # 安装vllm,注意所需版本，如无合适的whl需要考虑自己编译源码 RUN wget https://github.com/vllm-project/vllm/releases/download/v${VLLM_VERSION}/vllm-${VLLM_VERSION}+cu${CUDA_VERSION}-cp38-abi3-manylinux1_x86_64.whl RUN pip install vllm-${VLLM_VERSION}+cu${CUDA_VERSION}-cp38-abi3-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu${CUDA_VERSION} RUN rm -f vllm-${VLLM_VERSION}+cu${CUDA_VERSION}-cp38-abi3-manylinux1_x86_64.whl ENTRYPOINT [\u0026#34;python3\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;vllm.entrypoints.openai.api_server\u0026#34;] ﻿\n4. 打镜像并推送 核心命令如下：\n1 2 3 docker build --tag vllm/vllm-openai . # 如服务器配置较高，可以使用该参数--build-arg max_jobs=8 --build-arg nvcc_threads=2 或针对性调整，如资一般，不建议使用 docker tag vllm/vllm-openai registry.cn-hangzhou.aliyuncs.com/reg_pub/vllm_vllm-openai:cuda-12.1.1-20250320 docker push registry.cn-hangzhou.aliyuncs.com/reg_pub/vllm_vllm-openai:cuda-12.1.1-20250320 ﻿\n﻿\n﻿\n","date":"2025-03-11T08:38:25Z","permalink":"https://blog.liu-nian.top/ai/%E6%9E%84%E5%BB%BA%E6%8C%87%E5%AE%9Acuda%E7%89%88%E6%9C%AC%E7%9A%84vllm%E9%95%9C%E5%83%8F/","title":"构建指定cuda版本的vllm镜像"},{"content":"计算量评估 前馈神经网络计算量 前馈神经网络：Y = XW +B\n在这个过程中，完成的运算如下：\n$$ \\begin{bmatrix} x_0 \u0026amp; x_1 \u0026amp; x_2 \\end{bmatrix} \\times \\begin{bmatrix} w_{00} \u0026amp; w_{01} \\ w_{10} \u0026amp; w_{11} \\ w_{20} \u0026amp; w_{21} \\end{bmatrix} + \\begin{bmatrix} b_0 \\ b_1 \\end{bmatrix} $$\n其中，W是权重矩阵，X是输入向量，B是偏置项。即：\n$$ y = \\begin{bmatrix} y_0 \\ y_1 \\end{bmatrix} \\begin{matrix} y_0 = w_{00}x_0 + w_{01}x_1 + b_0\\ y_1 = w_{10}x_0 + w_{11}x_1 + b_1\\\n\\end{matrix} $$\n上述2个公式中，每个公式都有3个加法、3个乘法计算。所以[1,3]的矩阵和[3,2]的矩阵相乘共包含 2132 =12 次浮点计算量(称为FLOPs)。从而推广到一般：[m,k]的矩阵和[k,n]的矩阵相乘，需要2mkn FLOPs。而GPU在不同精度单位下，浮点数运算的效率是不同的。例如，A100在16位精度(FP16或BF16)下每秒可以进行$31410^{12}$次浮点运算。而在32位精度下，每秒只能进行$15610^{12}$次浮点运算。所以，我们可以理所当然的想到一个提速思路 - 让所有运算在低精度下进行来提高训练/推理效率。但是，这里有个无法忽略的问题：某些计算必须要在高精度下进行。例如全程在低精度下进行模型训练，往往会出现由于精度不够导致数值溢出和无法收敛的问题。所以目前大模型训练通常采用的是混合精度方式进行：前向传播、反向传播这些无需高精度的计算采用16位精度(FP16或BF16)进行，优化器状态更新在32位精度(FP32)进行。而模型推理的要求就比较低，可以在16位、8位甚至4位精度进行，整个任务的计算效率非常高。\n模型计算量分析 $$ 训练计算量 = F_{前向传播} + F_{反向传播}\\ 推理计算量 = F_{前向传播} \\ F_{反向传播} = 2*F_{前向传播} $$\n接下来进行$F_{前向传播}$的计算推导：\n先确定符号：我们令B对应batch size，s对应sequence length，h对应hidden dimension，l对应layers number；接着，我们再来观察大模型结构：大模型本身由l个transformer块串联而成。此外，首transformer块前和尾transformer块后还有一些结构。所以大模型的算力估算可以分为两大块：一、l个transformer块中的FLOPs；二、其他结构中的FLOPs：\ntransformer块中，每个transformer块都由多头注意力结构和MLP两种结构构成： 多头注意力机制所需计算量： QKV计算量：\n$$ Q = XW^Q, K=XW^K, V=XW^V, 其中： x \\in [B,s,h], W^Q\\in[h,h/a] $$\n涉及计算量大致如下：\n$$ 32Bshh/aa=6Bsh^2,其中a是多头注意力机制的head数。 $$\n注意力计算量：\u0026lt;br\u0026gt;\n注意力分数计算 单头注意力分数一般采用如下计算： $$ attention = QK^T / \\sqrt(d_k) $$\n计算量为：\n$$ 2Bsh/a*s=2Bs^2h/a,其中Q \\in [B,s,h/a], K^T \\in[B,h/a,s] $$\n因此多头注意力分数整体计算量：\n$$ 2Bs^2h/a*a=2Bsh^2 $$\n注意力输出层计算量： 单头注意力输出层计算量： 计算公式为： $$ Z= attention \\times V, 其中attention \\in [B,s,s], V\\in[B,s,h/a] $$\n计算量为：\n$$ 2Bss*h/a=2Bs^2h/a $$\n因此多头注意力输出层计算量为：$2Bs^2h/a*a=2Bs^2h$\n多头注意力合并层计算量\u0026lt;br\u0026gt; 计算公式为：\n$$ y = W^OZ + B \\ 其中,Z\\in[B,s,h], W^O \\in [h, h], Z为多头注意力输出层结果拼接 $$\n计算量为：\n$$ 2Bsh*h=2Bsh^2 $$\n因此整个自注意力层的计算量为：\n$$ 4Bs^2h+8Bsh^2 $$\nMLP层所需计算量 激活值和第一个线性层相乘：$[B,s,h]*[h,4h] 共8Bsh^2$ FLOPs； 激活值和第二个线性层相乘：$[B,s,4h]*[4h,h] 共8Bsh^2$ FLOPs； 激活层为约0FLOPs：GeLU是确定的数学缩放公式，可认为无需矩阵乘； 单个MLP共有包含$16Bsh^2$$ FLOPs。\u0026lt;br\u0026gt; 因此，l个transformer块的FLOPs为：$24Bsh^2+4Bs^2h$，所有transformer块的FLOPs为：$l(24Bsh^2+4Bs^2h)$。\n其他结构所需计算量 主要是最终输出层，计算公式为：\n$$ y = XW^O + B, 其中X\\in[B,s,h], W^O \\in [h,V], V为字典大小 $$\n其计算量大小为：\n$$ 2Bsh*V=2BsVh $$\n综上，整个模型的前向传播计算量为：\n$$ F_{前向传播} = l(24Bsh^2+4Bs^2h)+2BsVh \\ = 24Bsh^2l+4Bs^2hl+2BsVh \\ = 24Bsh^2l(1+\\frac{s}{6h}+\\frac{V}{12lh}) $$\n而通常在大模型中，6h远大于s，12lh远大于V，所以还可以进一步简化：\n$$ F_{前向传播} = 24Bsh^2l $$\n因此可推断，训练与推理所需的计算量为：\n$$ F_{训练} = 3F_{前向传播} = 72Bsh^2l \\ F_{训练} = 4F_{前向传播} = 96Bsh^2l,全量参数计算时 \\ F_{推理} = F_{前向传播} = 24Bsh^2l $$\n粗略估算一般基于transformer的大模型的参数量为$12lh^2$,可以发现： 1）模型训练中：如果没有应用激活值重计算，单模型副本处理每个tokens时，单参数上算力需求约为6FLOPs($\\frac{72Bsh^2l}{12lh^2Bs}$)。如果应用全激活值重计算，提升至8FLOPs； 2）模型推理中：单模型副本处理每个tokens时，单参数上算力需求约为6FLOPs；\n最常用来测量每秒浮点运算次数的基准程序（benchmark）之一，就是Linpack。 一个MFLOPS（megaFLOPS）等于每秒一百万（=10^6）次的浮点运算， 一个GFLOPS（gigaFLOPS）等于每秒十亿（=10^9）次的浮点运算， 一个TFLOPS（teraFLOPS）等于每秒一万亿（=10^12）次的浮点运算， 一个PFLOPS（petaFLOPS）等于每秒一千万亿（=10^15）次的浮点运算， 一个EFLOPS（exaFLOPS）等于每秒一百亿亿（=10^18）次的浮点运算。\n模型训练时间估算 根据阿姆达尔定律，由于整个系统中有网络通信等无法并行加速项的存在，集群中存在加速比上限。通常，训练集群中的GPU利用率通常约30%～55%之间。集群训练耗时计算公式如下： https://blog.csdn.net/weixin_43925843/article/details/145626893 https://www.nvidia.cn/data-center/v100/ https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md#%E8%BD%AF%E7%A1%AC%E4%BB%B6%E4%BE%9D%E8%B5%96\nLLaMA Factory 微调所需的硬件资源计算主要基于模型参数量、微调方法和训练配置三个核心维度。以下是具体计算方法与典型场景示例：\n一、显存需求计算公式 显存占用（GB）主要包含以下部分：\n$$ \\text{显存} = \\text{模型参数显存} + \\text{梯度显存} + \\text{优化器状态显存} + \\text{激活值显存} $$\n1. 模型参数显存\n全参数微调（FP16）：参数量 × 2 bytes（例如 7B 模型：7×10⁹ × 2 / 10⁹ = 14GB） LoRA（FP16）：仅需存储低秩矩阵（约 0.1%-1% 参数量）（例如 7B 模型：14GB × 1% ≈ 0.14GB） QLoRA（4-bit）：参数量 × 0.5 bytes （例如 7B 模型：7×10⁹ × 0.5 / 10⁹ = 3.5GB） 2. 梯度与优化器状态\n全参数微调（Adam优化器）：参数量 × 4 bytes（梯度+优化器状态） LoRA/QLoRA：仅需存储少量梯度（可忽略） 3. 激活值显存 与批次大小（batch size）和序列长度（seq_len）相关，计算公式：\n$$ \\text{激活值显存} \\approx \\text{batch_size} \\times \\text{seq_len} \\times \\text{hidden_dim} \\times 2 \\text{ bytes} $$\n二、典型场景硬件需求 模型规模 微调方法 显存需求（估算） 推荐 GPU 配置 数据来源 7B 全参数（FP16） 60GB 2×A100 80GB 7B QLoRA（4-bit） 6GB 单卡 RTX 3090/4090 13B LoRA（FP16） 12GB 单卡 A100 40GB 70B 全参数（FP16） 600GB 8×H100 80GB（NVLink） 三、关键影响因素 微调算法选择\n全参数微调：显存需求最高，适合高算力集群。 LoRA/QLoRA：显存需求降低 50%-90%，适合单卡训练。 GaLore/Freeze：通过梯度压缩或参数冻结进一步优化显存。 训练配置优化\n批次大小（batch_size）：增大 batch_size 会线性增加激活值显存。 梯度累积（gradient_accumulation）：通过累积梯度减少单步显存需求。 量化与混合精度：4-bit/8-bit 量化可降低显存 30%-70%。 分布式训练支持 LLaMA Factory 支持 DeepSpeed ZeRO 和模型并行，可将大模型拆分到多卡。\n四、实战示例（7B模型 + QLoRA） 硬件配置：RTX 4090（24GB 显存） 训练命令： 1 2 3 4 5 6 llamafactory-cli train \\ --model_name_or_path meta-llama/Llama-3-8b \\ --finetuning_type qlora \\ --quantization_bit 4 \\ --per_device_train_batch_size 4 \\ --gradient_accumulation_steps 8 显存占用：约 10-12GB（含激活值）。 五、资源估算工具 LLaMA Factory 提供显存估算工具，可通过以下命令生成报告：\n1 llamafactory-cli estimate --model_name_or_path meta-llama/Llama-3-8b --finetuning_type lora 如需更精确的计算，可参考 Hugging Face 的 Transformer 显存计算器。 https://mp.weixin.qq.com/s/CJxA-PxF_lvSpMr_7uHVVg\n推理所用显存： Total GPU Memory = 模型大小 + KV Cache + Memory Overhead 最后还是以 LLaMa-2 13B 来举例。假设有 10 个并发请求，同时请求 LLaMa-2 13B 以最大 Token数(4096) 进行模型推理。 那最终需要的 GPU Memory 计算过程如下： 模型大小= 13 Billion * 2 Bytes = 26 GB\nTotal KV cache= 800 KB * 4096 Tokens * 10 并发请求 = 32 GB\nMemory Overhead= 0.1 * (26 GB + 32 GB) = 5.8 GB\n所以最终需要总 GPU memory为: 26 GB + 32 GB + 5.8 GB = 63.8 GB。需要 2 块英伟达的 A100 芯片才可以。 https://github.com/manuelescobar-dev/LLM-Tools\nhttps://medium.com/@manuelescobar-dev/memory-requirements-for-llm-training-and-inference-97e4ab08091b\n显存计算 推理显存评估：\n$$ Total\\ Inference\\ Memory = Model Size + KV Cache + Activations $$\n模型大小：\nkv cache：\n$$ KV\\ Cache =2 × Batch Size × Sequence Length × Number of Layers × Hidden Size × Precision $$\n激活值：\n$$ Activation Memory = Batch Size × Sequence Length × Hidden Size × ( 34 + 5 × \\frac{Sequence Length ×Number of attention heads}{Hidden Size}) $$\n推理显存评估：\n$$ Total Memory=Model Size+KV Cache+Activations+(Optimizer States+Gradients)× Number of Trainable Parameters $$\nOptimizer States Optimization algorithms require resources to store the parameters and auxiliary variables. These variables include momentum and variance used by algorithms such as Adam (2 states) or SGD (1 state). The precision and type of optimizer affect memory usage.\nFormula [1]\nAdamW (2 states): 8 Bytes per parameter AdamW (bitsandbytes Quantized): 2 Bytes per parameter SGD (1 state): 4 Bytes per parameter Gradients Gradient values are computed during the backward pass of the model. They represent the rate of change of the loss function with respect to each model parameter and are crucial for updating the parameters during optimization. As with activations, they must be stored in FP32 for numerical stability.\nFormula [1]\n4 Bytes per parameter\n","date":"2025-02-19T08:38:25Z","permalink":"https://blog.liu-nian.top/ai/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86%E8%B5%84%E6%BA%90%E4%B8%8E%E8%AE%A1%E7%AE%97%E9%87%8F%E8%AF%84%E4%BC%B0/","title":"大模型训练推理资源与计算量评估"},{"content":"目标 docker-compose、k8s-yaml\n准备镜像 这里以llama.cpp的基础镜像作为参考。选择llama.cpp的原因是他支持非常多种类的加速卡，同时也支持cpu运行大模型。 使用官方已有镜像 如果机器环境的CUDA版本与官方镜像的兼容，可以直接使用官方镜像。 参考链接：https://github.com/ggerganov/llama.cpp/blob/master/docs/docker.md 如果有GPU，建议使用：ghcr.io/ggerganov/llama.cpp:server-cuda 如果没有gpu，建议使用：ghcr.io/ggerganov/llama.cpp:server 如果想要拉取默认的基础镜像为最新版本，check下最新的dockerfile文件，确认下是否与自己环境的cuda版本兼容。链接如下：https://github.com/ggerganov/llama.cpp/tree/master/.devops 如果最新的镜像不能满足需求，可以尝试搜索下历史镜像 可访问页面进行搜索:https://github.com/ggerganov/llama.cpp/pkgs/container/llama.cpp\n以最新的镜像为例，具体命令如下：\n1 2 docker pull ghcr.io/ggerganov/llama.cpp:server-cuda --platform linux/amd64 #如果镜像拉操作系统与最终操作系统不一致，需要加上--platform参数。如果一致，无需添加。 docker save -o llama.cpp.tar ghcr.io/ggerganov/llama.cpp:server-cuda #如果需要离线导入目标机器，可将镜像保存为tar包 自定义镜像 历史镜像可能不满足需求，可以尝试自定义镜像。这里以cuda 11.8.0为例。 查看nvidia/cuda镜像的tag，选择合适的版本。参考地址：https://hub.docker.com/r/nvidia/cuda/tags\n准备llama.cpp镜像 1 2 3 4 5 6 7 8 git clone https://github.com/ggerganov/llama.cpp cd llama.cpp vim .devops/cuda.Dockerfile # 修改对应的cuda版本，注意操作系统的版本也要和刚查到的镜像保持一致 docker build -t llama.cpp:server-cuda-11.8 .devops/cuda.Dockerfile # 保存为tar包，方便离线导入目标机器 docker save -o llama.cpp-server-cuda-11.8.tar llama.cpp:server-cuda-11.8 # 也可选择push到自己的镜像仓库 docker push ${ your hub }/llama.cpp:server-cuda-11.8 Dockerfile示例:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 ARG UBUNTU_VERSION=20.04 # 修改为目标版本 # 如果目标版本为20.04，会出现时区问题和cmake版本过低问题，需要单独调整 # This needs to generally match the container host\u0026#39;s environment. ARG CUDA_VERSION=11.5.2 # 修改为目标版本 # Target the CUDA build image ARG BASE_CUDA_DEV_CONTAINER=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu${UBUNTU_VERSION} # 确保dockerhub中有该镜像 ARG BASE_CUDA_RUN_CONTAINER=nvidia/cuda:${CUDA_VERSION}-runtime-ubuntu${UBUNTU_VERSION} # 确保dockerhub中有该镜像 ARG DEBIAN_FRONTEND=noninteractive FROM ${BASE_CUDA_DEV_CONTAINER} AS build # CUDA architecture to build for (defaults to all supported archs) ARG CUDA_DOCKER_ARCH=default # 版本为20.04时，需要单独调整时区问题，需要调整部分 ########################################### #ENV TZ=Etc/UTC # RUN apt-get update \u0026amp;\u0026amp; \\ # apt-get install -y tzdata wget tar build-essential python3 python3-pip git libcurl4-openssl-dev libgomp1 \u0026amp;\u0026amp; \\ # wget https://github.com/Kitware/CMake/releases/download/v3.31.5/cmake-3.31.5-linux-x86_64.tar.gz \u0026amp;\u0026amp; \\ # tar -zxvf cmake-3.31.5-linux-x86_64.tar.gz \u0026amp;\u0026amp; mv cmake-3.31.5-linux-x86_64 /opt/cmake \u0026amp;\u0026amp; \\ # ln -s /opt/cmake/bin/cmake /usr/bin/cmake \u0026amp;\u0026amp; cmake --version ########################################### RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y build-essential cmake python3 python3-pip git libcurl4-openssl-dev libgomp1 WORKDIR /app COPY . . RUN if [ \u0026#34;${CUDA_DOCKER_ARCH}\u0026#34; != \u0026#34;default\u0026#34; ]; then \\ export CMAKE_ARGS=\u0026#34;-DCMAKE_CUDA_ARCHITECTURES=${CUDA_DOCKER_ARCH}\u0026#34;; \\ fi \u0026amp;\u0026amp; \\ cmake -B build -DGGML_NATIVE=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON ${CMAKE_ARGS} -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined . \u0026amp;\u0026amp; \\ cmake --build build --config Release -j$(nproc) RUN mkdir -p /app/lib \u0026amp;\u0026amp; \\ find build -name \u0026#34;*.so\u0026#34; -exec cp {} /app/lib \\; RUN mkdir -p /app/full \\ \u0026amp;\u0026amp; cp build/bin/* /app/full \\ \u0026amp;\u0026amp; cp *.py /app/full \\ \u0026amp;\u0026amp; cp -r gguf-py /app/full \\ \u0026amp;\u0026amp; cp -r requirements /app/full \\ \u0026amp;\u0026amp; cp requirements.txt /app/full \\ \u0026amp;\u0026amp; cp .devops/tools.sh /app/full/tools.sh ## Base image FROM ${BASE_CUDA_RUN_CONTAINER} AS base RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install -y libgomp1 curl\\ \u0026amp;\u0026amp; apt autoremove -y \\ \u0026amp;\u0026amp; apt clean -y \\ \u0026amp;\u0026amp; rm -rf /tmp/* /var/tmp/* \\ \u0026amp;\u0026amp; find /var/cache/apt/archives /var/lib/apt/lists -not -name lock -type f -delete \\ \u0026amp;\u0026amp; find /var/cache -type f -delete COPY --from=build /app/lib/ /app # 此处删除了light full 等不必要的镜像信息，仅留了server信息 ### Server, Server only FROM base AS server ENV LLAMA_ARG_HOST=0.0.0.0 COPY --from=build /app/full/llama-server /app WORKDIR /app HEALTHCHECK CMD [ \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost:8080/health\u0026#34; ] ENTRYPOINT [ \u0026#34;/app/llama-server\u0026#34; ] 也可选择将llama.cpp仓库fork,修改为目标版本的Dockerfile，然后使用github的工作流去构建镜像。可参考镜像仓库：https://github.com/Williamyzd/appbuilder.git 可直接使用的tag:https://github.com/Williamyzd/appbuilder/pkgs/container/appbuilder%2Fllama.cpp\n准备模型 docker-compose撰写 k8s-yaml撰写 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 apiVersion: apps/v1 kind: Deployment metadata: name: qwen2-5-14b-gpu namespace: llms labels: app: qwen2-5-14b-gpu spec: replicas: 1 # 副本数量 selector: matchLabels: app: qwen2-5-14b-gpu template: metadata: labels: app: qwen2-5-14b-gpu spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/cluster-role operator: In values: - slave - master-compute # - key: kubernetes.io/gpu-type-name # operator: In # values: # - Tesla-P4 containers: - name: qwen2-5-14b-gpu-gpu image: 10.233.0.100:5000/llama_cpp_gpu_12_2:cuda12.2_img-csbhtgdrp9ts131y # 使用的镜像 imagePullPolicy: IfNotPresent # 镜像拉取策略 #command: [\u0026#34;nvidia-smi\u0026#34;,\u0026#34;\u0026amp;\u0026amp;\u0026#34;,\u0026#34;tail\u0026#34;, \u0026#34;-f\u0026#34;,\u0026#34;/dev/null\u0026#34;] env: - name: LLAMA_ARG_MODEL value: \u0026#34;/models/qwen2.5-14b-instruct-q5_k_m.gguf\u0026#34; - name: LLAMA_ARG_PORT value: \u0026#34;8000\u0026#34; - name: LLAMA_ARG_UBATCH value: \u0026#34;4096\u0026#34; - name: LLAMA_ARG_HOST value: \u0026#34;0.0.0.0\u0026#34; - name: LLAMA_ARG_N_PARALLEL value: \u0026#34;2\u0026#34; - name: LLAMA_ARG_N_PREDICT value: \u0026#34;4096\u0026#34; - name: LLAMA_ARG_CTX_SIZE value: \u0026#34;12000\u0026#34; - name: LLAMA_ARG_N_GPU_LAYERS value: \u0026#34;10\u0026#34; # # vgpu 资源限制 # - name: USE_GPU # value: \u0026#34;1\u0026#34; # - name: GPU_NUM # value: \u0026#34;100\u0026#34; resources: requests: memory: \u0026#34;1Gi\u0026#34; # 请求的内存大小 cpu: \u0026#34;1\u0026#34; # 请求的 CPU 大小（以毫核为单位） limits: memory: \u0026#34;64Gi\u0026#34; # 限制的内存大小 cpu: \u0026#34;8\u0026#34; securityContext: privileged: false runAsGroup: 0 runAsUser: 0 ports: - containerPort: 8000 # 容器暴露的端口 volumeMounts: - mountPath: /dev/shm name: cache-volume - mountPath: /models # subPath: v-wa9c73y3ge0wrvmm/org/william/qwen name: llm-models volumes: - name: cache-volume emptyDir: medium: Memory sizeLimit: \u0026#34;4Gi\u0026#34; # - name: gluster-llms # persistentVolumeClaim: # claimName: gluster-llms ### 此处根据实际填写 - hostPath: path: /models type: Directory name: llm-models --- apiVersion: v1 kind: Service metadata: name: qwen2-5-14b-gpu namespace: llms spec: selector: app: qwen2-5-14b-gpu # 确保这与 Deployment 中 Pod 的标签相匹配 ports: - protocol: TCP port: 8000 # Service 监听的端口 targetPort: 8000 # Pod 内应用程序监听的端口 nodePort: 8512 # Node 上开放的端口 type: NodePort # Service 类型设置为 NodePort 此处使用hostpath挂载模型，可根据实际环境选择更适合的存储方式。 注意对hostpath挂载的位置做调整 ","date":"2025-02-05T17:29:32+08:00","permalink":"https://blog.liu-nian.top/ai/%E5%A6%82%E4%BD%95%E9%83%A8%E7%BD%B2%E7%94%9F%E4%BA%A7%E7%BA%A7%E5%88%AB%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%8A%A1%E7%90%86/","title":"如何部署生产级别的大模型推理服务"},{"content":"llama.cpp推理-树莓派4b\n参考链接：https://github.com/ggerganov/llama.cpp\nhttps://qwen.readthedocs.io/zh-cn/latest/run_locally/llama.cpp.html\n裸机方式运行，非特殊框架，可以直接拉取官方镜像：\n硬件信息：\n平台架构：aarch64\n操作系统: Ubuntu 22.04.3 LTS\nCPU: 2\nMEM:4G\n创建环境 主要包括拉取代码、下载模型或生成模型等步骤\n1 2 3 4 5 6 7 8 9 10 11 # 1. 拉取代码 git clone https://github.com/ggerganov/llama.cpp llmama.cpp # 2. 编译代码 cd llama.cpp make make llama-cli # 如无需本地交互测试，不需要执行 # 3. 手动下载模型 #https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/tree/main https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF/blob/main/qwen2-1_5b-instruct-q5_k_m.gguf # 或自己生成，需要python环境 python convert-hf-to-gguf.py Qwen/Qwen2-7B-Instruct --outfile qwen2-1_5b-instruct-f16.gguf 交互式测试 参数说明：\n-m 指模型地址 -cnv 指会话模式 -p 指的输入信息，必须要传入 1 ./llama-cli -m models/qwen2-1_5b-instruct-q5_k_m.gguf -cnv -p \u0026#39;你是一个人工智能专家\u0026#39; 运行成功的截图如下：\nAPI 发布测试 1 ./llama-server -m /opt/codes/models/qwen2-1_5b-instruct-q5_k_m.gguf --port 8000 --host 0.0.0.0 前台页面访问测试\nhttp://192.168.2.189:8080/\n","date":"2024-09-25T08:38:25Z","permalink":"https://blog.liu-nian.top/ai/llama.cpp/","title":"llama.cpp推理-树莓派4b"},{"content":"硬件信息：\nBCC: 2核8G\n创建推理环境 参考链接\nhttps://docs.vllm.ai/en/stable/getting_started/cpu-installation.html\nhttps://github.com/Williamyzd/vllm/blob/main/Dockerfile.cpu\nhttps://github.com/vllm-project/vllm/blob/main/requirements-cpu.txt\nhttps://www.cnblogs.com/obullxl/p/18353447/NTopic2024081101\n准备镜像\n1 2 3 4 5 6 7 8 9 10 # 拉取代码 #如果没有git需要先安装 yum install git git clone https://github.com/vllm-project/vllm.git vllm-project cd vllm-project # 打镜像 # 以下命令在 Docker version 26.1.4 中测试 # 可编辑Dockerfile.cpu做优化 # 1. 修改基础镜像为：registry.cn-hangzhou.aliyuncs.com/reg_pub/ubuntu:22.04 # 2. 调整pip源和source为国内源 docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g . 如果执行速度慢可以考虑加上国内源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #!/bin/bash # 添加清华pip源 pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple # 备份原有的sources.list文件 sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak # 创建一个新的sources.list文件，并添加清华源 cat \u0026gt; /etc/apt/sources.list \u0026lt;\u0026lt;EOF # 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释 deb https://mirrors.aliyun.com/ubuntu/ jammy main restricted universe multiverse deb-src https://mirrors.aliyun.com/ubuntu/ jammy main restricted universe multiverse deb https://mirrors.aliyun.com/ubuntu/ jammy-security main restricted universe multiverse deb-src https://mirrors.aliyun.com/ubuntu/ jammy-security main restricted universe multiverse deb https://mirrors.aliyun.com/ubuntu/ jammy-updates main restricted universe multiverse deb-src https://mirrors.aliyun.com/ubuntu/ jammy-updates main restricted universe multiverse # deb https://mirrors.aliyun.com/ubuntu/ jammy-proposed main restricted universe multiverse # deb-src https://mirrors.aliyun.com/ubuntu/ jammy-proposed main restricted universe multiverse deb https://mirrors.aliyun.com/ubuntu/ jammy-backports main restricted universe multiverse deb-src https://mirrors.aliyun.com/ubuntu/ jammy-backports main restricted universe multiverse EOF dockerfile样例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 # https://github.com/Williamyzd/vllm/blob/main/Dockerfile.cpu # This vLLM Dockerfile is used to construct image that can build and run vLLM on x86 CPU platform. # \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;更换为国内镜像 FROM registry.cn-hangzhou.aliyuncs.com/reg_pub/ubuntu:22.04 AS cpu-test-1 ENV CCACHE_DIR=/root/.cache/ccache ENV CMAKE_CXX_COMPILER_LAUNCHER=ccache # \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;添加国内源 RUN cp /etc/apt/sources.list /etc/apt/sources.list.bak \\ \u0026amp;\u0026amp; echo \u0026#34;deb https://mirrors.aliyun.com/ubuntu/ jammy main restricted universe multiverse\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list \\ \u0026amp;\u0026amp; echo \u0026#34;deb-src https://mirrors.aliyun.com/ubuntu/ jammy main restricted universe multiverse\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list \\ \u0026amp;\u0026amp; echo \u0026#34;deb https://mirrors.aliyun.com/ubuntu/ jammy-security main restricted universe multiverse\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list \\ \u0026amp;\u0026amp; echo \u0026#34;deb-src https://mirrors.aliyun.com/ubuntu/ jammy-security main restricted universe multiverse\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list \\ \u0026amp;\u0026amp; echo \u0026#34;deb https://mirrors.aliyun.com/ubuntu/ jammy-updates main restricted universe multiverse\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list \\ \u0026amp;\u0026amp; echo \u0026#34;deb-src https://mirrors.aliyun.com/ubuntu/ jammy-updates main restricted universe multiverse\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list \\ \u0026amp;\u0026amp; echo \u0026#34;deb https://mirrors.aliyun.com/ubuntu/ jammy-backports main restricted universe multiverse\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list \\ \u0026amp;\u0026amp; echo \u0026#34;deb-src https://mirrors.aliyun.com/ubuntu/ jammy-backports main restricted universe multiverse\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list # \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;这里添加了清华pip源 RUN --mount=type=cache,target=/var/cache/apt \\ apt-get update -y \\ \u0026amp;\u0026amp; apt-get install -y curl ccache git wget vim numactl gcc-12 g++-12 python3 python3-pip libtcmalloc-minimal4 libnuma-dev \\ \u0026amp;\u0026amp; apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\ \u0026amp;\u0026amp; update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12 \\ \u0026amp;\u0026amp; pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple # https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html # intel-openmp provides additional performance improvement vs. openmp # tcmalloc provides better memory allocation efficiency, e.g, holding memory in caches to speed up access of commonly-used objects. RUN --mount=type=cache,target=/root/.cache/pip \\ pip install intel-openmp ENV LD_PRELOAD=\u0026#34;/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4:/usr/local/lib/libiomp5.so\u0026#34; RUN echo \u0026#39;ulimit -c 0\u0026#39; \u0026gt;\u0026gt; ~/.bashrc RUN pip install https://intel-extension-for-pytorch.s3.amazonaws.com/ipex_dev/cpu/intel_extension_for_pytorch-2.4.0%2Bgitfbaa4bc-cp310-cp310-linux_x86_64.whl ENV PIP_EXTRA_INDEX_URL=https://download.pytorch.org/whl/cpu RUN --mount=type=cache,target=/root/.cache/pip \\ --mount=type=bind,src=requirements-build.txt,target=requirements-build.txt \\ pip install --upgrade pip \u0026amp;\u0026amp; \\ pip install -r requirements-build.txt # install oneDNN RUN git clone -b rls-v3.5 https://github.com/oneapi-src/oneDNN.git RUN --mount=type=cache,target=/root/.cache/ccache \\ cmake -B ./oneDNN/build -S ./oneDNN -G Ninja -DONEDNN_LIBRARY_TYPE=STATIC \\ -DONEDNN_BUILD_DOC=OFF \\ -DONEDNN_BUILD_EXAMPLES=OFF \\ -DONEDNN_BUILD_TESTS=OFF \\ -DONEDNN_BUILD_GRAPH=OFF \\ -DONEDNN_ENABLE_WORKLOAD=INFERENCE \\ -DONEDNN_ENABLE_PRIMITIVE=MATMUL \u0026amp;\u0026amp; \\ cmake --build ./oneDNN/build --target install --config Release FROM cpu-test-1 AS build WORKDIR /workspace/vllm RUN --mount=type=cache,target=/root/.cache/pip \\ --mount=type=bind,src=requirements-common.txt,target=requirements-common.txt \\ --mount=type=bind,src=requirements-cpu.txt,target=requirements-cpu.txt \\ pip install -v -r requirements-cpu.txt COPY ./ ./ # Support for building with non-AVX512 vLLM: docker build --build-arg VLLM_CPU_DISABLE_AVX512=\u0026#34;true\u0026#34; ... ARG VLLM_CPU_DISABLE_AVX512 ENV VLLM_CPU_DISABLE_AVX512=${VLLM_CPU_DISABLE_AVX512} RUN --mount=type=cache,target=/root/.cache/pip \\ --mount=type=cache,target=/root/.cache/ccache \\ VLLM_TARGET_DEVICE=cpu python3 setup.py bdist_wheel \u0026amp;\u0026amp; \\ pip install dist/*.whl WORKDIR /workspace/ RUN ln -s /workspace/vllm/tests \u0026amp;\u0026amp; ln -s /workspace/vllm/examples \u0026amp;\u0026amp; ln -s /workspace/vllm/benchmarks ENTRYPOINT [\u0026#34;python3\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;vllm.entrypoints.openai.api_server\u0026#34;] 下载模型文件 方法一：使用git 1 2 3 4 5 6 7 # 使用git 下载，文件比较大，因而使用git-lfs来支持断点续传 安装参考：https://github.com/git-lfs/git-lfs/blob/main/INSTALLING.md curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash yum install git-lfs mkdir -p ~/ModelSpace \u0026amp;\u0026amp; cd ~/ModelSpace git lfs install git clone https://www.modelscope.cn/qwen/qwen2-0.5b.git Qwen2-0.5B 方法二：直接使用modelscope工具 1 2 3 4 pip install modelscope modelscope download --model qwen/qwen2-0.5b --local_dir ~/ModelSpace/ #modelscope -h 查看使用说明 #https://www.modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD 运行测试 直接跑会报错，重新写一个dockerfile以方便测试\n1 2 3 4 # 注意这里的镜像名称为上一步所打镜像 FROM vllm-cpu-env # 这里是为了覆盖掉老的endpoint以方便做测试 ENTRYPOINT tail -f /dev/null 执行以下命令进行测试，指定了 8000端口暴漏，同时将下载的模型挂载到容器，并指定共享内存为4G\ndocker run -d \u0026ndash;name llms -v /root/llmvs/ModelSpace:/workspace/ModelSpace -p 8000:8000 \u0026ndash;shm-size 4g llms:v0.2\n正式运行 1 docker run -d --name llms -v /root/llmvs/ModelSpace:/workspace/ModelSpace -p 8000:8000 --shm-size 4g vllm-cpu-env:latest --model /workspace/ModelSpace/Qwen2-0.5B 参考链接：https://qwen.readthedocs.io/en/latest/deployment/vllm.html\n1 2 3 4 5 6 7 8 9 10 11 curl -i http://localhost:8080/v1/chat/completions -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;/workspace/ModelSpace/Qwen2-0.5B\u0026#34;, \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你是一名人工智能领域的专家\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;如何学习大模型应用？\u0026#34;} ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;top_p\u0026#34;: 0.8, \u0026#34;repetition_penalty\u0026#34;: 1.05, \u0026#34;max_tokens\u0026#34;: 300 }\u0026#39; 如果以上过程你都不想做，可以直接拉取我打好的cpu镜像：registry.cn-hangzhou.aliyuncs.com/reg_pub/vllm-cpu-env:latest ","date":"2024-09-23T08:38:25Z","permalink":"https://blog.liu-nian.top/ai/vllm-cpu%E6%8E%A8%E7%90%86-%E4%BB%A5qwen2-0.5b%E4%B8%BA%E4%BE%8B/","title":"lvllm-cpu推理-以qwen2-0.5b为例"},{"content":"摘要： 本文档详细介绍了使用Hugo搭建Markdown博客的过程，包括本地编辑环境搭建、主题样式修改、远程部署及图床搭建等步骤。通过本地编辑Markdown文档并预览，成功后可推送至GitHub私有仓库。Hugo将Markdown文档渲染为HTML文件，可利用GitHub Pages服务或VPS中的容器对外暴露。同时，文档还介绍了如何配置SSH Key、克隆主题仓库、修改主题样式，以及通过GitHub Pages或自建服务器进行远程部署，并提供了基于GitHub的图床搭建方法。\nPowered by 百度Comate\n基本思路：\n使用在本地编辑Markdown文档，并测试预览。成功后推送到github私有仓库。hugo可以将markdown文档渲染为html文件。可以使用github的pages服务对外暴漏，也可在vps中启动一个容器来对外暴漏。\n1. 本地编辑环境搭建 1.1 二进制安装hugo 直接安装二进制文件，可参考：https://github.com/gohugoio/hugo/releases 下载对应版本并安装 以mac客户端为例，下载对应版本后解压到指定目录。\n1 2 3 4 5 6 7 # 文件解压 tar -zxvf hugo_0.143.0_darwin-universal.tar.gz cd hugo_0.143.0_darwin-universal chmod +x hugo mv hugo /usr/local/bin # 验证测试 hugo version 1.2 源码安装hugo(可选) 如需源码安装，以mac客户端为例，需要安装git、hugo。\n安装git mac自带git，无需安装。如需安装或更新，可参考：https://git-scm.com/downloads/mac\n安装go 可参考链接：https://go.dev/dl/选择对应版本的hugo，下载后解压到指定目录。\n安装hugo go install github.com/gohugoio/hugo@latest\n2. 修改主题样式 2.1 配置ssh key 此处为本地开发环境需要，方便免密提交文件到git仓库。这里以提交到github仓库为例。如果主要在国内访问，且有备案域名，建议使用gitee或自有git仓库。 可参考链接： ssh本地秘钥生成\n1 2 3 4 5 6 7 ssh-keygen -t ed25519 -C \u0026#34;your_email@example.com\u0026#34; # 邮箱为github注册邮箱,一路输入enter即可 # 如果你使用的是不支持 Ed25519 算法的旧系统，请使用以下命令： # ssh-keygen -t rsa -b 4096 -C \u0026#34;your_email@example.com\u0026#34; # 将 SSH 私钥添加到 ssh-agent。 ssh-add ~/.ssh/id_ed25519 # 查看公钥内容，将其复制 cat ~/.ssh/id_ed25519.pub 访问自己的github账号，在账号设置中添加ssh公钥 2.2 克隆主题仓库到本地 将自己喜欢的主题克隆到本地，和hugo一起使用。因涉及到对主题的更改和维护，建议使用git 子模块来维护。可以将该主题fork到自己的空间，然后将自己fork的主题作为子模块添加。这里以stack主题为例。\n1 2 3 4 5 6 7 8 9 10 # 创建第一个站点 hugo new site pages # pages 为站点文件夹名字 cd pages git init # 注意初始化的相对位置 # 添加主题，以stack为例。注意github仓库地址不要使用https协议的，否则会出现子模块的更改无法提交的情况 git submodule add --name stack git@github.com:Williamyzd/hugo-theme-stack.git themes/stack # 添加一个测试页面 hugo new content content/posts/my-first-post.md echo \u0026#34;hello,word!\u0026#34; \u0026gt;\u0026gt; content/posts/my-first-post.md hugo server -D -p 8000 # 可查看处于草稿状态下的文档内容 -p 制定可对外暴漏端口，默认是1314 浏览器访问页面：http://localhost:8000/posts/my-first-post/，可看到发布内容 2.3 修改主题样式 修改主题样式，主要在hugo.toml(yaml)文件中，或者在主题文件夹中修改。可将themes/stack/exampleSite/hugo.yaml 复制到站点根目录，并修改为自己的配置。\nparams-sidebar-avatar修改logo params-mainSections 指定主页默认获取的最新文章 menu-main 指定导航模块 menu-social 指定社交模块 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 baseurl: https://example.com/ languageCode: en-us # 主题名称，注意要和子模块的文件夹名一致 theme: stack # 网站标题 title: Example Site # 版权信息 copyright: Example Person # 多语言支持相关 # Theme i18n support # Available values: ar, bn, ca, de, el, en, es, fr, hu, id, it, ja, ko, nl, pt-br, th, uk, zh-cn, zh-hk, zh-tw DefaultContentLanguage: en # Set hasCJKLanguage to true if DefaultContentLanguage is in [zh-cn ja ko] # This will make .Summary and .WordCount behave correctly for CJK languages. hasCJKLanguage: false languages: en: languageName: English title: Example Site weight: 1 params: sidebar: subtitle: Example description zh-cn: languageName: 中文 title: 演示站点 weight: 2 params: sidebar: subtitle: 演示说明 pagination: pagerSize: 3 permalinks: post: /p/:slug/ page: /:slug/ params: # 这个参数比较关键，决定主页回去哪些目录下去取最新的文章来显示 mainSections: - ai - cs - ops - tools featuredImageField: image rssFullContent: true favicon: # e.g.: favicon placed in `static/favicon.ico` of your site folder, then set this field to `/favicon.ico` (`/` is necessary) footer: since: 2020 customText: dateFormat: published: Jan 02, 2006 lastUpdated: Jan 02, 2006 15:04 MST sidebar: emoji: 🍥 subtitle: Lorem ipsum dolor sit amet, consectetur adipiscing elit. avatar: enabled: true local: true src: img/avatar.png # logo存放的位置，在`static/img/`下 article: math: false toc: true readingTime: true license: enabled: true default: Licensed under CC BY-NC-SA 4.0 # 是否开启评论，建议不开 comments: enabled: false provider: disqus # 默认会开启的几个组件：搜索、归档、分类、标签云 widgets: homepage: - type: search - type: archives params: limit: 5 - type: categories params: limit: 10 - type: tag-cloud params: limit: 10 page: - type: toc opengraph: twitter: # Your Twitter username site: # Available values: summary, summary_large_image card: summary_large_image defaultImage: opengraph: enabled: false local: false src: colorScheme: # Display toggle toggle: true # Available values: auto, light, dark default: auto imageProcessing: cover: enabled: true content: enabled: true ### Custom menu ### See https://stack.jimmycai.com/config/menu ### To remove about, archive and search page menu item, remove `menu` field from their FrontMatter # 自定义的菜单项 menu: main: - identifier: ai #标识符，唯一值 name: AI # 显示的名称 url: /ai # 文件路径，相对于content文件夹 weight: -90 #权重 #权重越小，越靠前显示 params: ### For demonstration purpose, the home link will be open in a new tab newTab: false #是否新开标签页 icon: ai # 图标信息，查看和自定义图标，可修改 themes/stack/assets/icons 下的文件，将文件名写在这里 - identifier: ops name: 运维\u0026amp;工具 url: /ops weight: -90 #权重 params: ### For demonstration purpose, the home link will be open in a new tab newTab: false icon: ops #更多的图标信息见后 - identifier: cs name: CS基础 url: /cs weight: -90 #权重 params: ### For demonstration purpose, the home link will be open in a new tab newTab: true icon: cs #更多的图标信息见后 social: - identifier: github name: GitHub url: https://github.com/CaiJimmy/hugo-theme-stack params: icon: brand-github - identifier: twitter name: Twitter url: https://twitter.com params: icon: brand-twitter related: includeNewer: true threshold: 60 toLower: false indices: - name: tags weight: 100 - name: categories weight: 200 markup: goldmark: extensions: passthrough: enable: true delimiters: block: - - \\[ - \\] - - $$ - $$ inline: - - \\( - \\) renderer: ## Set to true if you have HTML content inside Markdown unsafe: true tableOfContents: endLevel: 4 ordered: true startLevel: 2 highlight: noClasses: false codeFences: true guessSyntax: true lineNoStart: 1 lineNos: true lineNumbersInTable: true tabWidth: 4 现在可以在content目录下创建文档了。 最终效果如下： 3. 远程部署 hugo 生成的静态html文件在public目录下，可以考虑单独创建一个仓库来管理，在主git仓库下再建一个子模块。\n3.1 添加静态文件子模块 添加子模块主要使用命令如下：\n新建远程仓库：git@github.com:Williamyzd/blogs-html.git 在主仓库下添加子模块： 1 git submodule add -n pub-html git@github.com:Williamyzd/blogs-html.git blog/public 如果 子模块添加错误，可用如下命令删除：\n1 2 3 git rm --cached pub-html # pub-html:子模块名称 rm -rf blog/public # 删除本地文件夹 vim .gitmodules # 删除.gitmodules文件中的子模块信息 3.2 远程服务器部署(可选) 创建Dockerfile文件，参考如下： 如无服务器可用，可以不考虑这一步。 通过vps（服务器）托管一个nginx服务。dockerfile参考： 1 2 3 4 5 6 7 8 9 10 11 12 FROM nginx:1.27.3-alpine WORKDIR /home COPY blog.conf /etc/nginx/conf.d/default.conf # 准备证书,如有可打开 # COPY cert /etc/nginx/cert RUN apk update \u0026amp;\u0026amp; apk add git \u0026amp;\u0026amp; \\ git clone https://github.com/Williamyzd/blogs-html.git /home/public \u0026amp;\u0026amp; \\ echo \u0026#34;echo \\$(date +%Y-%m-%d) \\$(date +%H:%M:%S) [info]: \\$(cd /home/public \u0026amp;\u0026amp; git pull) \u0026gt;\u0026gt; /var/log/blog/\\$(date +%Y-%m-%d).log\u0026#34; \u0026gt;\u0026gt; /home/blog.sh \u0026amp;\u0026amp; \\ chmod +x /home/blog.sh \u0026amp;\u0026amp; \\ mkdir -p /var/log/blog \u0026amp;\u0026amp; \\ echo \u0026#34;*/1 * * * * /home/blog.sh\u0026#34; \u0026gt;\u0026gt; /etc/crontabs/root CMD crond \u0026amp;\u0026amp; nginx \u0026amp;\u0026amp; tail -f /dev/null Dockerfile 内容说明： 这里使用了更加轻量版的alpine版本nginx，并安装了git。这个镜像本身具备定时功能，创建了一个定时任务，每天执行一次git pull命令。因仓库是只读且对外公开，所以这里不需要认证。最后的启动命令是先启动定时任务，再启动nginx服务。同时保留一个主进程，保证容器不会退出。 nginx配置文件参考： blog.conf参考(未包含https配置)： 1 2 3 4 5 6 7 8 server { listen 80; server_name blog.xxx.com; location / { root /home/public; index index.html index.htm; } } ssl配置的例子,需要提前准备下证书，并将dockerfile 中的注释去掉。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 以下属性中以ssl开头的属性代表与证书配置有关，其他属性请根据自己的需要进行配置。 server { listen 80; # 需要调整为您证书绑定的域名。 server_name blog.xxx.com return 301 https://$host/$request_uri; } server { listen 443 ssl; server_name blog.xxx.com; # localhost修改为您证书绑定的域名。 ssl_certificate /etc/nginx/cert/domain_ca.crt; # 将domain_ca.crt替换成您证书链的文件名(如下载文件没有domain_ca.crt则使用domain.crt或domain.pem)。 ssl_certificate_key /etc/nginx/cert/domain.key; # 将domain.key替换成您证书的密钥文件名。 ssl_session_timeout 5m; # 指定SSL/TLS会话的超时时间 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; #使用此加密套件。 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #使用该协议进行配置。 ssl_prefer_server_ciphers on; location / { root /home/public; #站点目录。 index index.html index.htm; } }\t创建docker-compose.yml文件，参考如下： 1 2 3 4 5 6 7 8 9 10 11 12 version: \u0026#39;3\u0026#39; services: blog: build: context: . dockerfile: Dockerfile image: blog:v1.0 container_name: blog restart: always ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; 构建并启动容器： 将ssl证书（可选）、blog.conf、Dockerfile、Dockerfile文件放到同一目录下，执行如下命令： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 一键构建并启动脚本 docker-compose up -d # 如果镜像版本不存在，会自动构建镜像，然后启动镜像 # 如需在启动时，都重新构建镜像可执行如下命令 docker-compose up -d --build # 单独构建镜像并启动容器 # 如当前操作环境的架构与目标机器的架构不同，可以构建镜像，并指定操作系统架构，如： docker build --platform linux/amd64,linux/arm64 -t blog:v1.0 . docker compose up -d # 停止脚本 docker-compose down 3.3 使用github pages托管 3.2 自建容器的方式部署，需要自备服务器、域名等，成本较高。不具备条件或者想节约成本，可以考虑白嫖github，使用github pages托管静态文件。只需要一些简单的配置即可。 上文我们已经创建了html文件的仓库：https://github.com/Williamyzd/blogs-html.git 我们只需要在仓库的设置中，开启github pages功能即可。\n进入仓库设置页面，选择pages选项卡 在Source中选择main分支，并选择/（根目录） 稍等片刻，即可通过域名访问了。 其中的自定义域名（标红的3），需要有自己的域名，并在域名解析中添加一条解析记录，指向github pages提供的域名。如本例中，域名pages.liu-nian.top需要添加的记录是：williamyzd.github.io。其模式为[github用户名].github.io。当前域名在百度智能云托管，其添加记录如下： 至此可以愉快地访问自己的博客了。\n4. 图床搭建-基于github 本人主要使用vscode撰写博客，vscode有丰富的插件。关于markdown相关的，当前使用到的如下： 在博客撰写过程中,不免要复制、粘贴图片，如果直接上传到博客中，涉及到图片存储为止的考虑。而huogo中如果想在编写和最终生产环境中能够展示图片，涉及到几处复杂配置。做过几次尝试便放弃了。博主之前使用过picgo等图床工具，可以直接将复制的图片上传到对应的文件服务器，同时生成一个文件的链接，直接插入文档即可。可惜picgo已经停止维护了。常见的图床常见的托管方如七牛、微博、腾讯云、阿里云等是免费的或一定限额的免费。处于不愿受制与人的考量，这里考虑使用github搭建一个自己的图床。\n4.1 创建仓库 选择创建一个公开的仓库，用于存放图片。 进入设置中，创建token，用于插件上传图片时使用。可直接点击链接配置：https://github.com/settings/personal-access-tokens github的开发者token可用api的方式对git仓库、账号进行控制。token分为两种，一种是账号级权限控制，另外一种是项目级别的控制。建议选用项目级别控制的 fine-grained personal token。 红框部分注意填写，其中过期时间根据自己需要填写。 注意选中repo权限，勾选repo的写入权限。 创建后页面会显示token信息，进行复制保存，在后续插件配置时使用。 4.2 安装vscode插件 这里使用markdown-image插件，安装后配置如下：\nbase 配置，主要是选github为默认存储方式、规定图片存储的格式等，其他内容酌情设置 File Name Format 这个选项比较关键，建议使用年月方式创建文件夹，当月的文件放在一起，以防止文件过多，导致github仓库管理麻烦货加载缓慢。 github配置，主要是填写仓库地址、token等信息。token信息来源于github账号中生成的token。 至此，图片上传配置完成。 参考链接：\n图标生成：https://www.iconfont.cn/search/index 博客搭建：https://gohugo.io/getting-started/quick-start/ ssh设置：https://docs.github.com/zh/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account 博客设置参考：https://yidude.com/html/15.html ","date":"2024-05-16T02:54:56+08:00","permalink":"https://blog.liu-nian.top/ops/%E6%90%AD%E5%BB%BAhugo%E5%8D%9A%E5%AE%A2/","title":"搭建Markdown博客--基于hugo"},{"content":"摘要: 本文详细介绍了Linux Volume Manager (LVM) 的相关命令，包括磁盘分区、物理卷创建、卷组管理、逻辑卷创建与管理、文件系统挂载与自动挂载等步骤。LVM 是一种逻辑卷管理技术，允许用户在物理磁盘上创建、扩展、缩减和移动分区，而不必重新分区或重新格式化磁盘。通过LVM，可以更灵活地管理存储空间，提高存储利用率和灵活性。\n参考链接：https://support.huaweicloud.com/bestpractice-evs/evs_02_0002.html\n磁盘分区 新挂载硬盘或者扩充硬盘\nparted相关命令 按照如下命令建立分区、文件系统格式、打上lvm标签\n1 2 3 4 5 6 7 8 9 lsblk # 查看挂载磁盘 parted /dev/vdd # 回车后按照以下操作： mklabel gpt # 大于2T以上磁盘使用该格式 mkpart p1 ext4 0 100GB # p1 分区名称,ext4 文件系统格式 0 起始位置（百分比or数字），100GB结束位置（百分比or数字） print #查看分区结果 toggle 3 lvm # 3 新创建分区的编号，lvm 标签，从而可以使用lvm来管理 quit # 退出parted命令 lsblk #查看分区情况 通过新增磁盘来创建物理卷 pvcreate /dev/vdd3\n物理卷 查看磁盘 1 2 3 fdisk -l | grep /dev/vd | grep -v vda Disk /dev/vdb: 10.7 GB, 10737418240 bytes, 20971520 sectors Disk /dev/vdc: 10.7 GB, 10737418240 bytes, 20971520 sectors 执行以下命令，将云硬盘创建为物理卷。 pvcreate 磁盘设备名1 磁盘设备名2 磁盘设备名3\n参数说明如下：\n磁盘设备名：此处需要填写磁盘的设备名称，如果需要批量创建，可以填写多个设备名称，中间以空格间隔。\n命令示例:\npvcreate /dev/vdb /dev/vdc\n1 2 3 pvcreate /dev/vdb /dev/vdc Physical volume \u0026#34;/dev/vdb\u0026#34; successfully created. Physical volume \u0026#34;/dev/vdc\u0026#34; successfully created. 执行如下命令，查看系统中物理卷的详细信息。 pvdisplay\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 pvdisplay \u0026#34;/dev/vdc\u0026#34; is a new physical volume of \u0026#34;10.00 GiB\u0026#34; --- NEW Physical volume --- PV Name /dev/vdc VG Name PV Size 10.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID dypyLh-xjIj-PvG3-jD0j-yup5-O7SI-462R7C \u0026#34;/dev/vdb\u0026#34; is a new physical volume of \u0026#34;10.00 GiB\u0026#34; --- NEW Physical volume --- PV Name /dev/vdb VG Name PV Size 10.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID srv5H1-tgLu-GRTl-Vns8-GfNK-jtHk-Ag4HHB 卷组 创建卷组 执行以下命令，创建卷组。 vgcreate 卷组名 物理卷名称1 物理卷名称2 物理卷名称3\u0026hellip;\n参数说明如下：\n​\t卷组名：可自定义，此处以vgdata为例。\n​\t物理卷名称：此处需要填写待添加进卷组的所有物理卷名称，中间以空格隔开。\n命令示例:\nvgcreate vgdata /dev/vdb /dev/vdc\n回显类似如下信息：\n1 2 vgcreate vgdata /dev/vdb /dev/vdc Volume group \u0026#34;vgdata\u0026#34; successfully created 执行如下命令，查看系统中卷组的详细信息。 vgdisplay\n卷组扩/缩容 查看磁盘，并如创建物理卷\nfdisk -l | grep /dev/vd | grep -v vda\npvcreate 磁盘**设备名\n添加新的物理卷\nvgextend 卷组名称 物理卷名称\n命令示例:\nvgextend vgdata /dev/vdd\n删除物理卷\nvgreduce 卷组名称 物理卷名称\n命令示例:\nvgreduce vgdata /dev/vdd\n逻辑卷 创建逻辑卷 lvcreate** -L 逻辑卷大小 -n 逻辑卷名称 卷组名称 参数说明如下：\n逻辑卷大小：该值应小于卷组剩余可用空间大小，单位可以选择“MB”或“GB”。\n逻辑卷名称：可自定义，此处以lvdata1为例。\n卷组名称：此处需要填写逻辑卷所在的卷组名称。\n命令示例:\nlvcreate -L 15GB -n lvdata1 vgdata\n1 2 lvcreate -L 15GB -n lvdata1 vgdata Logical volume \u0026#34;lvdata1\u0026#34; created. 执行如下命令，查询系统中逻辑卷的详细信息。\nlvdisplay\n扩充逻辑卷 执行如下命令，扩展逻辑卷的容量。 lvextend -L +增加容量 逻辑卷路径\n参数说明如下：\n增加容量：该值应小于组卷剩余可用空间大小，单位可以选择“MB”或“GB”。\n逻辑卷路径：此处需要填写待扩容的逻辑卷的路径。\n命令示例: lvextend -L +4GB /dev/vgdata/lvdata1\n回显类似如下信息：\n1 2 lvextend -L +4GB /dev/vgdata/lvdata1 Size of logical volume vgdata/lvdata1 changed from 15.00 GiB (3840 extents) to 19.00 GiB (4864 extents). Logical volume vgdata/lvdata1 successfully resized. 此时只是扩展的逻辑卷的容量，在其之上的文件系统也要随之进行扩展才能使用。\n执行如下命令，扩展文件系统的容量。\nresize2fs 逻辑卷路径\n命令示例: resize2fs /dev/vgdata/lvdata1\n回显类似如下信息：\n1 2 resize2fs /dev/vgdata/lvdata1 resize2fs 1.42.9 (28-Dec-2013) Filesystem at /dev/vgdata/lvdata1 is mounted on /Data1; on-line resizing required old_desc_blocks = 4, new_desc_blocks = 28 The filesystem on /dev/vgdata/lvdata1 is now 3657728 blocks long. 执行如下命令，查看文件系统容量是否增加。\ndf -h 回显类似如下信息：\n1 2 3 4 5 6 7 8 9 10 df -h Filesystem Size Used Avail Use% Mounted on /dev/vda2 39G 1.5G 35G 5% / devtmpfs 487M 0 487M 0% /dev tmpfs 496M 0 496M 0% /dev/shm tmpfs 496M 6.7M 490M 2% /run tmpfs 496M 0 496M 0% /sys/fs/cgroup /dev/vda1 976M 131M 779M 15% /boot tmpfs 100M 0 100M 0% /run/user/0 /dev/mapper/vgdata-lvdata1 19G 44M 18G 1% /Data1 可以看到，文件系统“/dev/mapper/vgdata-lvdata1”的容量相比之前增加了4GB。\n逻辑卷缩容 不可在线操作，需要先umount 挂载\n主要命令\numount /Data1 # /data1为挂载路径 e2fsck -f /dev/vgdata/lvdata1 # 进行磁盘检查 resize2fs /dev/vgdata/lvdata1 1G # 调整文件系统 lvreduce -L 1GB /dev/vgdata/lvdata # 通过lvduce命令进行缩 mount /dev/vgdata/lvdata1 /Data1 # 重新挂载 1 2 3 4 5 6 7 8 9 10 umount /Data1 # /data1为挂载路径 mount |grep Data1|wc -l # 查看是否成功卸载 lvs |grep Data1 # 查看磁盘状态 e2fsck -f /dev/vgdata/lvdata1 # 进行磁盘检查 resize2fs /dev/vgdata/lvdata1 1G # 调整文件系统 lvs |grep Data1 # 查看磁盘状态，未发生变化 lvreduce -L 1GB /dev/vgdata/lvdata # 通过lvduce命令进行缩 lvs |grep Data1 # 查看磁盘状态，可发现减少 mount /dev/vgdata/lvdata1 /Data1 # 重新挂载 df -lh |grep Data1 # 查看当前状态 挂载文件系统 执行如下命令，创建文件系统。 **mkfs.**文件格式 逻辑卷路径\n命令示例:\nmkfs.ext4 /dev/vgdata/lvdata1\n回显类似如下信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 mkfs.ext4 /dev/vgdata/lvdata1 mke2fs 1.42.9 (28-Dec-2013) Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) Stride=0 blocks, Stripe width=0 blocks 983040 inodes, 3932160 blocks 196608 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=2151677952 120 block groups 32768 blocks per group, 32768 fragments per group 8192 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208 Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): done Writing superblocks and filesystem accounting information: done 手动挂载 具备文件系统后才可以挂载\n创建挂载目录。 mkdir 挂载目录 命令示例: mkdir /Data1 执行如下命令，将文件系统挂载到目录下。 mount 逻辑卷路径 挂载目录 命令示例： mount /dev/vgdata/lvdata1 /Data1 执行如下命令，查询文件系统挂载信息。 mount | grep 挂载目录 命令示例： mount | grep /Data1 ​\t回显类似如下信息：\n1 2 mount | grep /Data1 /dev/mapper/vgdata-lvdata1 on /Data1 type ext4 (rw,relatime,data=ordered) 开机自动挂载 执行以下步骤，设置云服务器系统启动时自动挂载文件系统。\n执行如下命令，查询文件系统的UUID。\nblkid 文件系统路径\n以查询“dev/mapper/vgdata-lvdata1”的UUID为例：\nblkid /dev/mapper/vgdata-lvdata1\n1 2 blkid /dev/mapper/vgdata-lvdata1 /dev/mapper/vgdata-lvdata1: UUID=\u0026#34;c6a243ce-5150-41ac-8816-39db54d1a4b8\u0026#34; TYPE=\u0026#34;ext4\u0026#34; 执行以下命令，打开“/etc/fstab”文件。\nvi /etc/fstab\n1 2 3 4 5 6 7 8 9 10 11 vi /etc/fstab # # /etc/fstab # Created by anaconda on Tue Nov 7 14:28:26 2017 # # Accessible filesystems, by reference, are maintained under \u0026#39;/dev/disk\u0026#39; # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info # UUID=27f9be47-838b-4155-b20b-e4c5e013cdf3 / ext4 defaults 1 1 UUID=2b2000b1-f926-4b6b-ade8-695ee244a901 /boot ext4 defaults 1 2 按“i”进入编辑模式，将光标移至文件末尾，按“Enter”，添加如下内容。\n1 UUID=c6a243ce-5150-41ac-8816-39db54d1a4b8 /Data1 ext4 defaults 0 0 内容说明如下：\n第一列：UUID，此处填写1查询的UUID；\n第二列：文件系统的挂载目录；\n第三列：文件系统的文件格式，如文件格式“ext4”;\n第四列：挂载选项，此处以“defaults”为例；\n第五列：备份选项，设置为“1”时，系统自动对该文件系统进行备份；设置为“0”时，不进行备份。此处以“0”为例；\n第六列：扫描选项，设置为“1”时，系统在启动时自动对该文件系统进行扫描；设置为“0”时，不进行扫描。此处以“0”为例。\n按“Esc”，输入“:wq!”，并按“Enter”。保存设置并退出vi编辑器。\n执行以下步骤，验证自动挂载功能\n执行如下命令，卸载文件系统。\numount 逻辑卷路径\n命令示例： umount /dev/vgdata/lvdata1\n执行如下命令，将/etc/fstab文件所有内容重新加载。\nmount -a\n执行如下命令，查询文件系统挂载信息。\nmount | grep 挂载目录\n命令示例： mount | grep /Data1\n回显类似如下信息，说明自动挂载功能生效：\n1 2 mount | grep /Data1 /dev/mapper/vgdata-lvdata1 on /Data1 type ext4 (rw,relatime,data=ordered) ​\n","date":"2024-05-11T08:38:47Z","permalink":"https://blog.liu-nian.top/ops/lvm%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4/","title":"LVM相关命令"},{"content":"资源要求： 4C/8G\nhttps://kind.sigs.k8s.io/docs/user/known-issues/#failure-to-create-cluster-with-cgroups-v2\ncentos 7 在线升级内核 默认内核版本太低\n![image-20240513154841260](/Users/yangzedong/Library/Application Support/typora-user-images/image-20240513154841260.png)\n参考链接：https://www.cnblogs.com/liugp/p/16950443.html\n安装docker/kubectl kubectl :https://kubernetes.io/zh-cn/docs/tasks/tools/install-kubectl-linux/\n1 curl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; docker：https://www.runoob.com/docker/centos-docker-install.html\n1 curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun Helm:https://helm.sh/zh/docs/intro/install/\n安装k8s 参考文档：https://github.com/kubernetes-sigs/kind/tree/v0.22.0\n安装kind 二进制文件\n1 [ $(uname -m) = x86_64 ] \u0026amp;\u0026amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.21.0/kind-$(uname)-amd64 *可能比较慢，可以选择手动下载安装，二进制包下载地址：https://github.com/kubernetes-sigs/kind/releases\n1 2 3 4 # 执行安装操作 chmod +x ./kind chmod +x ./kind mv ./kind /usr/local/bin/ 拉取镜像（可选）\n1 2 3 4 5 6 7 8 # 选一个版本 $ docker search kindest/node NAME DESCRIPTION STARS OFFICIAL kindest/node https://sigs.k8s.io/kind node image 104 kindest/node-amd64 2 kindest/node-arm64 0 # 镜像拉取 docker pull kindest/node 安装集群\n1 2 3 4 # 在线创建集群 kind create cluster # 指定 配置文件 # kind create cluster --config kind-example-config.yaml 配置文件地址：https://kind.sigs.k8s.io/docs/user/configuration/\n安装ingress\n1 2 3 4 5 6 7 8 9 10 11 # 1. 获取yaml wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml # 2. 因服务器无外网，mac上进行下载，指定架构：一般是amd64和arm64、aarch64 # registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1 # registry.k8s.io/ingress-nginx/controller:v1.10.1 # registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.1 docker pull --platform amd64 xxx # 3. 装载image kind load docker-image img:v1 # 或者 kind load image-archive my-image-archive.tar ![image-20240513220418988](/Users/yangzedong/Library/Application Support/typora-user-images/image-20240513220418988.png)\nkind架构 参考文档：https://kind.sigs.k8s.io/docs/design/initial/\nk8s中的端口映射\n参考地址：https://www.cnblogs.com/baixiaoyong/p/16051137.html\nk8s中的pod:https://fly-luck.github.io/2018/04/15/Kubernetes%20Ports/\ncontainerPort vs. hostPort 出现在如Deployment、Pod等资源对象描述文件中的容器部分，针对容器端口起类似于docker run -p \u0026lt;containerPort\u0026gt;:\u0026lt;hostPort\u0026gt;的作用： containerPort：容器暴露的端口。 hostPort：容器暴露的端口直接映射到的主机端口。\nport vs. targetPort vs. nodePort 出现在Service描述文件中，当Service的类型为ClusterIP时： port：Service中ClusterIP对应的端口。 targetport：clusterIP作为负载均衡， 后端目标实例（容器）的端口，与上述containerPort保持一致。\n当Service的类型为NodePort时： nodePort：由于ClusterIP只能集群内访问，配置nodePort会在每个运行kubelet节点的宿主机打开一个端口，用于集群外部访问。\n","date":"2024-05-11T08:38:25Z","permalink":"https://blog.liu-nian.top/ai/kind-%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4/","title":"kind 安装k8s集群"},{"content":"https://blog.csdn.net/qq836825331/article/details/136846624\n下载镜像 Kustomize安装 1 2 curl -s \u0026#34;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\u0026#34; | bash chmod +x kustomize \u0026amp;\u0026amp; mv kustomize /usr/local/bin/ 下载镜像 1 2 3 4 5 6 7 wget https://github.com/kubeflow/manifests/archive/refs/tags/v1.8.1.tar.gz tar -zxvf v1.8.1.tar.gz cd v1.8.1 # 过滤出镜像，注意需要手动做下排查 kustomize build example |grep \u0026#39;image: \u0026#39;|awk \u0026#39;$2 != \u0026#34;\u0026#34; { print $2}\u0026#39; |sort -u # 下载镜像，并push到dockerhub \u0026#34;for i in `cat images.txt`; do docker pull $i ;tag_w=`echo \u0026#34;$i\u0026#34; | sed \u0026#39;s/\\//_/g\u0026#39;`;tag_new=\u0026#34;williamyang1/kubeflow_$tag_w\u0026#34;;docker tag $i $tag_new ;docker push $tag_new ;docker rmi $tag_new $i;echo \u0026#34;$tag_new\u0026#34; \u0026gt;\u0026gt; n_images.txt; done\u0026#34; 最终的镜像列表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 williamyang1/busybox:1.28 williamyang1/docker.io_istio_pilot:1.17.5 williamyang1/docker.io_istio_proxyv2:1.17.5 williamyang1/docker.io_kubeflowkatib_earlystopping-medianstop:v0.16.0 williamyang1/docker.io_kubeflowkatib_enas-cnn-cifar10-cpu:v0.16.0 williamyang1/docker.io_kubeflowkatib_file-metrics-collector:v0.16.0 williamyang1/docker.io_kubeflowkatib_katib-controller:v0.16.0 williamyang1/docker.io_kubeflowkatib_katib-db-manager:v0.16.0 williamyang1/docker.io_kubeflowkatib_katib-ui:v0.16.0 williamyang1/docker.io_kubeflowkatib_mxnet-mnist:v0.16.0 williamyang1/docker.io_kubeflowkatib_pytorch-mnist-cpu:v0.16.0 williamyang1/docker.io_kubeflowkatib_suggestion-darts:v0.16.0 williamyang1/docker.io_kubeflowkatib_suggestion-enas:v0.16.0 williamyang1/docker.io_kubeflowkatib_suggestion-goptuna:v0.16.0 williamyang1/docker.io_kubeflowkatib_suggestion-hyperband:v0.16.0 williamyang1/docker.io_kubeflowkatib_suggestion-hyperopt:v0.16.0 williamyang1/docker.io_kubeflowkatib_suggestion-optuna:v0.16.0 williamyang1/docker.io_kubeflowkatib_suggestion-pbt:v0.16.0 williamyang1/docker.io_kubeflowkatib_suggestion-skopt:v0.16.0 williamyang1/docker.io_kubeflowkatib_tfevent-metrics-collector:v0.16.0 williamyang1/docker.io_kubeflowmanifestswg_oidc-authservice:e236439 williamyang1/docker.io_kubeflownotebookswg_centraldashboard:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_jupyter-web-app:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_kfam:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_notebook-controller:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_poddefaults-webhook:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_profile-controller:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_pvcviewer-controller:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_tensorboard-controller:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_tensorboards-web-app:v1.8.0 williamyang1/docker.io_kubeflownotebookswg_volumes-web-app:v1.8.0 williamyang1/docker.io_metacontrollerio_metacontroller:v2.0.4 williamyang1/docker.io_seldonio_mlserver:1.3.2 williamyang1/gcr.io_knative-releases_knative.dev_eventing_cmd_controller@sha256:92967bab4ad8f7d55ce3a77ba8868f3f2ce173c010958c28b9a690964ad6ee9b williamyang1/gcr.io_knative-releases_knative.dev_eventing_cmd_mtping@sha256:6d35cc98baa098fc0c5b4290859e363a8350a9dadc31d1191b0b5c9796958223 williamyang1/gcr.io_knative-releases_knative.dev_eventing_cmd_webhook@sha256:ebf93652f0254ac56600bedf4a7d81611b3e1e7f6526c6998da5dd24cdc67ee1 williamyang1/gcr.io_knative-releases_knative.dev_net-istio_cmd_controller@sha256:421aa67057240fa0c56ebf2c6e5b482a12842005805c46e067129402d1751220 williamyang1/gcr.io_knative-releases_knative.dev_net-istio_cmd_webhook@sha256:bfa1dfea77aff6dfa7959f4822d8e61c4f7933053874cd3f27352323e6ecd985 williamyang1/gcr.io_knative-releases_knative.dev_serving_cmd_activator@sha256:c2994c2b6c2c7f38ad1b85c71789bf1753cc8979926423c83231e62258837cb9 williamyang1/gcr.io_knative-releases_knative.dev_serving_cmd_autoscaler@sha256:8319aa662b4912e8175018bd7cc90c63838562a27515197b803bdcd5634c7007 williamyang1/gcr.io_knative-releases_knative.dev_serving_cmd_controller@sha256:98a2cc7fd62ee95e137116504e7166c32c65efef42c3d1454630780410abf943 williamyang1/gcr.io_knative-releases_knative.dev_serving_cmd_domain-mapping@sha256:f66c41ad7a73f5d4f4bdfec4294d5459c477f09f3ce52934d1a215e32316b59b williamyang1/gcr.io_knative-releases_knative.dev_serving_cmd_domain-mapping-webhook@sha256:7368aaddf2be8d8784dc7195f5bc272ecfe49d429697f48de0ddc44f278167aa williamyang1/gcr.io_knative-releases_knative.dev_serving_cmd_queue@sha256:dabaecec38860ca4c972e6821d5dc825549faf50c6feb8feb4c04802f2338b8a williamyang1/gcr.io_knative-releases_knative.dev_serving_cmd_webhook@sha256:4305209ce498caf783f39c8f3e85dfa635ece6947033bf50b0b627983fd65953 williamyang1/gcr.io_kubebuilder_kube-rbac-proxy:v0.13.1 williamyang1/gcr.io_kubebuilder_kube-rbac-proxy:v0.8.0 williamyang1/gcr.io_ml-pipeline_api-server:2.0.5 williamyang1/gcr.io_ml-pipeline_cache-server:2.0.5 williamyang1/gcr.io_ml-pipeline_frontend williamyang1/gcr.io_ml-pipeline_frontend:2.0.5 williamyang1/gcr.io_ml-pipeline_metadata-writer:2.0.5 williamyang1/gcr.io_ml-pipeline_minio:RELEASE.2019-08-14T20-37-41Z-license-compliance williamyang1/gcr.io_ml-pipeline_mysql:8.0.26 williamyang1/gcr.io_ml-pipeline_persistenceagent:2.0.5 williamyang1/gcr.io_ml-pipeline_scheduledworkflow:2.0.5 williamyang1/gcr.io_ml-pipeline_viewer-crd-controller:2.0.5 williamyang1/gcr.io_ml-pipeline_visualization-server williamyang1/gcr.io_ml-pipeline_workflow-controller:v3.3.10-license-compliance williamyang1/gcr.io_tfx-oss-public_ml_metadata_store_server:1.14.0 williamyang1/ghcr.io_dexidp_dex:v2.36.0 williamyang1/kserve_kserve-controller:v0.11.2 williamyang1/kserve_lgbserver:v0.11.2 williamyang1/kserve_models-web-app:v0.10.0 williamyang1/kserve_paddleserver:v0.11.2 williamyang1/kserve_pmmlserver:v0.11.2 williamyang1/kserve_sklearnserver:v0.11.2 williamyang1/kserve_xgbserver:v0.11.2 williamyang1/kubeflow_training-operator:v1-855e096 williamyang1/mysql:8.0.29 williamyang1/nvcr.io_nvidia_tritonserver:23.05-py3 williamyang1/python:3.7 williamyang1/pytorch_torchserve-kfs:0.8.2 williamyang1/quay.io_jetstack_cert-manager-cainjector:v1.12.2 williamyang1/quay.io_jetstack_cert-manager-controller:v1.12.2 williamyang1/quay.io_jetstack_cert-manager-webhook:v1.12.2 williamyang1/tensorflow_serving:2.6.2 安装k8s Kubesphere 安装k8s\u0026ndash;v1.27.10 https://kubesphere.io/zh/docs/v3.4/quick-start/all-in-one-on-linux/\nhttps://kubesphere.io/zh/blogs/using-kubekey-deploy-k8s-v1.28.8/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 安装基础软件 yum install -y socat conntrack ebtables ipset ipvsadm # 安装工具 # shell安装 或者离线下载 curl -sfL https://get-kk.kubesphere.io | sh - ## 离线下载 https://github.com/kubesphere/kubekey/releases/download/v3.1.1/kubekey-v3.1.1-linux-amd64.tar.gz chmod +x kk mv kk /usr/local/bin/ # 确认是否包含想要安装的版本 kk version --show-supported-k8s # 创建manifest 导出供离线下载使用 kk create manifest --with-kubernetes v1.27 # 下载基础环境镜像-manifest用到的工具，单独下载--可选 # 镜像所在地址 https://github.com/kubesphere/kubekey/releases/tag/v3.1.1 # 找一台访问速度快的机器下载 https://github.com/kubesphere/kubekey/releases/download/v3.1.1/centos7-rpms-amd64.iso # 这一步骤会下载manifest-sample.yaml中涉及的的工具如kubelet/containerd/helm等 ./kk artifact export -m manifest-sample.yaml -o kubesphere.tar.gz # 此步骤为方便没有外网的机器创建集群。 ./kk create cluster -f config-sample.yaml -a kubesphere.tar.gz --with-packages 安装kubeflow https://github.com/kubeflow/manifests\n1 2 # 下载镜像 for i in `cat images.txt`; do ctr imgaes pull $i ;done ","date":"2024-05-11T08:38:25Z","permalink":"https://blog.liu-nian.top/ai/kubeflow%E5%AE%89%E8%A3%85/","title":"Kubeflow 安装"},{"content":"参考文献：https://www.jianshu.com/p/d78fff321005\n数据存储格式：WKT VS GeoJSON WKT是什么？ WKT(well-know text)是开放地理空间联盟OGC制定的一种文本标记语言，用于表示矢量几何对象、空间参照系统及空间参照系统之间的转换。\nWKB是什么？ WKB(well-know binary)是WKT的二进制表现形式，解决WKT表达冗余的问题，便于传输和存储在数据库中。\nGeoJSON是什么？ 以JSON的格式输出空间数据，便于被javascript等脚本调用。\nWKT与GeoJSON WKT与GeoJSON分为点、线、面、几何集合四种： 点 线 面 组合 Point MultiPoint LineString MultiLineString Polygon MultiPolygon GeometryCollection 类型 WKT GeoJSON Point POINT(10 10) { \u0026ldquo;type\u0026rdquo;: \u0026ldquo;Point\u0026rdquo;, \u0026ldquo;coordinates\u0026rdquo;: [10, 10] } LineString LINESTRING(10 10, 20 30) { \u0026ldquo;type\u0026rdquo;: \u0026ldquo;Point\u0026rdquo;, \u0026ldquo;coordinates\u0026rdquo;:[ [10, 10], [20, 30] ] } Polygon POLYGON(10 10, 15 16, 22 10, 30 32) { \u0026ldquo;type\u0026rdquo;: \u0026ldquo;Polygon\u0026rdquo;, \u0026ldquo;coordinates\u0026rdquo;: [ [ [10, 10], [10, 10], [10, 10], [10, 10] ] ] } MultiPoint MULTIPOINT(*) { \u0026ldquo;type\u0026rdquo;: \u0026ldquo;MultiPoint\u0026rdquo;, \u0026ldquo;coordinates\u0026rdquo;: [*] } MultiLineString MULTILINESTRING (*) { \u0026ldquo;type\u0026rdquo;: \u0026ldquo;MultiLineString\u0026rdquo;, \u0026ldquo;coordinates\u0026rdquo;: [*] } MultiPolygon MULTIPOLYGON (*) { \u0026ldquo;type\u0026rdquo;: \u0026ldquo;MultiPolygon\u0026rdquo;, \u0026ldquo;coordinates\u0026rdquo;: [*] } GEOMETRYCOLLECTION GEOMETRYCOLLECTION(POINT(2 3),LINESTRING(2 3,3 4)) - WKT与GeoJSON的区别 WKT是用来单独表示空间点、线、面数据，GeoJSON还可以用来表示空间数据和属性数据的集合 （crs、bbox属性）。 使用GeoPandas 读取 wkt字符串数据 获取山东省地图土地轮廓并使用GeoPandas加载展示\n通过百度地图api获取数据信息 将形状字符串转换为MultiPolygon类型数据 通过GeoPandas加载数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # encoding:utf-8 # 根据您选择的AK已为您生成调用代码 # 检测到您当前的AK设置了IP白名单校验 # 您的IP白名单中的IP非公网IP，请设置为公网IP，否则将请求失败 # 请在IP地址为0.0.0.0/0 外网IP的计算发起请求，否则将请求失败 import requests sds =None # 服务地址 host = \u0026#34;https://api.map.baidu.com\u0026#34; # 接口地址 uri = \u0026#34;/api_region_search/v1/\u0026#34; # 此处填写你在控制台-应用管理-创建应用后获取的AK ak = \u0026#34;\u0026#34; params = { \u0026#34;keyword\u0026#34;: \u0026#34;山东省\u0026#34;, \u0026#34;sub_admin\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;ak\u0026#34;: ak, \u0026#34;extensions_code\u0026#34;:1, \u0026#34;boundary\u0026#34;:1, } response = requests.get(url = host + uri, params = params) sds =None if response: # print(response.json()) sds = response.json() print(sds[\u0026#39;districts\u0026#39;]) # pd.DataFrame(sds[\u0026#39;districts\u0026#39;]) ","date":"2024-05-11T08:38:25Z","permalink":"https://blog.liu-nian.top/ai/%E5%9C%B0%E7%90%86%E4%BF%A1%E6%81%AF%E5%8F%AF%E8%A7%86%E5%8C%96/","title":"地理信息可视化"}]
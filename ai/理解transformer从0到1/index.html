<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="建议书籍： 实战书籍\n《机器学习实战（原书第2版）》 基于sklearn、Tesnsorflow\nhttps://book.douban.com/subject/35218199/\ufeff\n动手学深度学习》 (书籍、代码、视频-沐神出品，可以关注沐神的知乎和b站，经常会发干货)\nhttp://zh.d2l.ai/chapter_preface/index.html\ufeff\n漫画机器学 https://book.sciencereading.cn/shop/book/Booksimple/onlineRead.do?id=BBFFD36F5A94C6D7DE053010B0A0AB613000&amp;readMark=0\ufeff\n理论书籍\n《机器学习》 （西瓜书-周志华-比较通俗易懂）\nhttps://book.douban.com/subject/26708119/\ufeff\n">
<title></title>

<link rel='canonical' href='https://blog.liu-nian.top/ai/%E7%90%86%E8%A7%A3transformer%E4%BB%8E0%E5%88%B01/'>

<link rel="stylesheet" href="/scss/style.min.946cca6c6259ef94ac55abfae7c7bf3291ea3ed5eea17ef77500b257217c6710.css"><meta property='og:title' content="">
<meta property='og:description' content="建议书籍： 实战书籍\n《机器学习实战（原书第2版）》 基于sklearn、Tesnsorflow\nhttps://book.douban.com/subject/35218199/\ufeff\n动手学深度学习》 (书籍、代码、视频-沐神出品，可以关注沐神的知乎和b站，经常会发干货)\nhttp://zh.d2l.ai/chapter_preface/index.html\ufeff\n漫画机器学 https://book.sciencereading.cn/shop/book/Booksimple/onlineRead.do?id=BBFFD36F5A94C6D7DE053010B0A0AB613000&amp;readMark=0\ufeff\n理论书籍\n《机器学习》 （西瓜书-周志华-比较通俗易懂）\nhttps://book.douban.com/subject/26708119/\ufeff\n">
<meta property='og:url' content='https://blog.liu-nian.top/ai/%E7%90%86%E8%A7%A3transformer%E4%BB%8E0%E5%88%B01/'>
<meta property='og:site_name' content='流年'>
<meta property='og:type' content='article'><meta property='article:section' content='Ai' />
<meta name="twitter:title" content="">
<meta name="twitter:description" content="建议书籍： 实战书籍\n《机器学习实战（原书第2版）》 基于sklearn、Tesnsorflow\nhttps://book.douban.com/subject/35218199/\ufeff\n动手学深度学习》 (书籍、代码、视频-沐神出品，可以关注沐神的知乎和b站，经常会发干货)\nhttp://zh.d2l.ai/chapter_preface/index.html\ufeff\n漫画机器学 https://book.sciencereading.cn/shop/book/Booksimple/onlineRead.do?id=BBFFD36F5A94C6D7DE053010B0A0AB613000&amp;readMark=0\ufeff\n理论书籍\n《机器学习》 （西瓜书-周志华-比较通俗易懂）\nhttps://book.douban.com/subject/26708119/\ufeff\n">
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="切换菜单">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/logo_hu_fd3008965b65babe.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">🍥</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">流年</a></h1>
            <h2 class="site-description">这个人很懒，暂时啥都没写</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://github.com/Williamyzd'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://www.zhihu.com/people/william-19-37'
                        target="_blank"
                        title="知乎"
                        rel="me"
                    >
                        
                        
                            <?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1738662787698" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="10144" xmlns:xlink="http://www.w3.org/1999/xlink" width="48" height="48"><path d="M526.677333 766.421333l-72.021333 45.824-90.922667-142.933333c-18.773333 59.818667-50.005333 113.706667-91.264 163.2-17.152 20.608-34.986667 39.168-55.509333 58.666667-6.613333 6.272-33.066667 30.592-37.461333 34.986666l-60.330667-60.330666c5.930667-5.930667 33.578667-31.36 39.04-36.522667 18.346667-17.408 33.92-33.706667 48.725333-51.456 54.016-64.768 86.613333-136.96 91.178667-223.189333H128v-85.333334h170.666667V298.666667h-37.034667c-29.397333 54.016-66.474667 94.805333-111.701333 121.898666L106.069333 347.434667c59.52-35.754667 103.466667-111.104 129.621334-228.693334l83.285333 18.517334c-5.973333 27.008-12.928 52.352-20.864 76.074666H490.666667v85.333334H384v170.666666h106.666667v85.333334H391.893333l134.784 211.754666z m163.754667-2.986666L738.048 725.333333H810.666667V298.666667h-170.666667v426.666666h31.402667l19.029333 38.101334zM554.666667 213.333333h341.333333v597.333334h-128l-106.666667 85.333333-42.666666-85.333333H554.666667V213.333333z" p-id="10145" fill="#8a8a8a"></path></svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://mp.weixin.qq.com/s/YOHKU850wcmPfWCjeAZ7zw'
                        target="_blank"
                        title="wechat"
                        rel="me"
                    >
                        
                        
                            <?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1738662705411" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="9050" xmlns:xlink="http://www.w3.org/1999/xlink" width="48" height="48"><path d="M767.818667 409.173333C867.338667 444.266667 938.666667 539.136 938.666667 650.666667c0 42.709333-10.496 83.978667-30.261334 120.842666-1.792 3.338667-4.992 8.928-9.696 16.96l14.613334 53.557334c6.506667 23.893333-15.402667 45.813333-39.296 39.296l-53.642667-14.634667-6.229333 3.669333A254.933333 254.933333 0 0 1 682.666667 906.666667c-77.994667 0-147.84-34.88-194.805334-89.888a352.608 352.608 0 0 1-56.64 4.554666c-63.338667 0-124.266667-16.853333-177.472-48.298666-1.834667-1.088-6.410667-3.733333-13.632-7.893334l-80.544 21.653334c-23.914667 6.432-45.76-15.573333-39.146666-39.434667l21.792-78.752a961.205333 961.205333 0 0 1-15.904-27.317333A336.384 336.384 0 0 1 85.333333 480c0-188.618667 154.965333-341.333333 345.888-341.333333 159.914667 0 297.984 108.010667 335.818667 259.296 0.949333 3.765333 1.173333 7.552 0.778667 11.2z m-68.106667-13.952C662.88 282.037333 555.178667 202.666667 431.221333 202.666667 275.434667 202.666667 149.333333 326.933333 149.333333 480c0 46.272 11.498667 90.837333 33.194667 130.698667 2.88 5.290667 10.176 17.706667 21.621333 36.746666a32 32 0 0 1 3.413334 25.013334l-10.517334 37.994666 39.232-10.549333a32 32 0 0 1 24.234667 3.146667c14.272 8.192 22.773333 13.098667 25.802667 14.890666A283.882667 283.882667 0 0 0 431.221333 757.333333c6.154667 0 12.288-0.192 18.389334-0.576A255.061333 255.061333 0 0 1 426.666667 650.666667c0-141.386667 114.613333-256 256-256 5.728 0 11.413333 0.192 17.045333 0.554666z m133.706667 397.056a32 32 0 0 1 3.338666-24.725333 996.672 996.672 0 0 0 15.242667-26.293333A190.997333 190.997333 0 0 0 874.666667 650.666667c0-106.037333-85.962667-192-192-192s-192 85.962667-192 192 85.962667 192 192 192a190.933333 190.933333 0 0 0 98.570666-27.2c2.208-1.322667 8.288-4.874667 18.517334-10.837334a32 32 0 0 1 24.522666-3.210666l12.565334 3.424-3.424-12.565334zM330.666667 426.666667a42.666667 42.666667 0 1 1 0-85.333334 42.666667 42.666667 0 0 1 0 85.333334z m192 0a42.666667 42.666667 0 1 1 0-85.333334 42.666667 42.666667 0 0 1 0 85.333334z m85.333333 202.666666a32 32 0 1 1 0-64 32 32 0 0 1 0 64z m149.333333 0a32 32 0 1 1 0-64 32 32 0 0 1 0 64z" fill="#8a8a8a" p-id="9051"></path></svg>
                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/%E5%85%B3%E4%BA%8E/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>关于</span>
            </a>
        </li>
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>主页</span>
            </a>
        </li>
        
        
        <li >
            <a href='/ai' >
                
                
                
                    <?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1738342212304" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4225" width="48" height="48" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M501.824 32C303.552 32 141.504 176.992 141.504 357.76c0 23.712 2.816 47.104 8.32 69.856l-51.008 114.208a32 32 0 0 0 24.704 44.736c54.272 7.744 76.672 31.168 76.672 77.312v111.552a64 64 0 0 0 64 64h20.704a64 64 0 0 1 64 64V960a32 32 0 0 0 32 32h345.6a32 32 0 0 0 0-64h-313.6v-24.608a128 128 0 0 0-128-128h-20.736v-111.552c0-65.664-32.192-110.688-91.2-131.136l39.872-89.28a31.968 31.968 0 0 0 1.568-21.792 233.088 233.088 0 0 1-8.896-63.904c0-143.712 131.936-261.76 296.32-261.76s296.32 118.016 296.32 261.76a32 32 0 0 0 64 0C862.144 176.992 700.064 32 501.824 32zM904 448a32 32 0 0 0-32 32v360a32 32 0 0 0 64 0V480a32 32 0 0 0-32-32z" p-id="4226"></path><path d="M673.888 466.656c-11.744-25.568-48.416-24.64-58.816 1.536l-132.8 333.76a32 32 0 0 0 59.488 23.68l32.608-81.92c0.576 0.032 1.088 0.32 1.664 0.32h154.848l38.176 83.104a31.968 31.968 0 1 0 58.144-26.72l-153.312-333.76zM599.68 680l47.264-118.72 54.528 118.72H599.68z" p-id="4227"></path></svg>
                
                <span>AI</span>
            </a>
        </li>
        
        
        <li >
            <a href='/cs' >
                
                
                
                    <?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1738342654057" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="9507" xmlns:xlink="http://www.w3.org/1999/xlink" width="48" height="48"><path d="M954.1888 786.3552h-332.032v178.176h130.8416c16.0256 0 29.4912 12.928 29.4912 29.4656 0 16.5632-12.928 30.0032-29.4912 30.0032H271.0016a29.4912 29.4912 0 0 1-29.4912-29.4912c0-16.5376 12.928-29.9776 29.4912-29.9776h130.3296v-178.176H69.8112A69.8368 69.8368 0 0 1 0 716.544V69.8112A70.1696 70.1696 0 0 1 69.8112 0h884.3776A69.8368 69.8368 0 0 1 1024 69.8112V716.544a69.8368 69.8368 0 0 1-69.8112 69.8112z m-492.8768 178.176h100.864v-178.176h-100.864v178.176zM964.5312 59.4688H59.4688v667.4176h905.0624V59.4688zM175.3344 364.8512l107.5456-108.0832c5.7088-5.6832 12.9536-8.7808 21.2224-8.7808 7.7568 0 15.5136 3.0976 21.1968 8.7808 11.904 11.392 11.3664 30.5152 0 41.8816l-92.0576 92.0832 92.0576 92.032a30.1568 30.1568 0 0 1-0.512 42.4192 28.16 28.16 0 0 1-20.6848 8.7808c-7.7568 0-15.0016-3.0976-20.6848-8.7808l-107.0592-107.0592-1.024-1.024a59.52 59.52 0 0 1-5.7088-4.6592 29.2864 29.2864 0 0 1-8.7808-21.1968v-1.0496c0-8.2688 3.0976-15.5136 8.7808-21.1968a23.3472 23.3472 0 0 1 5.7088-4.1472z m391.4752-112.7424a30.9248 30.9248 0 0 1 25.8816-15.488c5.1712 0 10.3424 1.536 14.976 4.1216 7.2448 4.1216 11.904 10.3424 13.9776 18.0992 2.0736 7.7568 1.024 16.0256-3.0976 22.7584l-160.8448 278.2464a30.1312 30.1312 0 0 1-40.3456 10.8544 29.2352 29.2352 0 0 1-11.3664-40.3456l160.8192-278.2464z m123.1104 41.9072c0-8.2944 3.0976-15.5136 8.7808-21.2224 5.6832-5.6832 13.44-8.7808 21.1968-8.7808 7.7568 0 15.0016 3.0976 20.6848 8.7808l107.0592 107.0592 1.0496 0.512c2.048 1.0496 4.1216 2.5856 5.6832 4.1472 5.6832 5.6832 8.7808 13.44 8.7808 21.1968v3.1232c-0.512 7.2448-3.6096 13.952-8.7808 19.1232-1.5616 1.536-3.0976 2.5856-5.1712 5.6832l-108.1088 108.0832a30.08 30.08 0 0 1-21.1968 8.8064c-8.2688 0-15.5136-3.0976-21.1968-8.8064a30.1568 30.1568 0 0 1-8.7808-21.1968c0-8.2688 3.0976-15.5136 8.7808-21.1968l92.0576-92.0576-92.0576-92.0576a30.1568 30.1568 0 0 1-8.7808-21.1968z" fill="#7C848E" p-id="9508"></path></svg>
                
                <span>CS基础</span>
            </a>
        </li>
        
        
        <li >
            <a href='/ops' >
                
                
                
                    <?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1738342768989" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="11403" xmlns:xlink="http://www.w3.org/1999/xlink" width="48" height="48"><path d="M685.1 727.2h4.1c77.5-19 135.9-69.3 150.9-133.2 8.2-36.7 1.4-66.6-1.4-77.5-17.7-69.3-77.5-119.6-150.9-126.4h-19c-32.6-59.8-95.2-96.5-163.1-96.5-91.1 0-168.6 66.6-183.5 153.6-73.4 4.1-130.5 63.9-130.5 138.7 0 77.5 63.9 140 141.4 140h334.4M656.4 672H346.7c-51.6 0-93.2-41.6-93.2-91.9 0-51.6 41.6-93.2 93.2-93.2h2.5l20.1 1.3 1.3-18.9c5-70.5 64.2-124.7 134.7-124.7 52.9 0 100.7 31.5 123.4 79.3l6.3 12.6 13.9-1.3c7.6-1.3 13.9-1.3 22.7-1.3 51.6 5 93.2 40.3 105.8 89.4 2.5 7.6 7.6 29 1.3 54.1-10.1 44.1-52.9 79.3-108.3 94.4" fill="#444444" p-id="11404"></path><path d="M949.7 475c-13.3 0-25-9.7-27.1-23.3C908 357.5 860 271 787.4 208.2c-35.9-31.1-76.6-55.3-120.8-72.1-45.8-17.4-94-26.2-143.4-26.2-136.4 0-262.7 68.1-337.8 182.3-8.4 12.7-25.4 16.2-38.1 7.9-12.7-8.4-16.2-25.4-7.9-38.1C224.7 132.4 368.2 55 523.2 55c56.1 0 110.9 10 162.9 29.8 50.2 19.1 96.4 46.6 137.2 81.9 82.4 71.3 137 169.5 153.6 276.7 2.3 15-8 29.1-23 31.4-1.4 0.1-2.8 0.2-4.2 0.2zM523.2 973.2c-62 0-122.1-12.1-178.7-36.1-54.7-23.1-103.8-56.2-145.9-98.4-42.2-42.2-75.3-91.3-98.4-145.9-24-56.6-36.1-116.8-36.1-178.7 0-15.5 0.8-31.2 2.3-46.6 1.5-15.1 15-26.1 30.1-24.6 15.1 1.5 26.1 15 24.6 30.1-1.4 13.6-2.1 27.4-2.1 41.1 0 54.6 10.7 107.5 31.7 157.3 20.4 48.1 49.5 91.3 86.6 128.5 37.1 37.1 80.3 66.3 128.5 86.6 49.8 21.1 102.7 31.7 157.3 31.7 86.1 0 168.4-26.7 237.7-77.3 33.4-24.4 62.8-53.6 87.4-86.8 24.9-33.6 44.3-70.7 57.6-110.1 4.9-14.4 20.5-22.1 34.9-17.2 14.4 4.9 22.1 20.5 17.2 34.9-15.2 44.8-37.3 86.9-65.5 125.1-27.9 37.7-61.3 70.9-99.3 98.6-78.6 57.5-172 87.8-269.9 87.8z" fill="#444444" p-id="11405"></path><path d="M75.5 405.1l-56.6 98c-6.2 10.8 1.6 24.2 14 24.2H146c12.4 0 20.2-13.5 14-24.2l-56.6-98c-6.2-10.7-21.7-10.7-27.9 0zM952.3 501.2l54.4-94.2c6.7-11.6-1.7-26.1-15.1-26.1H882.8c-13.4 0-21.8 14.5-15.1 26.1l54.4 94.2c6.8 11.6 23.5 11.6 30.2 0z" fill="#444444" p-id="11406"></path></svg>
                
                <span>运维&amp;工具</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        
        <li >
            <a href='/links/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>Links</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>暗色模式</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">目录</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#建议书籍"><strong>建议书籍</strong>：</a></li>
    <li><a href="#标量向量张量">标量、向量、张量</a>
      <ol>
        <li><a href="#标量-一个单独的数字"><strong><strong>标量：</strong></strong> 一个单独的数字</a></li>
        <li><a href="#向量-一个向量是一列数词向量就是一列数字"><strong><strong>向量</strong></strong> ：一个向量是一列数。词向量就是一列数字。</a></li>
        <li><a href="#张量"><strong><strong>张量：</strong></strong></a>
          <ol>
            <li><a href="#从视觉角度">从视觉角度：</a></li>
            <li><a href="#从nlp角度">从NLP角度：</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#分词与词向量">分词与词向量</a>
      <ol>
        <li><a href="#为什么要分词">为什么要分词？</a>
          <ol>
            <li><a href="#自然语言类任务的流程中分词是重要一环">自然语言类任务的流程中分词是重要一环</a></li>
            <li><a href="#中文分词难点">中文分词难点</a></li>
          </ol>
        </li>
        <li><a href="#中文分词算法">中文分词算法</a>
          <ol>
            <li><a href="#基于词典的分词算法">基于词典的分词算法</a></li>
            <li><a href="#子词分词">子词分词</a></li>
          </ol>
        </li>
        <li><a href="#获取词向量">获取词向量：</a>
          <ol>
            <li><a href="#独热表示onehotrepresentation">独热表示（Onehotrepresentation）</a></li>
            <li><a href="#分布表示distributedrepresentation">分布表示（DistributedRepresentation）</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#需要掌握的神经网络结构rnnlstmseq2seq">需要掌握的神经网络结构：RNN、LSTM、Seq2Seq</a>
      <ol>
        <li><a href="#rnn与bi-rnn">RNN与Bi-RNN:</a></li>
        <li><a href="#序列到序列seq2seq">序列到序列（Seq2Seq）</a></li>
      </ol>
    </li>
    <li><a href="#注意力机制注意力自注意力多头注意力">注意力机制：注意力、自注意力、多头注意力</a>
      <ol>
        <li><a href="#注意力">注意力</a></li>
        <li><a href="#自注意力">自注意力</a></li>
        <li><a href="#自注意力和注意力机制的区别">自注意力和注意力机制的区别：</a></li>
      </ol>
    </li>
    <li><a href="#编码器与解码器">编码器与解码器</a>
      <ol>
        <li><a href="#位置编码">位置编码</a></li>
      </ol>
    </li>
    <li><a href="#编码器">编码器</a>
      <ol>
        <li><a href="#解码器">解码器：</a></li>
      </ol>
    </li>
    <li><a href="#transformer模型的计算量评估">Transformer模型的计算量评估</a></li>
    <li><a href="#transformer模型可视化">Transformer模型可视化</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/ai/%E7%90%86%E8%A7%A3transformer%E4%BB%8E0%E5%88%B01/"></a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    阅读时长: 21 分钟
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h2 id="建议书籍"><strong>建议书籍</strong>：
</h2><p><strong>实战书籍</strong></p>
<ul>
<li>《机器学习实战（原书第2版）》</li>
</ul>
<p>基于sklearn、Tesnsorflow</p>
<p><a class="link" href="https://book.douban.com/subject/35218199/"  target="_blank" rel="noopener"
    >https://book.douban.com/subject/35218199/</a>﻿</p>
<ul>
<li>动手学深度学习》</li>
</ul>
<p>(书籍、代码、视频-沐神出品，可以关注沐神的知乎和b站，经常会发干货)</p>
<p><a class="link" href="http://zh.d2l.ai/chapter_preface/index.html"  target="_blank" rel="noopener"
    >http://zh.d2l.ai/chapter_preface/index.html</a>﻿</p>
<ul>
<li>漫画机器学</li>
</ul>
<p><a class="link" href="https://book.sciencereading.cn/shop/book/Booksimple/onlineRead.do?id=BBFFD36F5A94C6D7DE053010B0A0AB613000&amp;readMark=0"  target="_blank" rel="noopener"
    >https://book.sciencereading.cn/shop/book/Booksimple/onlineRead.do?id=BBFFD36F5A94C6D7DE053010B0A0AB613000&amp;readMark=0</a>﻿</p>
<p><strong>理论书籍</strong></p>
<ul>
<li>《机器学习》</li>
</ul>
<p>（西瓜书-周志华-比较通俗易懂）</p>
<p><a class="link" href="https://book.douban.com/subject/26708119/"  target="_blank" rel="noopener"
    >https://book.douban.com/subject/26708119/</a>﻿</p>
<ul>
<li>《统计学习方法（第2版）》</li>
</ul>
<p>（大量公式-比较难啃，但是知识点非常全面，类似于知识清单）</p>
<p><a class="link" href="https://book.douban.com/subject/33437381/"  target="_blank" rel="noopener"
    >https://book.douban.com/subject/33437381/</a>﻿</p>
<p>*《深度学习》</p>
<p>（花书-深度学习三巨头鸿篇巨制）</p>
<p><a class="link" href="https://book.douban.com/subject/27087503/"  target="_blank" rel="noopener"
    >https://book.douban.com/subject/27087503/</a>﻿</p>
<p><strong>NLP相关</strong></p>
<ul>
<li>《文本数据挖掘》</li>
</ul>
<p>nlp-领域大佬新做</p>
<p><a class="link" href="https://book.douban.com/subject/34441323/"  target="_blank" rel="noopener"
    >https://book.douban.com/subject/34441323/</a>﻿</p>
<ul>
<li>《Speech and Language Processing (3rd ed. draft)》</li>
</ul>
<p>斯坦福大佬著作，不断更新中。中文译名，《自然语言处理综述》，对应版本为第二版。</p>
<p><a class="link" href="https://web.stanford.edu/~jurafsky/slp3/"  target="_blank" rel="noopener"
    >https://web.stanford.edu/~jurafsky/slp3/</a>﻿</p>
<p>参考资料：</p>
<p><a class="link" href="https://llmbook-zh.github.io/LLMBook.pdf"  target="_blank" rel="noopener"
    >https://llmbook-zh.github.io/LLMBook.pdf</a></p>
<h2 id="标量向量张量">标量、向量、张量
</h2><h3 id="标量-一个单独的数字"><strong><strong>标量：</strong></strong> 一个单独的数字
</h3><h3 id="向量-一个向量是一列数词向量就是一列数字"><strong><strong>向量</strong></strong> ：一个向量是一列数。词向量就是一列数字。
</h3><h3 id="张量"><strong><strong>张量：</strong></strong>
</h3><h4 id="从视觉角度">从视觉角度：
</h4><p>一阶张量是一个一维数组，即一组数，我们可以将一组数表示为一个矢量</p>
<p>二阶张量是一个矩阵，因此我们可以将张量作为矢量和矩阵概念的推广。</p>
<p>三阶张量
对于 RGB 图片，我们可以理解为由三张分别表示 R,G,B 分量的图片堆叠而成，如下</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=849d6fd7beb94891863d86015a307582&amp;docGuid=shKc-gyOcGMxHN"
	
	
	
	loading="lazy"
	
	
></p>
<p>对于每个分量图片，我们都可以看成一个矩阵，那么一张 RGB 图片就可以用三阶的张量进行表示。</p>
<p>四阶张量
对于多张 RGB 图片，我们可以用四阶张量进行表示，可以看作是三阶张量（单张RGB图像）的数组（多张 RGB 图片）。</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=d1472cf3275c4f8889526b8d70619de4&amp;docGuid=shKc-gyOcGMxHN"
	
	
	
	loading="lazy"
	
	
></p>
<p>五阶张量
视频是由多张图片组成的，因此每个视频可以用一个四阶张量表示，显然，多个视频可以用五阶张量表示。</p>
<h4 id="从nlp角度">从NLP角度：
</h4><p>一阶张量一个字或词</p>
<p>二阶张量是一句话。</p>
<p>三阶以上的张量：一段话或者一篇文章，或者是将大量的语句分成若干个批次。</p>
<p>﻿</p>
<h2 id="分词与词向量">分词与词向量
</h2><p>参考链接：<a class="link" href="https://blog.csdn.net/m0_63171455/article/details/142107937"  target="_blank" rel="noopener"
    >AI大模型基础：1.分词_大模型分词-CSDN博客</a>﻿</p>
<h3 id="为什么要分词">为什么要分词？
</h3><h4 id="自然语言类任务的流程中分词是重要一环">自然语言类任务的流程中分词是重要一环
</h4><p>分词是自然语言处理的基础，分词准确度直接决定了后面的词性标注、句法分析、词向量以及文本分析的质量。英文语句使用空格将单词进行分隔，除了某些特定词，如how many，New York等外，大部分情况下不需要考虑分词问题。但中文不同，天然缺少分隔符，需要读者自行分词和断句。故在做中文自然语言处理时，需要先进行分词。</p>
<p>﻿</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=35655eec53f24844aed43a9cecc39e21&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<p>﻿</p>
<p>分词就是将句子、段落、文章这种长文本，分解为以字词甚至更小单位的数据结构</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=01c4414d2b2d467794fa3d7ae321905d&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<p>﻿</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=838acce57b234215aba983f07f19e78a&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<h4 id="中文分词难点">中文分词难点
</h4><p>中文分词不像英文那样，天然有空格作为分隔。而且中文词语组合繁多，分词很容易产生歧义。因此中文分词一直以来都是NLP的一个重点，也是一个难点。难点主要集中在分词标准，切分歧义和未登录词三部分。</p>
<h3 id="中文分词算法">中文分词算法
</h3><p>传统的分词方法：</p>
<p>主要分为两类，基于词典的规则匹配方法，和基于统计的机器学习方法。</p>
<h4 id="基于词典的分词算法">基于词典的分词算法
</h4><p>基于词典的分词算法，本质上就是字符串匹配。将待匹配的字符串基于一定的算法策略，和一个足够大的词典进行字符串匹配，如果匹配命中，则可以分词。根据不同的匹配策略，又分为正向最大匹配法，逆向最大匹配法，双向匹配分词，全切分路径选择等。</p>
<p>最大匹配法主要分为三种：</p>
<p><strong><strong>正向最大匹配法</strong></strong> ，从左到右对语句进行匹配，匹配的词越长越好。比如“商务处女干事”，划分为“商务处/女干事”，而不是“商务/处女/干事”。这种方式切分会有歧义问题出现，比如“结婚和尚未结婚的同事”，会被划分为“结婚/和尚/未/结婚/的/同事”。</p>
<p><strong><strong>逆向最大匹配法</strong></strong> ，从右到左对语句进行匹配，同样也是匹配的词越长越好。比如“他从东经过我家”，划分为“他/从/东/经过/我家”。这种方式同样也会有歧义问题，比如“他们昨日本应该回来”，会被划分为“他们/昨/日本/应该/回来”。</p>
<p><strong><strong>双向匹配分词</strong></strong> ，则同时采用正向最大匹配和逆向最大匹配，选择二者分词结果中词数较少者。但这种方式同样会产生歧义问题，比如“他将来上海”，会被划分为“他/将来/上海”。由此可见，词数少也不一定划分就正确。</p>
<p><strong><strong>全切分路径选择</strong></strong> ，将所有可能的切分结果全部列出来，从中选择最佳的切分路径。分为两种选择方法</p>
<p>n最短路径方法。将所有的切分结果组成有向无环图，切词结果作为节点，词和词之间的边赋予权重，找到权重和最小的路径即为最终结果。比如可以通过词频作为权重，找到一条总词频最大的路径即可认为是最佳路径。</p>
<p>n元语法模型。同样采用n最短路径，只不过路径构成时会考虑词的上下文关系。一元表示考虑词的前后一个词，二元则表示考虑词的前后两个词。然后根据语料库的统计结果，找到概率最大的路径。</p>
<h4 id="子词分词">子词分词
</h4><h5 id="bpe">BPE:
</h5><p>在 1994 年,BPE 算法被提出,最早用于通用的数据压缩 [137]。随后,自然  语言处理领域的研究人员将其进行适配,并应用于文本分词 [138]。BPE 算法从一  组基本符号(例如字母和边界字符)开始,迭代地寻找语料库中的两个相邻词元,  并将它们替换为新的词元,这一过程被称为合并。合并的选择标准是计算两个连  续词元的共现频率,也就是每次迭代中,最频繁出现的一对词元会被选择与合并。  合并过程将一直持续达到预定义的词表大小。</p>
<p>算法逻辑：</p>
<ol>
<li>准备足够大的训练语料</li>
<li>确定期望的subword词表大小</li>
<li>将单词拆分为字符序列并在末尾添加后缀“ &lt;/ w&gt;”，统计单词频率。 本阶段的subword的粒度是字符。 例如，“ low”的频率为5，那么我们将其改写为“ l o w &lt;/ w&gt;”：5</li>
<li>统计每一个连续字节对的出现频率，选择最高频者合并成新的subword</li>
<li>重复第4步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1</li>
</ol>
<p>停止符&quot;<code>&lt;/w&gt;</code>&ldquo;的意义在于表示subword是词后缀。举例来说：&ldquo;st&quot;字词不加&rdquo;<code>&lt;/w&gt;</code>&ldquo;可以出现在词首如&quot;st ar&rdquo;，加了&rdquo;<code>&lt;/w&gt;</code>&ldquo;表明改字词位于词尾，如&quot;wide st <code>&lt;/w&gt;</code>&quot;，二者意义截然不同。</p>
<p>每次合并后词表可能出现3种变化：</p>
<ul>
<li>+1，表明加入合并后的新字词，同时原来的2个子词还保留（2个字词不是完全同时连续出现）</li>
<li>+0，表明加入合并后的新字词，同时原来的2个子词中一个保留，一个被消解（一个字词完全随着另一个字词的出现而紧跟着出现）</li>
<li>-1，表明加入合并后的新字词，同时原来的2个子词都被消解（2个字词同时连续出现）</li>
</ul>
<p>实际上，随着合并的次数增加，词表大小通常先增加后减小。</p>
<p>﻿</p>
<p>论文地址：</p>
<p>﻿<a class="link" href="https://aclanthology.org/P18-1007.pdf"  target="_blank" rel="noopener"
    >https://aclanthology.org/P18-1007.pdf</a>﻿</p>
<p>﻿<a class="link" href="https://zhuanlan.zhihu.com/p/86965595"  target="_blank" rel="noopener"
    >https://zhuanlan.zhihu.com/p/86965595</a>﻿</p>
<p>﻿</p>
<h5 id="wordpiece-">WordPiece ：
</h5><p>WordPiece 是谷歌内部非公开的分词算法,最初是由谷歌研究人员在开发语音  搜索系统时提出的 [140]。随后,在 2016 年被用于机器翻译系统 [141],并于 2018  年被 BERT 采用作为分词器 [13]。WordPiece 分词和 BPE 分词的想法非常相似,都  是通过迭代合并连续的词元,但是合并的选择标准略有不同。在合并前,WordPiece  分词算法会首先训练一个语言模型,并用这个语言模型对所有可能的词元对进行  评分。然后,在每次合并时,它都会选择使得训练数据的似然性增加最多的词元  对。  由于谷歌并未发布 WordPiece 分词算法的官方实现,这里我们参考了 Hugging  Face 在线自然语言课程中给出的 WordPiece 算法的一种实现。与 BPE 类似,WordPiece 分词算法也是从一个小的词汇表开始,其中包括模型使用的特殊词元和初始  词汇表。由于它是通过添加前缀(如 BERT 的##)来识别子词的,因此每个词的初  始拆分都是将前缀添加到词内的所有字符上。举例来说,“word”会被拆分为:“w  ##o ##r ##d”。与 BPE 方法的另一个不同点在于,WordPiece 分词算法并不选择最  频繁的词对,而是使用下面的公式为每个词对计算分数:</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=ef9ab091017545d2b1be723df35449b3&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<h5 id="unigram">Unigram:
</h5><p>BPE存在的问题：一个词可能有多个分法</p>
<p>与 BPE 分词和 WordPiece 分词不同,Unigram 分词方法 [142] 从语料库的一  组足够大的字符串或词元初始集合开始,迭代地删除其中的词元,直到达到预期  的词表大小。它假设从当前词表中删除某个词元,并计算训练语料的似然增加情  况,以此来作为选择标准。这个步骤是基于一个训练好的一元语言模型来进行的。  为估计一元语言模型,它采用期望最大化(Expectation–Maximization, EM)算法:  在每次迭代中,首先基于旧的语言模型找到当前最优的分词方式,然后重新估计  一元概率从而更新语言模型。这个过程中一般使用动态规划算法(即维特比算法,  Viterbi Algorithm)来高效地找到语言模型对词汇的最优分词方式。采用这种分词  方法的代表性模型包括 T5 和 mBART。</p>
<p>论文地址：</p>
<p>﻿<a class="link" href="https://aclanthology.org/P16-1162.pdf"  target="_blank" rel="noopener"
    >https://aclanthology.org/P16-1162.pdf</a>﻿</p>
<p>﻿<a class="link" href="https://blog.csdn.net/m0_72947390/article/details/134821437"  target="_blank" rel="noopener"
    >[UIM]论文解读：subword Regularization: Multiple Subword Candidates_unigram language model论文-CSDN博客</a>﻿</p>
<p>﻿</p>
<h5 id="byte-level-bpebbpe">Byte-level BPE(BBPE)
</h5><p>基础知识：</p>
<p><strong><strong>Unicode：</strong></strong> Unicode 是一种字符集，旨在涵盖地球上几乎所有的书写系统和字符。它为每个字符分配了一个唯一的代码点（code point）用于标识字符。Unicode 不关注字符在计算机内部的具体表示方式，而只是提供了一种字符到代码点的映射。Unicode 的出现解决了字符集的碎片化问题，使得不同的语言和字符能够在一个共同的标准下共存。然而，Unicode 并没有规定如何在<a class="link" href="https://zhida.zhihu.com/search?content_id=233133558&amp;content_type=Article&amp;match_order=1&amp;q=%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%86%85%E5%AD%98&amp;zhida_source=entity"  target="_blank" rel="noopener"
    >计算机内存</a>中存储和传输这些字符。</p>
<p><strong><strong>UTF-8：</strong></strong> UTF-8（Unicode Transformation Format-8）是一种变长的字符编码方案，它将 Unicode 中的代码点转换为字节序列。UTF-8 的一个重要特点是它是向后兼容 ASCII 的，这意味着标准的 ASCII 字符在 UTF-8 中使用相同的字节表示，从而确保现有的 ASCII 文本可以无缝地与 UTF-8 共存。在 UTF-8 编码中，字符的表示长度可以是1到4个字节，不同范围的 Unicode 代码点使用不同长度的字节序列表示，这样可以高效地表示整个 Unicode 字符集。UTF-8 的<a class="link" href="https://zhida.zhihu.com/search?content_id=233133558&amp;content_type=Article&amp;match_order=1&amp;q=%E7%BC%96%E7%A0%81%E8%A7%84%E5%88%99&amp;zhida_source=entity"  target="_blank" rel="noopener"
    >编码规则</a>是：</p>
<ul>
<li>单字节字符（ASCII 范围内的字符）使用一个字节表示，保持与 ASCII 编码的兼容性。</li>
<li>带有更高代码点的字符使用多个字节表示。UTF-8 使用特定的字节序列来指示一个字符所需的字节数，以及字符的实际数据。</li>
</ul>
<p>例如，英文字母 &ldquo;A&rdquo; 的 Unicode 代码点是U+0041，在 UTF-8 中表示为 0x41（与 ASCII 编码相同）；而中文汉字 &ldquo;你&rdquo; 的 Unicode 代码点是U+4F60，在 UTF-8 中表示为0xE4 0xBD 0xA0三个字节的序列。</p>
<p>所以简单的来说：</p>
<ul>
<li>Unicode 是字符集，为每个字符分配唯一的代码点。</li>
<li>UTF-8 是一种基于 Unicode 的字符<a class="link" href="https://zhida.zhihu.com/search?content_id=233133558&amp;content_type=Article&amp;match_order=1&amp;q=%E7%BC%96%E7%A0%81%E6%96%B9%E5%BC%8F&amp;zhida_source=entity"  target="_blank" rel="noopener"
    >编码方式</a>，用于在计算机中存储和传输字符。</li>
</ul>
<p><strong><strong>Byte(字节)</strong></strong> ：计算机存储和数据处理时，字节是最小的单位。一个字节包含8个(Bit)二进制位，每个位可以是0或1，每位的不同排列和组合可以表示不同的数据，所以一个字节能表示的范围是256个。</p>
<p>言归正传：</p>
<p>Byte-level BPE(BBPE)和Byte-Pair Encoding (BPE)区别就是BPE是最小词汇是字符级别，而BBPE是字节级别的，通过UTF-8的编码方式这一个字节的256的范围，理论上可以表示这个世界上的所有字符。</p>
<p>所以实现的步骤和BPE就是实现的粒度不一样，其他的都是一样的。</p>
<ol>
<li>初始词表：构建初始词表，包含一个字节的所有表示(256)。</li>
<li>构建频率统计：统计所有子词单元对（两个连续的子词）在文本中的出现频率。</li>
<li>合并频率最高的子词对：选择出现频率最高的子词对，将它们合并成一个新的子词单元，并更新词汇表。</li>
<li>重复合并步骤：不断重复步骤 2 和步骤 3，直到达到预定的词汇表大小、合并次数，或者直到不再有有意义的合并（即，进一步合并不会显著提高词汇表的效益）。</li>
<li>分词：使用最终得到的词汇表对文本进行分词。</li>
</ol>
<h3 id="获取词向量">获取词向量：
</h3><h4 id="独热表示onehotrepresentation">独热表示（Onehotrepresentation）
</h4><p>在自然语言处理的各项任务中，需要将字、词进行编码，形成数字表达的向量的形式，以便进行运算。常见的一种编码方式是独热表示（Onehotrepresentation）。这种编码方式是很多编码方式的基础。具体做法是首先有一个词汇表V，并按指定规则对词汇进行排序。定义一个向量用来表示词汇，向量大小同词汇表大小。词汇w的向量表示为c(w)，其在词汇表中的次序是i。c(w)的值第i位为1，其他位置是0。以‘我爱NLP’未例，具体表示如下：</p>
<p>w= ‘我‘，在词表中的次序是1；</p>
<p>c(w) =[0,1,0,0,…,0,0,0]；</p>
<p>W = ‘爱‘，在词表中的次序是3；</p>
<p>c(w) =[0,0,0,1,…,0,0,0]；</p>
<p>W = ‘NLP‘，在词表中的次序是2；</p>
<p>c(w)=[0,1,0,0,…,0,0,0]；</p>
<p>独热表示存在如下的问题：</p>
<ul>
<li>独热向量中的每一个位置都表示一个单词，但词汇之间没有相关性；</li>
<li>向量大小受词表的大小影响，词汇量大时便造成维数爆炸问题；</li>
<li>该词向量也无法表示未出现的词；</li>
</ul>
<h4 id="分布表示distributedrepresentation">分布表示（DistributedRepresentation）
</h4><p>另外一种词汇的表示是Hinton提出的[49]分布表示（DistributedRepresentation）。主要做法是基于大量的文本数据学习，将语言中每个词汇对应到固定大小的向量。其长度一般远远小于词表大小。每个词汇可以再空间中映射成某一点，从而将距离概念引入词的表达，可以便捷地计算词汇之间在句法、语义上的相似性。</p>
<p>在文本的表示方面，主要有两种不同的表示方式：上下文无关的文本表示和上下文相关的文本表示。上下文无关的文本表示以Word2vec为代表，每个词映射成固定的词向量；上下文相关的文本表示以ELMO为发端，每个词汇表示为动态的词向量，根据上下文的不同，词向量不同。Word2vec不能解决一词多义的问题，而且存在词表之外词的表示问题（OOV）。</p>
<ul>
<li><strong><strong>NNLM时代</strong></strong></li>
<li>Word2Vec时代</li>
<li>ELMO-基于上下文的动态词向量</li>
</ul>
<p>ELMO[55]是一种基于语境的词汇表达模型，不但可以解决一词多义的问题，而且可以挖掘出词汇的复杂特征，如词性特征、语法特征等。模型的核心思想是将句子来作为上下文，来训练词向量，学习当前上下文环境下的词汇表征。</p>
<ul>
<li>大模型时代的词向量，以Bert为开端</li>
</ul>
<p>额外补充：目前中文词嵌入效果最好的模型之一：bge:</p>
<p>﻿<a class="link" href="https://arxiv.org/pdf/2309.07597"  target="_blank" rel="noopener"
    >https://arxiv.org/pdf/2309.07597</a>﻿</p>
<p>﻿<a class="link" href="https://www.modelscope.cn/models/AI-ModelScope/bge-large-zh#usage"  target="_blank" rel="noopener"
    >https://www.modelscope.cn/models/AI-ModelScope/bge-large-zh#usage</a>﻿</p>
<p>﻿</p>
<h2 id="需要掌握的神经网络结构rnnlstmseq2seq">需要掌握的神经网络结构：RNN、LSTM、Seq2Seq
</h2><p>大模型之前的NLP模型、算法都在尝试解决的问题：更好的表示输入，更好地利用知识（记忆）</p>
<h3 id="rnn与bi-rnn">RNN与Bi-RNN:
</h3><p>NLP的某些任务如机器翻译、语音识别等都属于序列生成任务，每一时刻都输出都依赖与上一时刻以及之前的输出。RNN（RecurrentNeuralNetworks）是一种处理序列任务的神经网络[6]。</p>
<p>循环神经网络由数据获取层、数据流出层、隐藏层构成，与前馈神经网络相似。不同的是，RNN的隐藏层之间相互连接，使得RNN有了记忆之前输入内容的能力。隐藏层包含着之前每个时刻的信息，因此使得输出能够获得充分的信息。RNN的基本结构示意如图2.1：</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=4c5ddbd9c6fd4811babe4bc6634d1d34&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<p>在实际的应用中，只考虑输入的从左到右，并不能完全利用输入的信息。例如在文本摘要的任务重，某一时刻的输入不但与之前的输入信息有关，也与未输入的词汇有关。双向的神经网络从两个方向来考虑输入，能够对信息进行更充分的抽取和编码。其结构也比较简单，在原有的从左到右的隐藏层之上加上了一层从右到左的隐藏层，并将两者链接。双向神经网络可以获取两个方向的文本表示，增强了隐藏层的信息表达。图2.2是双向神经网络的示意图。ho层是从左到右的网络层，g0层是从由到左的网络层。这两层的信息拼接，作为下一层隐藏层的输入。</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=124782cde67c495e8475e53c70d1969a&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<p>﻿</p>
<p>[流程图]### LSTM</p>
<p>RNN模型存在两个问题：长期依赖问题和梯度消失问题[45]。长期依赖问题是指RNN网络无法记忆较长的输入内容，只能记住最近的内容，这是链式求导数导致的问题[7]。长短期记忆模型与标准的循环神经网络的差别是体现在隐藏层。LSTM通过隐藏单元的门控机制来筛选信息，从而减弱了长期依赖和梯度小时问题。LSTM的门控主要包括输入门，遗忘门，输出门三部分，具体的计算如下</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=fecd58dbfa814bf58fa7bd32c4b220af&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=6ef8c9694d3f4f0b9c6976ee4ecd8503&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<h3 id="序列到序列seq2seq">序列到序列（Seq2Seq）
</h3><p>DNN(深度神经网络，多层神经网络的叠加)是非常强大的机器学习工具，在自然语言处理的多个任务上取得了不错成绩。但是DNN只能处理输入和输出维度为固定的任务[34]。但是如翻译，摘要等任务的输出的长度是不定的，这类问题需要输入与输出是域无关的方案。这类的任务需要解决数据对齐的问题。序列到序列（SequencetoSequence）便是这样一套方案。</p>
<p>序列到序列结构的主要分为两部分：编码器和解码器。编码器和解码器一般都是用RNN。编码器通过RNN网络将数据编码为指定的维度的输出，解码器从编码器的输出中解码为需要的维度，从而解决了输入与输出的对齐问题。</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=6687cd95afe547efa971a92958e4cc76&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<p>﻿</p>
<p>以一个对话系统为例，主要的结构如图3.1。</p>
<p>图3.1Seq2Seq结构示意图</p>
<p>输入的句子是“我爱NLP”,经过编码器的序列，编码器的隐藏层链接输出层的隐藏层。<code>&lt;S&gt;</code>为输出的开始。“<code>&lt;S&gt;</code>”经过隐藏层，输出第一个预测：“Me”,输出值“Me”作为下一刻的输入，再进入隐藏层。当输出为 <code>&lt;/S&gt;</code>表示输出的结束。这时候整个输入输出的流程就结束了。</p>
<p>循环神经网络因其本身的时序特性和前后依赖特性，适合用来做生成式任务。但是因为循环神经网络自身存在的长期依赖性问题，在某一时刻的预测若与该时刻较远的输入有很大相关关系时，RNN可能是不包含这些信息的，导致预测不准确。因此，在Seq2Seq的机构中，通常使用长短期记忆模型（LSTM）通过输入门、遗忘门和输出门来控制RNN要记住的内容，从而提升了对长距离内容的记忆效果。</p>
<p>图3.1是最基本的Seq2Seq结构，在实际的使用中通常会在编码器和解码器之间有一个存储区域（context），用来存储编码器的所有隐藏层信息，而不是解码器仅仅依赖最后一层隐藏层。此时，编码器将输入映射为固定长度，解码器根据编码器的(context)来生成内容输出。户保田[75]等人的论文中证明，使用了context的Seq2Seq模型相比不使用context，生成的摘要在R-1,R-2,R-L都有较大幅度的提升。</p>
<p>Seq2Seq结构有效地解决了源输入数据与输出数据的对齐问题。文本摘要的长度通常都小于文本的长度。但是仅仅依赖与循环神经网络的Seq2Seq模型的信息的编码能力仍有待加强。特别是解码器不考虑长度，将输入内容压缩为固定的长度带来信息的丢失。输入是特别长的文章时，这种信息的丢失将会更加明显。在输入的文本中，不同的句子对最终的摘要重要性是不同的，但在编码器中被压缩的文本并没有权重上的分别。</p>
<h2 id="注意力机制注意力自注意力多头注意力">注意力机制：注意力、自注意力、多头注意力
</h2><h3 id="注意力">注意力
</h3><p>基本的序列到序列模型的编码器会把输入内容压缩到固定的长度，但是在输入的内容比较长的时候，就不能有效地记住足够的信息了，这是序列到序列模型的一个重要问题。Bahdanau等人[46]在2015年发表了一篇机器翻译的文章提到了一种注意力机制。注意力机制在编码器的部分，对输入的不同部分建立一个索引，在输出阶段可以随时来回看与输入相关的输入片段，这样编码器在输出时就不需要存储所有的信息了。加入注意力机制的序列到序列模型可以有效地解决这个问题。其基本的结构如图3.2。</p>
<p>这里编码器使用了一个双向的LSTM模型，Attention机制主要是在解码器进行解码时产生作用。其中涉及到的公式如下：</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=422dd4221cb7496985434521da6d5cf7&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<p>﻿**$a_t=attn(hs,s_i), i =t-1$**﻿</p>
<p>﻿**$a_s = Softmax(a)_t$**﻿</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=5c0f393e8da44555a46a81101a8d62ae&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<p>﻿</p>
<p>解码器在t时刻时，根据编码器的隐藏层输出h,和当前的解码器的隐藏层输出s_t，来计算在不同的输入i(i表示输入的次序，h_i表示第i个输入的隐藏层输出，包括正反两个方向)的得分e_i^t。然后将得分归一化，生成Attention的权重a^t。Attention的权重与对应的编码器的隐藏层相乘，得到t时刻输出应该关注的输入内容h_t^*，进而根据输入h_t^*和解码器的隐藏层输出s_t得到当前的输出P_t (w)。</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=0eb1109cbed34a44834d176dc99128b8&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<p>注意力机制去掉了编码器与解码器之间的数据压缩过程，直接数据流通，可以使得Seq2Seq结构不必再映射成为一个固定的长度。同时解码器的每一时刻都直接关注与当前输出有关的编码端信息，不但加速了信息的流动速度，也增强了获取信息的质量。Nallapati[36]等人的实验也证明了注意力机制在文本摘要任务中的有效性。</p>
<p>但是注意力机制的运用也使得另一个问题愈发明显：内容重复生成。这是因为注意力机制使得生成内容时，会重点关注输入中某一部分重点内容，从使得生成的句子冲出现重复的单词。</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=42af4e36e6444445b8d056ca91a0c00f&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<p>﻿<a class="link" href="https://blog.csdn.net/SunshineSki/article/details/115680648"  target="_blank" rel="noopener"
    >深度学习笔记：如何理解激活函数？（附常用激活函数）_什么是激活函数?举例说明几种常见的激活函数及其用途-CSDN博客</a>﻿</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=7f86feabcb9642879c2e7c87fca864dd&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<h3 id="自注意力">自注意力
</h3><p>注意机制关注的是t时刻的输出与输入序列各部分的关系，而自注意力机制关注的是输入序列中各个部分的相互关系。根据这些相互关系来重新表示每个词汇。如句子：“我爱自然语言处理，学习它我能得到很高的成就感”。句子中的“它”指的是“自然语言处理”，两者具有很强的关系；“我”和“自然语言处理”分别是“学习”的执行者和执行对象，也都有一定的关系。自注意力机制便是要挖掘词汇中的这些关系。具体的计算如下：</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=7be2857259e24c44aaf1f6c5795c33de&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=9d76e3bd4246439fb25112fad958bd20&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<p>﻿<a class="link" href="https://0809zheng.github.io/2020/04/24/self-attention.html"  target="_blank" rel="noopener"
    >https://0809zheng.github.io/2020/04/24/self-attention.html</a>﻿</p>
<h3 id="自注意力和注意力机制的区别">自注意力和注意力机制的区别：
</h3><ul>
<li>注意力机制的权重参数是一个全局可学习参数，对于模型来说是固定的；而自注意力机制的权重参数是由输入决定的，即使是同一个模型，对于不同的输入也会有不同的权重参数。</li>
<li>注意力机制的输出序列长度与输入序列长度可以是不同的；而自注意力机制的的输出序列长度与输入序列长度必须是相同的。</li>
<li>注意力机制在一个模型中通常只使用一次，作为编码器和解码器之间的连接部分；而自注意力机制在同一个模型中可以使用很多次，作为网络结构的一部分。</li>
<li>注意力机制擅长捕捉两个序列之间的关系，如机器翻译任务中将一个序列映射为另一个序列；而自注意力机制擅长捕捉单个序列内部的关系，如作为预训练语言模型的基本结构。</li>
</ul>
<h2 id="编码器与解码器">编码器与解码器
</h2><p>﻿<a class="link" href="https://blog.csdn.net/nocml/article/details/103082600"  target="_blank" rel="noopener"
    >Transformer(一)&ndash;论文翻译：Attention Is All You Need 中文版-CSDN博客</a>﻿</p>
<h3 id="位置编码">位置编码
</h3><p>自注意力机使得模型可以有效地并行计算，但是却没有RNN的输入序列信息。为了解决这一问题，在输入词汇性自注意力计算之前加上一层位置编码，再进行运算，使得位置信息也能参与到自注意力的表示中。Transformers中使用使用正弦函数和余弦函数来构造位置编码:</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=bd0b800f3aea4e688575289bea4719ea&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<h2 id="编码器">编码器
</h2><p>编码器由输入层和若干个叠加的编码层构成。输入层包括合并词向量和位置编码。每个编码层中又顺序包括自注意力层、正则化层、全连接的前馈神经网络层、正则化层。输入层的词向量首先进入自注意力层获取由相关关系表示的输入，进入正则化层进行处理，再进入前馈网络层，再进行正则化，最后输出，进入下一个编码层，直到最后输出。具体流程如图2.4所示</p>
<ul>
<li>残差链接层&amp;归一化层（add &amp;normalize）</li>
</ul>
<p>﻿</p>
<p>参考链接：</p>
<p>﻿<a class="link" href="https://developer.volcengine.com/articles/7389519960881496075"  target="_blank" rel="noopener"
    >https://developer.volcengine.com/articles/7389519960881496075</a>﻿</p>
<p><strong><strong>基本上所有的归一化技术，都可以概括为如下的公式：</strong></strong></p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=3e04ad12b5544b2198f99c559500a74b&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<p><strong><strong>对于隐层中某个节点的输出，即激活值a，进行非线性变换（如ReLU、tanh等）后得到h。</strong></strong></p>
<p>****层归一化的过程<strong><strong><strong><strong>就是先计算这一层所有激活值的均值μ和方差σ²，然后使用这些统计量</strong></strong></strong></strong>对h进行分布调整。****<strong><strong>这种调整就是把“高瘦”和“矮胖”的都调整回正常体型（深粉色），把偏离x=0的拉回中间来（淡紫色）。</strong></strong></p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=ce3adf080058463da8f15a565ace3feb&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<h3 id="解码器">解码器：
</h3><p>类似于编码器，解码器也包含若干个叠加的解码层，最后通过一个线性连接层和输出层输出预测。在每个编码层中，首先是解码器的输入转化为输入向量（词向量+位置编码），通过注意力层和正则化层。这时候，需要将编码器的输出映射成为K,和V,来计算输入层在此刻输出的权重，并对输入进行加权计算。然后再进入一个正则化层，前向网络层，再进入一个正则化层，输出到下一个解码层。在所有的解码器层顶端通过线性连接层和softmax层输出结果。编码器到解码器的具体的流程如图2.5。</p>
<p>综合自注意力机制、位置编码、编码器和解码器，Transformer的整体的模型结构如</p>
<p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=6bfc819ee2404f47a34edd18c48c2fcc&amp;docGuid=oAA1OqieLOlvNV"
	
	
	
	loading="lazy"
	
	
></p>
<p>﻿</p>
<p>﻿</p>
<h2 id="transformer模型的计算量评估">Transformer模型的计算量评估
</h2><p>﻿<a class="link" href="https://mp.weixin.qq.com/s/NoREsyLXNVk1aABtSkhBDA"  target="_blank" rel="noopener"
    >万字长文解析：大模型需要怎样的硬件算力</a>﻿</p>
<h2 id="transformer模型可视化">Transformer模型可视化
</h2><p>﻿<a class="link" href="https://bbycroft.net/llm"  target="_blank" rel="noopener"
    >https://bbycroft.net/llm</a>﻿</p>
<p>﻿</p>
<p>﻿</p>
<p><a class="link" href="" >理解Transformer_从0到1.pdf</a></p>

</section>


    <footer class="article-footer">
    

    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
	const mainArticleElement = document.querySelector(".main-article");
        renderMathInElement(mainArticleElement, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>

    
</article>

    

    

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2025 流年
    </section>
    
    <section class="powerby">
        使用 <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> 构建 <br />
        主题 <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.30.0">Stack</a></b> 由 <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a> 设计
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
